{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 3 - Making RL PySC2 Agent with sparse reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unfortunately, PySC2 uses Abseil, which treats python code as if its run like an app\n",
    "# This does not play well with jupyter notebook\n",
    "# So we will need to monkeypatch sys.argv\n",
    "\n",
    "\n",
    "import sys\n",
    "#sys.argv = [\"python\", \"--map\", \"AbyssalReef\"]\n",
    "sys.argv = [\"python\", \"--map\", \"Simple64\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Runnning 'Agent code' on jupyter notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Copyright 2017 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS-IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"Run an agent.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import importlib\n",
    "import threading\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from future.builtins import range  # pylint: disable=redefined-builtin\n",
    "\n",
    "from pysc2 import maps\n",
    "from pysc2.env import available_actions_printer\n",
    "from pysc2.env import run_loop\n",
    "from pysc2.env import sc2_env\n",
    "from pysc2.lib import point_flag\n",
    "from pysc2.lib import stopwatch\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "# because of Abseil's horrible design for running code underneath Colabs\n",
    "# We have to pull out this ugly hack from the hat\n",
    "if \"flags_defined\" not in globals():\n",
    "    flags.DEFINE_bool(\"render\", False, \"Whether to render with pygame.\")\n",
    "    point_flag.DEFINE_point(\"feature_screen_size\", \"84\",\n",
    "                            \"Resolution for screen feature layers.\")\n",
    "    point_flag.DEFINE_point(\"feature_minimap_size\", \"64\",\n",
    "                            \"Resolution for minimap feature layers.\")\n",
    "    point_flag.DEFINE_point(\"rgb_screen_size\", None,\n",
    "                            \"Resolution for rendered screen.\")\n",
    "    point_flag.DEFINE_point(\"rgb_minimap_size\", None,\n",
    "                            \"Resolution for rendered minimap.\")\n",
    "    flags.DEFINE_enum(\"action_space\", None, sc2_env.ActionSpace._member_names_,  # pylint: disable=protected-access\n",
    "                      \"Which action space to use. Needed if you take both feature \"\n",
    "                      \"and rgb observations.\")\n",
    "    flags.DEFINE_bool(\"use_feature_units\", True,\n",
    "                      \"Whether to include feature units.\")\n",
    "    flags.DEFINE_bool(\"disable_fog\", True, \"Whether to disable Fog of War.\")\n",
    "\n",
    "    flags.DEFINE_integer(\"max_agent_steps\", 0, \"Total agent steps.\")\n",
    "    flags.DEFINE_integer(\"game_steps_per_episode\", None, \"Game steps per episode.\")\n",
    "    flags.DEFINE_integer(\"max_episodes\", 0, \"Total episodes.\")\n",
    "    flags.DEFINE_integer(\"step_mul\", 8, \"Game steps per agent step.\")\n",
    "    flags.DEFINE_float(\"fps\", 22.4, \"Frames per second to run the game.\")\n",
    "\n",
    "    #flags.DEFINE_string(\"agent\", \"sc2.agent.BasicAgent.ZergBasicAgent\",\n",
    "    #                    \"Which agent to run, as a python path to an Agent class.\")\n",
    "    #flags.DEFINE_enum(\"agent_race\", \"zerg\", sc2_env.Race._member_names_,  # pylint: disable=protected-access\n",
    "    #                  \"Agent 1's race.\")\n",
    "    flags.DEFINE_string(\"agent\", \"TerranSparseRewardRLAgent\",\n",
    "                        \"Which agent to run, as a python path to an Agent class.\")\n",
    "    flags.DEFINE_enum(\"agent_race\", \"terran\", sc2_env.Race._member_names_,  # pylint: disable=protected-access\n",
    "                      \"Agent 1's race.\")\n",
    "\n",
    "    flags.DEFINE_string(\"agent2\", \"Bot\", \"Second agent, either Bot or agent class.\")\n",
    "    flags.DEFINE_enum(\"agent2_race\", \"terran\", sc2_env.Race._member_names_,  # pylint: disable=protected-access\n",
    "                      \"Agent 2's race.\")\n",
    "    flags.DEFINE_enum(\"difficulty\", \"very_easy\", sc2_env.Difficulty._member_names_,  # pylint: disable=protected-access\n",
    "                      \"If agent2 is a built-in Bot, it's strength.\")\n",
    "\n",
    "    flags.DEFINE_bool(\"profile\", False, \"Whether to turn on code profiling.\")\n",
    "    flags.DEFINE_bool(\"trace\", False, \"Whether to trace the code execution.\")\n",
    "    flags.DEFINE_integer(\"parallel\", 1, \"How many instances to run in parallel.\")\n",
    "\n",
    "    flags.DEFINE_bool(\"save_replay\", True, \"Whether to save a replay at the end.\")\n",
    "\n",
    "    flags.DEFINE_string(\"map\", None, \"Name of a map to use.\")\n",
    "    flags.mark_flag_as_required(\"map\")\n",
    "\n",
    "flags_defined = True\n",
    "\n",
    "def run_thread(agent_classes, players, map_name, visualize):\n",
    "  \"\"\"Run one thread worth of the environment with agents.\"\"\"\n",
    "  with sc2_env.SC2Env(\n",
    "      map_name=map_name,\n",
    "      players=players,\n",
    "      agent_interface_format=sc2_env.parse_agent_interface_format(\n",
    "          feature_screen=FLAGS.feature_screen_size,\n",
    "          feature_minimap=FLAGS.feature_minimap_size,\n",
    "          rgb_screen=FLAGS.rgb_screen_size,\n",
    "          rgb_minimap=FLAGS.rgb_minimap_size,\n",
    "          action_space=FLAGS.action_space,\n",
    "          use_feature_units=FLAGS.use_feature_units),\n",
    "      step_mul=FLAGS.step_mul,\n",
    "      game_steps_per_episode=FLAGS.game_steps_per_episode,\n",
    "      disable_fog=FLAGS.disable_fog,\n",
    "      visualize=visualize) as env:\n",
    "    env = available_actions_printer.AvailableActionsPrinter(env)\n",
    "    agents = [agent_cls() for agent_cls in agent_classes]\n",
    "    run_loop.run_loop(agents, env, FLAGS.max_agent_steps, FLAGS.max_episodes)\n",
    "    if FLAGS.save_replay:\n",
    "      env.save_replay(agent_classes[0].__name__)\n",
    "\n",
    "def main(unused_argv):\n",
    "  \"\"\"Run an agent.\"\"\"\n",
    "  #stopwatch.sw.enabled = FLAGS.profile or FLAGS.trace\n",
    "  #stopwatch.sw.trace = FLAGS.trace\n",
    "\n",
    "  map_inst = maps.get(FLAGS.map)\n",
    "\n",
    "  agent_classes = []\n",
    "  players = []\n",
    "\n",
    "  #agent_module, agent_name = FLAGS.agent.rsplit(\".\", 1)\n",
    "  #agent_cls = getattr(importlib.import_module(agent_module), agent_name)\n",
    "  #agent_classes.append(agent_cls)\n",
    "  agent_classes.append(TerranSparseRewardRLAgent)\n",
    "  players.append(sc2_env.Agent(sc2_env.Race[FLAGS.agent_race]))\n",
    "\n",
    "  if map_inst.players >= 2:\n",
    "    if FLAGS.agent2 == \"Bot\":\n",
    "      players.append(sc2_env.Bot(sc2_env.Race[FLAGS.agent2_race],\n",
    "                                 sc2_env.Difficulty[FLAGS.difficulty]))\n",
    "    else:\n",
    "      agent_module, agent_name = FLAGS.agent2.rsplit(\".\", 1)\n",
    "      agent_cls = getattr(importlib.import_module(agent_module), agent_name)\n",
    "      agent_classes.append(agent_cls)\n",
    "      players.append(sc2_env.Agent(sc2_env.Race[FLAGS.agent2_race]))\n",
    "\n",
    "  threads = []\n",
    "  for _ in range(FLAGS.parallel - 1):\n",
    "    t = threading.Thread(target=run_thread,\n",
    "                         args=(agent_classes, players, FLAGS.map, False))\n",
    "    threads.append(t)\n",
    "    t.start()\n",
    "\n",
    "  run_thread(agent_classes, players, FLAGS.map, FLAGS.render)\n",
    "\n",
    "  for t in threads:\n",
    "    t.join()\n",
    "\n",
    "  if FLAGS.profile:\n",
    "    pass\n",
    "    #print(stopwatch.sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creating a RL PySC2 Agent with Sparse Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import math\n",
    "import os.path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from pysc2.agents import base_agent\n",
    "from pysc2.env import sc2_env\n",
    "from pysc2.lib import actions, features, units\n",
    "from absl import app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = 'rlagent_with_sparse_reward_learning_data'\n",
    "\n",
    "ACTION_DO_NOTHING = 'donothing'\n",
    "ACTION_BUILD_SUPPLY_DEPOT = 'buildsupplydepot'\n",
    "ACTION_BUILD_BARRACKS = 'buildbarracks'\n",
    "ACTION_BUILD_MARINE = 'buildmarine'\n",
    "ACTION_ATTACK = 'attack'\n",
    "\n",
    "smart_actions = [\n",
    "    ACTION_DO_NOTHING,\n",
    "    ACTION_BUILD_SUPPLY_DEPOT,\n",
    "    ACTION_BUILD_BARRACKS,\n",
    "    ACTION_BUILD_MARINE,\n",
    "]\n",
    "\n",
    "for mm_x in range(0, 64):\n",
    "    for mm_y in range(0, 64):\n",
    "        if (mm_x + 1) % 32 == 0 and (mm_y + 1) % 32 == 0:\n",
    "            smart_actions.append(ACTION_ATTACK + '_' + str(mm_x - 16) + '_' + str(mm_y - 16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QLearningTable 클래스 learn() 함수 수정\n",
    "\n",
    "#### Sparse Reward를 처리하기 위해 Terminal State 일때만 Reward가 발생하는 상황을 고려함.\n",
    "\n",
    "#### < AS-IS >\n",
    "    ...\n",
    "        q_predict = self.q_table.ix[s, a]\n",
    "        q_target = r + self.gamma * self.q_table.ix[s_, :].max()\n",
    "\n",
    "        # update\n",
    "        self.q_table.ix[s, a] += self.lr * (q_target - q_predict)\n",
    "    ...\n",
    "    \n",
    "#### < TO-BE >\n",
    "    ...\n",
    "        q_predict = self.q_table.ix[s, a]\n",
    "\n",
    "        if s_ != 'terminal':\n",
    "            q_target = r + self.gamma * self.q_table.loc[s_, self.q_table.columns[:]].max()\n",
    "        else:\n",
    "            q_target = r  # next state is terminal\n",
    "            \n",
    "        # update\n",
    "        self.q_table.loc[s, a] += self.lr * (q_target - q_predict)\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference from https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow\n",
    "class QLearningTable:\n",
    "    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9):\n",
    "        self.actions = actions  # a list\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = reward_decay\n",
    "        self.epsilon = e_greedy\n",
    "        self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64)\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        self.check_state_exist(observation)\n",
    "        \n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            # choose best action\n",
    "            #state_action = self.q_table.ix[observation, :]\n",
    "            state_action = self.q_table.loc[observation, self.q_table.columns[:]]\n",
    "            \n",
    "            # some actions have the same value\n",
    "            state_action = state_action.reindex(np.random.permutation(state_action.index))\n",
    "            \n",
    "            action = state_action.idxmax()\n",
    "        else:\n",
    "            # choose random action\n",
    "            action = np.random.choice(self.actions)\n",
    "            \n",
    "        return action\n",
    "\n",
    "    def learn(self, s, a, r, s_):\n",
    "        self.check_state_exist(s_)\n",
    "        self.check_state_exist(s)\n",
    "        \n",
    "        #q_predict = self.q_table.ix[s, a]\n",
    "        q_predict = self.q_table.loc[s, a]\n",
    "        \n",
    "        if s_ != 'terminal':\n",
    "            #q_target = r + self.gamma * self.q_table.ix[s_, :].max()\n",
    "            q_target = r + self.gamma * self.q_table.loc[s_, self.q_table.columns[:]].max()\n",
    "        else:\n",
    "            q_target = r  # next state is terminal\n",
    "            \n",
    "        # update\n",
    "        #self.q_table.ix[s, a] += self.lr * (q_target - q_predict)\n",
    "        self.q_table.loc[s, a] += self.lr * (q_target - q_predict)\n",
    "\n",
    "    def check_state_exist(self, state):\n",
    "        if state not in self.q_table.index:\n",
    "            # append new state to q table\n",
    "            self.q_table = self.q_table.append(pd.Series([0] * len(self.actions), index=self.q_table.columns, name=state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TerranSparseRewardRLAgent(base_agent.BaseAgent):\n",
    "    def __init__(self):\n",
    "        super(TerranSparseRewardRLAgent, self).__init__()\n",
    "        \n",
    "        self.qlearn = QLearningTable(actions=list(range(len(smart_actions))))\n",
    "        \n",
    "        self.previous_action = None\n",
    "        self.previous_state = None\n",
    "        \n",
    "        self.cc_y = None\n",
    "        self.cc_x = None\n",
    "        \n",
    "        self.move_number = 0\n",
    "        \n",
    "        if os.path.isfile(DATA_FILE + '.gz'):\n",
    "            self.qlearn.q_table = pd.read_pickle(DATA_FILE + '.gz', compression='gzip')\n",
    "\n",
    "    def transformDistance(self, x, x_distance, y, y_distance):\n",
    "        if not self.base_top_left:\n",
    "            return [x - x_distance, y - y_distance]\n",
    "        \n",
    "        return [x + x_distance, y + y_distance]\n",
    "    \n",
    "    def transformLocation(self, x, y):\n",
    "        if not self.base_top_left:\n",
    "            return [64 - x, 64 - y]\n",
    "        \n",
    "        return [x, y]\n",
    "    \n",
    "    def getMeanLocation(self, unitList):\n",
    "        sum_x = 0\n",
    "        sum_y = 0\n",
    "        for unit in unitList:\n",
    "            sum_x += unit.x\n",
    "            sum_y += unit.y\n",
    "        mean_x = sum_x / len(unitList)\n",
    "        mean_y = sum_y / len(unitList)\n",
    "        \n",
    "        return [mean_x, mean_y]\n",
    "    \n",
    "    def splitAction(self, action_id):\n",
    "        smart_action = smart_actions[action_id]\n",
    "            \n",
    "        x = 0\n",
    "        y = 0\n",
    "        if '_' in smart_action:\n",
    "            smart_action, x, y = smart_action.split('_')\n",
    "\n",
    "        return (smart_action, x, y)\n",
    "    \n",
    "    def unit_type_is_selected(self, obs, unit_type):\n",
    "        if (len(obs.observation.single_select) > 0 and\n",
    "            obs.observation.single_select[0].unit_type == unit_type):\n",
    "              return True\n",
    "\n",
    "        if (len(obs.observation.multi_select) > 0 and\n",
    "            obs.observation.multi_select[0].unit_type == unit_type):\n",
    "              return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def get_units_by_type(self, obs, unit_type):\n",
    "        return [unit for unit in obs.observation.feature_units\n",
    "                if unit.unit_type == unit_type]\n",
    "\n",
    "    def can_do(self, obs, action):\n",
    "        return action in obs.observation.available_actions\n",
    "        \n",
    "    def step(self, obs):\n",
    "        super(TerranSparseRewardRLAgent, self).step(obs)\n",
    "\n",
    "        #time.sleep(0.5)\n",
    "        \n",
    "        if obs.first():\n",
    "            player_y, player_x = (obs.observation.feature_minimap.player_relative == features.PlayerRelative.SELF).nonzero()\n",
    "            self.base_top_left = 1 if player_y.any() and player_y.mean() <= 31 else 0\n",
    "            print(\"player_y: \", player_y)\n",
    "            print(\"player_y.mean(): \", player_y.mean())\n",
    "            print(\"base_top_left: \", self.base_top_left)\n",
    "            print(\"smart_actions: \", smart_actions)\n",
    "        \n",
    "        ccs = self.get_units_by_type(obs, units.Terran.CommandCenter)\n",
    "        if len(ccs) > 0:\n",
    "            self.cc_x, self.cc_y = self.getMeanLocation(ccs)\n",
    "            \n",
    "        cc_count = len(ccs)\n",
    "        \n",
    "        supply_depot_count = len(self.get_units_by_type(obs, units.Terran.SupplyDepot))\n",
    "\n",
    "        barracks_count = len(self.get_units_by_type(obs, units.Terran.Barracks))\n",
    "            \n",
    "        return actions.FUNCTIONS.no_op()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [run code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  app.run(main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Adding 1st Step of Hierarchy Actions\n",
    "\n",
    "### < Hierarchy Actions >\n",
    "#### Action Space를 줄여서 Q-table의 크기를 줄여 학습을 빠르게 하기 위함.\n",
    "\n",
    "* Do nothing — do nothing for 3 steps\n",
    "* Build supply depot — select SCV, build supply depot, send SCV to harvest minerals\n",
    "* Build barracks — select SCV, build barracks, send SCV to harvest minerals\n",
    "* Build marine — select all barracks, train marine, do nothing\n",
    "* Attack (x, y) — select army, attack coordinates, do nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import math\n",
    "import os.path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from pysc2.agents import base_agent\n",
    "from pysc2.env import sc2_env\n",
    "from pysc2.lib import actions, features, units\n",
    "from absl import app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = 'rlagent_with_sparse_reward_learning_data'\n",
    "\n",
    "ACTION_DO_NOTHING = 'donothing'\n",
    "ACTION_BUILD_SUPPLY_DEPOT = 'buildsupplydepot'\n",
    "ACTION_BUILD_BARRACKS = 'buildbarracks'\n",
    "ACTION_BUILD_MARINE = 'buildmarine'\n",
    "ACTION_ATTACK = 'attack'\n",
    "\n",
    "smart_actions = [\n",
    "    ACTION_DO_NOTHING,\n",
    "    ACTION_BUILD_SUPPLY_DEPOT,\n",
    "    ACTION_BUILD_BARRACKS,\n",
    "    ACTION_BUILD_MARINE,\n",
    "]\n",
    "\n",
    "for mm_x in range(0, 64):\n",
    "    for mm_y in range(0, 64):\n",
    "        if (mm_x + 1) % 32 == 0 and (mm_y + 1) % 32 == 0:\n",
    "            smart_actions.append(ACTION_ATTACK + '_' + str(mm_x - 16) + '_' + str(mm_y - 16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference from https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow\n",
    "class QLearningTable:\n",
    "    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9):\n",
    "        self.actions = actions  # a list\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = reward_decay\n",
    "        self.epsilon = e_greedy\n",
    "        self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64)\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        self.check_state_exist(observation)\n",
    "        \n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            # choose best action\n",
    "            #state_action = self.q_table.ix[observation, :]\n",
    "            state_action = self.q_table.loc[observation, self.q_table.columns[:]]\n",
    "            \n",
    "            # some actions have the same value\n",
    "            state_action = state_action.reindex(np.random.permutation(state_action.index))\n",
    "            \n",
    "            action = state_action.idxmax()\n",
    "        else:\n",
    "            # choose random action\n",
    "            action = np.random.choice(self.actions)\n",
    "            \n",
    "        return action\n",
    "\n",
    "    def learn(self, s, a, r, s_):\n",
    "        self.check_state_exist(s_)\n",
    "        self.check_state_exist(s)\n",
    "        \n",
    "        #q_predict = self.q_table.ix[s, a]\n",
    "        q_predict = self.q_table.loc[s, a]\n",
    "        \n",
    "        if s_ != 'terminal':\n",
    "            #q_target = r + self.gamma * self.q_table.ix[s_, :].max()\n",
    "            q_target = r + self.gamma * self.q_table.loc[s_, self.q_table.columns[:]].max()\n",
    "        else:\n",
    "            q_target = r  # next state is terminal\n",
    "            \n",
    "        # update\n",
    "        #self.q_table.ix[s, a] += self.lr * (q_target - q_predict)\n",
    "        self.q_table.loc[s, a] += self.lr * (q_target - q_predict)\n",
    "\n",
    "    def check_state_exist(self, state):\n",
    "        if state not in self.q_table.index:\n",
    "            # append new state to q table\n",
    "            self.q_table = self.q_table.append(pd.Series([0] * len(self.actions), index=self.q_table.columns, name=state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TerranSparseRewardRLAgent(base_agent.BaseAgent):\n",
    "    def __init__(self):\n",
    "        super(TerranSparseRewardRLAgent, self).__init__()\n",
    "        \n",
    "        self.qlearn = QLearningTable(actions=list(range(len(smart_actions))))\n",
    "        \n",
    "        self.previous_action = None\n",
    "        self.previous_state = None\n",
    "        \n",
    "        self.cc_y = None\n",
    "        self.cc_x = None\n",
    "        \n",
    "        self.move_number = 0\n",
    "        \n",
    "        if os.path.isfile(DATA_FILE + '.gz'):\n",
    "            self.qlearn.q_table = pd.read_pickle(DATA_FILE + '.gz', compression='gzip')\n",
    "\n",
    "    def transformDistance(self, x, x_distance, y, y_distance):\n",
    "        if not self.base_top_left:\n",
    "            return [x - x_distance, y - y_distance]\n",
    "        \n",
    "        return [x + x_distance, y + y_distance]\n",
    "    \n",
    "    def transformLocation(self, x, y):\n",
    "        if not self.base_top_left:\n",
    "            return [64 - x, 64 - y]\n",
    "        \n",
    "        return [x, y]\n",
    "    \n",
    "    def getMeanLocation(self, unitList):\n",
    "        sum_x = 0\n",
    "        sum_y = 0\n",
    "        for unit in unitList:\n",
    "            sum_x += unit.x\n",
    "            sum_y += unit.y\n",
    "        mean_x = sum_x / len(unitList)\n",
    "        mean_y = sum_y / len(unitList)\n",
    "        \n",
    "        return [mean_x, mean_y]\n",
    "    \n",
    "    def splitAction(self, action_id):\n",
    "        smart_action = smart_actions[action_id]\n",
    "            \n",
    "        x = 0\n",
    "        y = 0\n",
    "        if '_' in smart_action:\n",
    "            smart_action, x, y = smart_action.split('_')\n",
    "\n",
    "        return (smart_action, x, y)\n",
    "    \n",
    "    def unit_type_is_selected(self, obs, unit_type):\n",
    "        if (len(obs.observation.single_select) > 0 and\n",
    "            obs.observation.single_select[0].unit_type == unit_type):\n",
    "              return True\n",
    "\n",
    "        if (len(obs.observation.multi_select) > 0 and\n",
    "            obs.observation.multi_select[0].unit_type == unit_type):\n",
    "              return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def get_units_by_type(self, obs, unit_type):\n",
    "        return [unit for unit in obs.observation.feature_units\n",
    "                if unit.unit_type == unit_type]\n",
    "\n",
    "    def can_do(self, obs, action):\n",
    "        return action in obs.observation.available_actions\n",
    "        \n",
    "    def step(self, obs):\n",
    "        super(TerranSparseRewardRLAgent, self).step(obs)\n",
    "\n",
    "        #time.sleep(0.5)\n",
    "        \n",
    "        if obs.first():\n",
    "            player_y, player_x = (obs.observation.feature_minimap.player_relative == features.PlayerRelative.SELF).nonzero()\n",
    "            self.base_top_left = 1 if player_y.any() and player_y.mean() <= 31 else 0\n",
    "        \n",
    "        ccs = self.get_units_by_type(obs, units.Terran.CommandCenter)\n",
    "        if len(ccs) > 0:\n",
    "            self.cc_x, self.cc_y = self.getMeanLocation(ccs)\n",
    "            \n",
    "        cc_count = len(ccs)\n",
    "        \n",
    "        supply_depot_count = len(self.get_units_by_type(obs, units.Terran.SupplyDepot))\n",
    "\n",
    "        barracks_count = len(self.get_units_by_type(obs, units.Terran.Barracks))\n",
    "\n",
    "        army_supply = obs.observation.player.food_used\n",
    "        \n",
    "        if self.move_number == 0:\n",
    "            self.move_number += 1\n",
    "            \n",
    "            current_state = np.zeros(8)\n",
    "            current_state[0] = cc_count\n",
    "            current_state[1] = supply_depot_count\n",
    "            current_state[2] = barracks_count\n",
    "            current_state[3] = army_supply\n",
    "    \n",
    "            hot_squares = np.zeros(4)        \n",
    "            enemy_y, enemy_x = (obs.observation.feature_minimap.player_relative == features.PlayerRelative.ENEMY).nonzero()\n",
    "            for i in range(0, len(enemy_y)):\n",
    "                y = int(math.ceil((enemy_y[i] + 1) / 32))\n",
    "                x = int(math.ceil((enemy_x[i] + 1) / 32))\n",
    "                \n",
    "                hot_squares[((y - 1) * 2) + (x - 1)] = 1\n",
    "            \n",
    "            if not self.base_top_left:\n",
    "                hot_squares = hot_squares[::-1]\n",
    "            \n",
    "            for i in range(0, 4):\n",
    "                current_state[i + 4] = hot_squares[i]\n",
    "    \n",
    "            if self.previous_action is not None:\n",
    "                self.qlearn.learn(str(self.previous_state), self.previous_action, 0, str(current_state))\n",
    "        \n",
    "            rl_action = self.qlearn.choose_action(str(current_state))\n",
    "\n",
    "            self.previous_state = current_state\n",
    "            self.previous_action = rl_action\n",
    "        \n",
    "            smart_action, x, y = self.splitAction(self.previous_action)\n",
    "            \n",
    "            if smart_action == ACTION_BUILD_BARRACKS or smart_action == ACTION_BUILD_SUPPLY_DEPOT:\n",
    "                if self.can_do(obs, actions.FUNCTIONS.select_point.id):\n",
    "                    scvs = self.get_units_by_type(obs, units.Terran.SCV)\n",
    "                    if len(scvs) > 0:\n",
    "                        scv = random.choice(scvs)\n",
    "                        if scv.x >= 0 and scv.y >= 0:\n",
    "                            return actions.FUNCTIONS.select_point(\"select\", (scv.x,\n",
    "                                                                              scv.y))\n",
    "                \n",
    "            elif smart_action == ACTION_BUILD_MARINE:\n",
    "                if self.can_do(obs, actions.FUNCTIONS.select_point.id):\n",
    "                    barracks = self.get_units_by_type(obs, units.Terran.Barracks)\n",
    "                    if len(barracks) > 0:\n",
    "                        barrack = random.choice(barracks)\n",
    "                        if barrack.x >= 0 and barrack.y >= 0:\n",
    "                            return actions.FUNCTIONS.select_point(\"select_all_type\", (barrack.x,\n",
    "                                                                                  barrack.y))\n",
    "                \n",
    "            elif smart_action == ACTION_ATTACK:\n",
    "                if self.can_do(obs, actions.FUNCTIONS.select_army.id):\n",
    "                    return actions.FUNCTIONS.select_army(\"select\")\n",
    "        \n",
    "        return actions.FUNCTIONS.no_op()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [run code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  app.run(main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Adding 2nd Step of Hierarchy Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import math\n",
    "import os.path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from pysc2.agents import base_agent\n",
    "from pysc2.env import sc2_env\n",
    "from pysc2.lib import actions, features, units\n",
    "from absl import app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = 'rlagent_with_sparse_reward_learning_data'\n",
    "\n",
    "ACTION_DO_NOTHING = 'donothing'\n",
    "ACTION_BUILD_SUPPLY_DEPOT = 'buildsupplydepot'\n",
    "ACTION_BUILD_BARRACKS = 'buildbarracks'\n",
    "ACTION_BUILD_MARINE = 'buildmarine'\n",
    "ACTION_ATTACK = 'attack'\n",
    "\n",
    "smart_actions = [\n",
    "    ACTION_DO_NOTHING,\n",
    "    ACTION_BUILD_SUPPLY_DEPOT,\n",
    "    ACTION_BUILD_BARRACKS,\n",
    "    ACTION_BUILD_MARINE,\n",
    "]\n",
    "\n",
    "for mm_x in range(0, 64):\n",
    "    for mm_y in range(0, 64):\n",
    "        if (mm_x + 1) % 32 == 0 and (mm_y + 1) % 32 == 0:\n",
    "            smart_actions.append(ACTION_ATTACK + '_' + str(mm_x - 16) + '_' + str(mm_y - 16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference from https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow\n",
    "class QLearningTable:\n",
    "    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9):\n",
    "        self.actions = actions  # a list\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = reward_decay\n",
    "        self.epsilon = e_greedy\n",
    "        self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64)\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        self.check_state_exist(observation)\n",
    "        \n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            # choose best action\n",
    "            #state_action = self.q_table.ix[observation, :]\n",
    "            state_action = self.q_table.loc[observation, self.q_table.columns[:]]\n",
    "            \n",
    "            # some actions have the same value\n",
    "            state_action = state_action.reindex(np.random.permutation(state_action.index))\n",
    "            \n",
    "            action = state_action.idxmax()\n",
    "        else:\n",
    "            # choose random action\n",
    "            action = np.random.choice(self.actions)\n",
    "            \n",
    "        return action\n",
    "\n",
    "    def learn(self, s, a, r, s_):\n",
    "        self.check_state_exist(s_)\n",
    "        self.check_state_exist(s)\n",
    "        \n",
    "        #q_predict = self.q_table.ix[s, a]\n",
    "        q_predict = self.q_table.loc[s, a]\n",
    "        \n",
    "        if s_ != 'terminal':\n",
    "            #q_target = r + self.gamma * self.q_table.ix[s_, :].max()\n",
    "            q_target = r + self.gamma * self.q_table.loc[s_, self.q_table.columns[:]].max()\n",
    "        else:\n",
    "            q_target = r  # next state is terminal\n",
    "            \n",
    "        # update\n",
    "        #self.q_table.ix[s, a] += self.lr * (q_target - q_predict)\n",
    "        self.q_table.loc[s, a] += self.lr * (q_target - q_predict)\n",
    "\n",
    "    def check_state_exist(self, state):\n",
    "        if state not in self.q_table.index:\n",
    "            # append new state to q table\n",
    "            self.q_table = self.q_table.append(pd.Series([0] * len(self.actions), index=self.q_table.columns, name=state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TerranSparseRewardRLAgent(base_agent.BaseAgent):\n",
    "    def __init__(self):\n",
    "        super(TerranSparseRewardRLAgent, self).__init__()\n",
    "        \n",
    "        self.qlearn = QLearningTable(actions=list(range(len(smart_actions))))\n",
    "        \n",
    "        self.previous_action = None\n",
    "        self.previous_state = None\n",
    "        \n",
    "        self.cc_y = None\n",
    "        self.cc_x = None\n",
    "        \n",
    "        self.move_number = 0\n",
    "        \n",
    "        if os.path.isfile(DATA_FILE + '.gz'):\n",
    "            self.qlearn.q_table = pd.read_pickle(DATA_FILE + '.gz', compression='gzip')\n",
    "\n",
    "    def transformDistance(self, x, x_distance, y, y_distance):\n",
    "        if not self.base_top_left:\n",
    "            return [x - x_distance, y - y_distance]\n",
    "        \n",
    "        return [x + x_distance, y + y_distance]\n",
    "    \n",
    "    def transformLocation(self, x, y):\n",
    "        if not self.base_top_left:\n",
    "            return [64 - x, 64 - y]\n",
    "        \n",
    "        return [x, y]\n",
    "    \n",
    "    def getMeanLocation(self, unitList):\n",
    "        sum_x = 0\n",
    "        sum_y = 0\n",
    "        for unit in unitList:\n",
    "            sum_x += unit.x\n",
    "            sum_y += unit.y\n",
    "        mean_x = sum_x / len(unitList)\n",
    "        mean_y = sum_y / len(unitList)\n",
    "        \n",
    "        return [mean_x, mean_y]\n",
    "    \n",
    "    def splitAction(self, action_id):\n",
    "        smart_action = smart_actions[action_id]\n",
    "            \n",
    "        x = 0\n",
    "        y = 0\n",
    "        if '_' in smart_action:\n",
    "            smart_action, x, y = smart_action.split('_')\n",
    "\n",
    "        return (smart_action, x, y)\n",
    "    \n",
    "    def unit_type_is_selected(self, obs, unit_type):\n",
    "        if (len(obs.observation.single_select) > 0 and\n",
    "            obs.observation.single_select[0].unit_type == unit_type):\n",
    "              return True\n",
    "\n",
    "        if (len(obs.observation.multi_select) > 0 and\n",
    "            obs.observation.multi_select[0].unit_type == unit_type):\n",
    "              return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def get_units_by_type(self, obs, unit_type):\n",
    "        return [unit for unit in obs.observation.feature_units\n",
    "                if unit.unit_type == unit_type]\n",
    "\n",
    "    def can_do(self, obs, action):\n",
    "        return action in obs.observation.available_actions\n",
    "        \n",
    "    def step(self, obs):\n",
    "        super(TerranSparseRewardRLAgent, self).step(obs)\n",
    "\n",
    "        #time.sleep(0.5)\n",
    "        \n",
    "        if obs.first():\n",
    "            player_y, player_x = (obs.observation.feature_minimap.player_relative == features.PlayerRelative.SELF).nonzero()\n",
    "            self.base_top_left = 1 if player_y.any() and player_y.mean() <= 31 else 0\n",
    "        \n",
    "        ccs = self.get_units_by_type(obs, units.Terran.CommandCenter)\n",
    "        if len(ccs) > 0:\n",
    "            self.cc_x, self.cc_y = self.getMeanLocation(ccs)\n",
    "            \n",
    "        cc_count = len(ccs)\n",
    "        \n",
    "        supply_depot_count = len(self.get_units_by_type(obs, units.Terran.SupplyDepot))\n",
    "\n",
    "        barracks_count = len(self.get_units_by_type(obs, units.Terran.Barracks))\n",
    "\n",
    "        army_supply = obs.observation.player.food_used\n",
    "        \n",
    "        if self.move_number == 0:\n",
    "            self.move_number += 1\n",
    "            \n",
    "            current_state = np.zeros(8)\n",
    "            current_state[0] = cc_count\n",
    "            current_state[1] = supply_depot_count\n",
    "            current_state[2] = barracks_count\n",
    "            current_state[3] = army_supply\n",
    "    \n",
    "            hot_squares = np.zeros(4)        \n",
    "            enemy_y, enemy_x = (obs.observation.feature_minimap.player_relative == features.PlayerRelative.ENEMY).nonzero()\n",
    "            for i in range(0, len(enemy_y)):\n",
    "                y = int(math.ceil((enemy_y[i] + 1) / 32))\n",
    "                x = int(math.ceil((enemy_x[i] + 1) / 32))\n",
    "                \n",
    "                hot_squares[((y - 1) * 2) + (x - 1)] = 1\n",
    "            \n",
    "            if not self.base_top_left:\n",
    "                hot_squares = hot_squares[::-1]\n",
    "            \n",
    "            for i in range(0, 4):\n",
    "                current_state[i + 4] = hot_squares[i]\n",
    "    \n",
    "            if self.previous_action is not None:\n",
    "                self.qlearn.learn(str(self.previous_state), self.previous_action, 0, str(current_state))\n",
    "        \n",
    "            rl_action = self.qlearn.choose_action(str(current_state))\n",
    "\n",
    "            self.previous_state = current_state\n",
    "            self.previous_action = rl_action\n",
    "        \n",
    "            smart_action, x, y = self.splitAction(self.previous_action)\n",
    "            \n",
    "            if smart_action == ACTION_BUILD_BARRACKS or smart_action == ACTION_BUILD_SUPPLY_DEPOT:\n",
    "                if self.can_do(obs, actions.FUNCTIONS.select_point.id):\n",
    "                    scvs = self.get_units_by_type(obs, units.Terran.SCV)\n",
    "                    if len(scvs) > 0:\n",
    "                        scv = random.choice(scvs)\n",
    "                        if scv.x >= 0 and scv.y >= 0:\n",
    "                            return actions.FUNCTIONS.select_point(\"select\", (scv.x,\n",
    "                                                                              scv.y))\n",
    "                \n",
    "            elif smart_action == ACTION_BUILD_MARINE:\n",
    "                if self.can_do(obs, actions.FUNCTIONS.select_point.id):\n",
    "                    barracks = self.get_units_by_type(obs, units.Terran.Barracks)\n",
    "                    if len(barracks) > 0:\n",
    "                        barrack = random.choice(barracks)\n",
    "                        if barrack.x >= 0 and barrack.y >= 0:\n",
    "                            return actions.FUNCTIONS.select_point(\"select_all_type\", (barrack.x,\n",
    "                                                                                  barrack.y))\n",
    "                \n",
    "            elif smart_action == ACTION_ATTACK:\n",
    "                if self.can_do(obs, actions.FUNCTIONS.select_army.id):\n",
    "                    return actions.FUNCTIONS.select_army(\"select\")\n",
    "                \n",
    "        elif self.move_number == 1:\n",
    "            self.move_number += 1\n",
    "            \n",
    "            smart_action, x, y = self.splitAction(self.previous_action)\n",
    "                \n",
    "            if smart_action == ACTION_BUILD_SUPPLY_DEPOT:\n",
    "                if supply_depot_count < 2 and self.can_do(obs, actions.FUNCTIONS.Build_SupplyDepot_screen.id):\n",
    "                    if len(ccs) > 0:\n",
    "                        if supply_depot_count == 0:\n",
    "                            target = self.transformDistance(self.cc_x, -35, self.cc_y, 0)\n",
    "                        elif supply_depot_count == 1:\n",
    "                            target = self.transformDistance(self.cc_x, -25, self.cc_y, -25)\n",
    "    \n",
    "                        return actions.FUNCTIONS.Build_SupplyDepot_screen(\"now\", target) \n",
    "            \n",
    "            elif smart_action == ACTION_BUILD_BARRACKS:\n",
    "                if barracks_count < 2 and self.can_do(obs, actions.FUNCTIONS.Build_Barracks_screen.id):\n",
    "                    if len(ccs) > 0:\n",
    "                        if  barracks_count == 0:\n",
    "                            target = self.transformDistance(self.cc_x, 15, self.cc_y, -9)\n",
    "                        elif  barracks_count == 1:\n",
    "                            target = self.transformDistance(self.cc_x, 15, self.cc_y, 12)\n",
    "    \n",
    "                        return actions.FUNCTIONS.Build_Barracks_screen(\"now\", target)\n",
    "    \n",
    "            elif smart_action == ACTION_BUILD_MARINE:\n",
    "                if self.can_do(obs, actions.FUNCTIONS.Train_Marine_quick.id):\n",
    "                    return actions.FUNCTIONS.Train_Marine_quick(\"queued\")\n",
    "        \n",
    "            elif smart_action == ACTION_ATTACK:\n",
    "                if self.can_do(obs, actions.FUNCTIONS.Attack_minimap.id):\n",
    "                    x_offset = random.randint(-1, 1)\n",
    "                    y_offset = random.randint(-1, 1)\n",
    "                    \n",
    "                    return actions.FUNCTIONS.Attack_minimap(\"now\", self.transformLocation(int(x) + (x_offset * 8), int(y) + (y_offset * 8)))\n",
    "            \n",
    "        return actions.FUNCTIONS.no_op()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [run code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  app.run(main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Adding 3rd Step of Hierarchy Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import math\n",
    "import os.path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from pysc2.agents import base_agent\n",
    "from pysc2.env import sc2_env\n",
    "from pysc2.lib import actions, features, units\n",
    "from absl import app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = 'rlagent_with_sparse_reward_learning_data'\n",
    "\n",
    "ACTION_DO_NOTHING = 'donothing'\n",
    "ACTION_BUILD_SUPPLY_DEPOT = 'buildsupplydepot'\n",
    "ACTION_BUILD_BARRACKS = 'buildbarracks'\n",
    "ACTION_BUILD_MARINE = 'buildmarine'\n",
    "ACTION_ATTACK = 'attack'\n",
    "\n",
    "smart_actions = [\n",
    "    ACTION_DO_NOTHING,\n",
    "    ACTION_BUILD_SUPPLY_DEPOT,\n",
    "    ACTION_BUILD_BARRACKS,\n",
    "    ACTION_BUILD_MARINE,\n",
    "]\n",
    "\n",
    "for mm_x in range(0, 64):\n",
    "    for mm_y in range(0, 64):\n",
    "        if (mm_x + 1) % 32 == 0 and (mm_y + 1) % 32 == 0:\n",
    "            smart_actions.append(ACTION_ATTACK + '_' + str(mm_x - 16) + '_' + str(mm_y - 16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference from https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow\n",
    "class QLearningTable:\n",
    "    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9):\n",
    "        self.actions = actions  # a list\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = reward_decay\n",
    "        self.epsilon = e_greedy\n",
    "        self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64)\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        self.check_state_exist(observation)\n",
    "        \n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            # choose best action\n",
    "            #state_action = self.q_table.ix[observation, :]\n",
    "            state_action = self.q_table.loc[observation, self.q_table.columns[:]]\n",
    "            \n",
    "            # some actions have the same value\n",
    "            state_action = state_action.reindex(np.random.permutation(state_action.index))\n",
    "            \n",
    "            action = state_action.idxmax()\n",
    "        else:\n",
    "            # choose random action\n",
    "            action = np.random.choice(self.actions)\n",
    "            \n",
    "        return action\n",
    "\n",
    "    def learn(self, s, a, r, s_):\n",
    "        self.check_state_exist(s_)\n",
    "        self.check_state_exist(s)\n",
    "        \n",
    "        #q_predict = self.q_table.ix[s, a]\n",
    "        q_predict = self.q_table.loc[s, a]\n",
    "        \n",
    "        if s_ != 'terminal':\n",
    "            #q_target = r + self.gamma * self.q_table.ix[s_, :].max()\n",
    "            q_target = r + self.gamma * self.q_table.loc[s_, self.q_table.columns[:]].max()\n",
    "        else:\n",
    "            q_target = r  # next state is terminal\n",
    "            \n",
    "        # update\n",
    "        #self.q_table.ix[s, a] += self.lr * (q_target - q_predict)\n",
    "        self.q_table.loc[s, a] += self.lr * (q_target - q_predict)\n",
    "\n",
    "    def check_state_exist(self, state):\n",
    "        if state not in self.q_table.index:\n",
    "            # append new state to q table\n",
    "            self.q_table = self.q_table.append(pd.Series([0] * len(self.actions), index=self.q_table.columns, name=state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TerranSparseRewardRLAgent(base_agent.BaseAgent):\n",
    "    def __init__(self):\n",
    "        super(TerranSparseRewardRLAgent, self).__init__()\n",
    "        \n",
    "        self.qlearn = QLearningTable(actions=list(range(len(smart_actions))))\n",
    "        \n",
    "        self.previous_action = None\n",
    "        self.previous_state = None\n",
    "        \n",
    "        self.cc_y = None\n",
    "        self.cc_x = None\n",
    "        \n",
    "        self.move_number = 0\n",
    "        \n",
    "        if os.path.isfile(DATA_FILE + '.gz'):\n",
    "            self.qlearn.q_table = pd.read_pickle(DATA_FILE + '.gz', compression='gzip')\n",
    "\n",
    "    def transformDistance(self, x, x_distance, y, y_distance):\n",
    "        if not self.base_top_left:\n",
    "            return [x - x_distance, y - y_distance]\n",
    "        \n",
    "        return [x + x_distance, y + y_distance]\n",
    "    \n",
    "    def transformLocation(self, x, y):\n",
    "        if not self.base_top_left:\n",
    "            return [64 - x, 64 - y]\n",
    "        \n",
    "        return [x, y]\n",
    "    \n",
    "    def getMeanLocation(self, unitList):\n",
    "        sum_x = 0\n",
    "        sum_y = 0\n",
    "        for unit in unitList:\n",
    "            sum_x += unit.x\n",
    "            sum_y += unit.y\n",
    "        mean_x = sum_x / len(unitList)\n",
    "        mean_y = sum_y / len(unitList)\n",
    "        \n",
    "        return [mean_x, mean_y]\n",
    "    \n",
    "    def splitAction(self, action_id):\n",
    "        smart_action = smart_actions[action_id]\n",
    "            \n",
    "        x = 0\n",
    "        y = 0\n",
    "        if '_' in smart_action:\n",
    "            smart_action, x, y = smart_action.split('_')\n",
    "\n",
    "        return (smart_action, x, y)\n",
    "    \n",
    "    def unit_type_is_selected(self, obs, unit_type):\n",
    "        if (len(obs.observation.single_select) > 0 and\n",
    "            obs.observation.single_select[0].unit_type == unit_type):\n",
    "              return True\n",
    "\n",
    "        if (len(obs.observation.multi_select) > 0 and\n",
    "            obs.observation.multi_select[0].unit_type == unit_type):\n",
    "              return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def get_units_by_type(self, obs, unit_type):\n",
    "        return [unit for unit in obs.observation.feature_units\n",
    "                if unit.unit_type == unit_type]\n",
    "\n",
    "    def can_do(self, obs, action):\n",
    "        return action in obs.observation.available_actions\n",
    "        \n",
    "    def step(self, obs):\n",
    "        super(TerranSparseRewardRLAgent, self).step(obs)\n",
    "\n",
    "        #time.sleep(0.5)\n",
    "        \n",
    "        if obs.first():\n",
    "            player_y, player_x = (obs.observation.feature_minimap.player_relative == features.PlayerRelative.SELF).nonzero()\n",
    "            self.base_top_left = 1 if player_y.any() and player_y.mean() <= 31 else 0\n",
    "        \n",
    "        ccs = self.get_units_by_type(obs, units.Terran.CommandCenter)\n",
    "        if len(ccs) > 0:\n",
    "            self.cc_x, self.cc_y = self.getMeanLocation(ccs)\n",
    "            \n",
    "        cc_count = len(ccs)\n",
    "        \n",
    "        supply_depot_count = len(self.get_units_by_type(obs, units.Terran.SupplyDepot))\n",
    "\n",
    "        barracks_count = len(self.get_units_by_type(obs, units.Terran.Barracks))\n",
    "\n",
    "        army_supply = obs.observation.player.food_used\n",
    "        \n",
    "        if self.move_number == 0:\n",
    "            self.move_number += 1\n",
    "            \n",
    "            current_state = np.zeros(8)\n",
    "            current_state[0] = cc_count\n",
    "            current_state[1] = supply_depot_count\n",
    "            current_state[2] = barracks_count\n",
    "            current_state[3] = army_supply\n",
    "    \n",
    "            hot_squares = np.zeros(4)        \n",
    "            enemy_y, enemy_x = (obs.observation.feature_minimap.player_relative == features.PlayerRelative.ENEMY).nonzero()\n",
    "            for i in range(0, len(enemy_y)):\n",
    "                y = int(math.ceil((enemy_y[i] + 1) / 32))\n",
    "                x = int(math.ceil((enemy_x[i] + 1) / 32))\n",
    "                \n",
    "                hot_squares[((y - 1) * 2) + (x - 1)] = 1\n",
    "            \n",
    "            if not self.base_top_left:\n",
    "                hot_squares = hot_squares[::-1]\n",
    "            \n",
    "            for i in range(0, 4):\n",
    "                current_state[i + 4] = hot_squares[i]\n",
    "    \n",
    "            if self.previous_action is not None:\n",
    "                self.qlearn.learn(str(self.previous_state), self.previous_action, 0, str(current_state))\n",
    "        \n",
    "            rl_action = self.qlearn.choose_action(str(current_state))\n",
    "\n",
    "            self.previous_state = current_state\n",
    "            self.previous_action = rl_action\n",
    "        \n",
    "            smart_action, x, y = self.splitAction(self.previous_action)\n",
    "            \n",
    "            if smart_action == ACTION_BUILD_BARRACKS or smart_action == ACTION_BUILD_SUPPLY_DEPOT:\n",
    "                if self.can_do(obs, actions.FUNCTIONS.select_point.id):\n",
    "                    scvs = self.get_units_by_type(obs, units.Terran.SCV)\n",
    "                    if len(scvs) > 0:\n",
    "                        scv = random.choice(scvs)\n",
    "                        if scv.x >= 0 and scv.y >= 0:\n",
    "                            return actions.FUNCTIONS.select_point(\"select\", (scv.x,\n",
    "                                                                              scv.y))\n",
    "                \n",
    "            elif smart_action == ACTION_BUILD_MARINE:\n",
    "                if self.can_do(obs, actions.FUNCTIONS.select_point.id):\n",
    "                    barracks = self.get_units_by_type(obs, units.Terran.Barracks)\n",
    "                    if len(barracks) > 0:\n",
    "                        barrack = random.choice(barracks)\n",
    "                        if barrack.x >= 0 and barrack.y >= 0:\n",
    "                            return actions.FUNCTIONS.select_point(\"select_all_type\", (barrack.x,\n",
    "                                                                                  barrack.y))\n",
    "                \n",
    "            elif smart_action == ACTION_ATTACK:\n",
    "                if self.can_do(obs, actions.FUNCTIONS.select_army.id):\n",
    "                    return actions.FUNCTIONS.select_army(\"select\")\n",
    "                \n",
    "        elif self.move_number == 1:\n",
    "            self.move_number += 1\n",
    "            \n",
    "            smart_action, x, y = self.splitAction(self.previous_action)\n",
    "                \n",
    "            if smart_action == ACTION_BUILD_SUPPLY_DEPOT:\n",
    "                if supply_depot_count < 2 and self.can_do(obs, actions.FUNCTIONS.Build_SupplyDepot_screen.id):\n",
    "                    if len(ccs) > 0:\n",
    "                        if supply_depot_count == 0:\n",
    "                            target = self.transformDistance(self.cc_x, -35, self.cc_y, 0)\n",
    "                        elif supply_depot_count == 1:\n",
    "                            target = self.transformDistance(self.cc_x, -25, self.cc_y, -25)\n",
    "    \n",
    "                        return actions.FUNCTIONS.Build_SupplyDepot_screen(\"now\", target) \n",
    "            \n",
    "            elif smart_action == ACTION_BUILD_BARRACKS:\n",
    "                if barracks_count < 2 and self.can_do(obs, actions.FUNCTIONS.Build_Barracks_screen.id):\n",
    "                    if len(ccs) > 0:\n",
    "                        if  barracks_count == 0:\n",
    "                            target = self.transformDistance(self.cc_x, 15, self.cc_y, -9)\n",
    "                        elif  barracks_count == 1:\n",
    "                            target = self.transformDistance(self.cc_x, 15, self.cc_y, 12)\n",
    "    \n",
    "                        return actions.FUNCTIONS.Build_Barracks_screen(\"now\", target)\n",
    "    \n",
    "            elif smart_action == ACTION_BUILD_MARINE:\n",
    "                if self.can_do(obs, actions.FUNCTIONS.Train_Marine_quick.id):\n",
    "                    return actions.FUNCTIONS.Train_Marine_quick(\"queued\")\n",
    "        \n",
    "            elif smart_action == ACTION_ATTACK:\n",
    "                if self.can_do(obs, actions.FUNCTIONS.Attack_minimap.id):\n",
    "                    x_offset = random.randint(-1, 1)\n",
    "                    y_offset = random.randint(-1, 1)\n",
    "                    \n",
    "                    return actions.FUNCTIONS.Attack_minimap(\"now\", self.transformLocation(int(x) + (x_offset * 8), int(y) + (y_offset * 8)))\n",
    "            \n",
    "        elif self.move_number == 2:\n",
    "            self.move_number = 0\n",
    "            \n",
    "            smart_action, x, y = self.splitAction(self.previous_action)\n",
    "                \n",
    "            if smart_action == ACTION_BUILD_BARRACKS or smart_action == ACTION_BUILD_SUPPLY_DEPOT:\n",
    "                if self.can_do(obs, actions.FUNCTIONS.Harvest_Gather_screen.id):\n",
    "                    mfs = self.get_units_by_type(obs, units.Neutral.MineralField)\n",
    "                    if len(mfs) > 0:\n",
    "                        mf = random.choice(mfs)\n",
    "                        if mf.x >= 0 and mf.y >= 0:\n",
    "                            return actions.FUNCTIONS.Harvest_Gather_screen(\"now\", (mf.x,mf.y))\n",
    "            \n",
    "        return actions.FUNCTIONS.no_op()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [run code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  app.run(main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Detecting End of Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import math\n",
    "import os.path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from pysc2.agents import base_agent\n",
    "from pysc2.env import sc2_env\n",
    "from pysc2.lib import actions, features, units\n",
    "from absl import app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = 'rlagent_with_sparse_reward_learning_data'\n",
    "\n",
    "ACTION_DO_NOTHING = 'donothing'\n",
    "ACTION_BUILD_SUPPLY_DEPOT = 'buildsupplydepot'\n",
    "ACTION_BUILD_BARRACKS = 'buildbarracks'\n",
    "ACTION_BUILD_MARINE = 'buildmarine'\n",
    "ACTION_ATTACK = 'attack'\n",
    "\n",
    "smart_actions = [\n",
    "    ACTION_DO_NOTHING,\n",
    "    ACTION_BUILD_SUPPLY_DEPOT,\n",
    "    ACTION_BUILD_BARRACKS,\n",
    "    ACTION_BUILD_MARINE,\n",
    "]\n",
    "\n",
    "for mm_x in range(0, 64):\n",
    "    for mm_y in range(0, 64):\n",
    "        if (mm_x + 1) % 32 == 0 and (mm_y + 1) % 32 == 0:\n",
    "            smart_actions.append(ACTION_ATTACK + '_' + str(mm_x - 16) + '_' + str(mm_y - 16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference from https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow\n",
    "class QLearningTable:\n",
    "    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9):\n",
    "        self.actions = actions  # a list\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = reward_decay\n",
    "        self.epsilon = e_greedy\n",
    "        self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64)\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        self.check_state_exist(observation)\n",
    "        \n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            # choose best action\n",
    "            #state_action = self.q_table.ix[observation, :]\n",
    "            state_action = self.q_table.loc[observation, self.q_table.columns[:]]\n",
    "            \n",
    "            # some actions have the same value\n",
    "            state_action = state_action.reindex(np.random.permutation(state_action.index))\n",
    "            \n",
    "            action = state_action.idxmax()\n",
    "        else:\n",
    "            # choose random action\n",
    "            action = np.random.choice(self.actions)\n",
    "            \n",
    "        return action\n",
    "\n",
    "    def learn(self, s, a, r, s_):\n",
    "        self.check_state_exist(s_)\n",
    "        self.check_state_exist(s)\n",
    "        \n",
    "        #q_predict = self.q_table.ix[s, a]\n",
    "        q_predict = self.q_table.loc[s, a]\n",
    "        \n",
    "        if s_ != 'terminal':\n",
    "            #q_target = r + self.gamma * self.q_table.ix[s_, :].max()\n",
    "            q_target = r + self.gamma * self.q_table.loc[s_, self.q_table.columns[:]].max()\n",
    "        else:\n",
    "            q_target = r  # next state is terminal\n",
    "            \n",
    "        # update\n",
    "        #self.q_table.ix[s, a] += self.lr * (q_target - q_predict)\n",
    "        self.q_table.loc[s, a] += self.lr * (q_target - q_predict)\n",
    "\n",
    "    def check_state_exist(self, state):\n",
    "        if state not in self.q_table.index:\n",
    "            # append new state to q table\n",
    "            self.q_table = self.q_table.append(pd.Series([0] * len(self.actions), index=self.q_table.columns, name=state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TerranSparseRewardRLAgent(base_agent.BaseAgent):\n",
    "    def __init__(self):\n",
    "        super(TerranSparseRewardRLAgent, self).__init__()\n",
    "        \n",
    "        self.qlearn = QLearningTable(actions=list(range(len(smart_actions))))\n",
    "        \n",
    "        self.previous_action = None\n",
    "        self.previous_state = None\n",
    "        \n",
    "        self.cc_y = None\n",
    "        self.cc_x = None\n",
    "        \n",
    "        self.move_number = 0\n",
    "        \n",
    "        if os.path.isfile(DATA_FILE + '.gz'):\n",
    "            self.qlearn.q_table = pd.read_pickle(DATA_FILE + '.gz', compression='gzip')\n",
    "\n",
    "    def transformDistance(self, x, x_distance, y, y_distance):\n",
    "        if not self.base_top_left:\n",
    "            return [x - x_distance, y - y_distance]\n",
    "        \n",
    "        return [x + x_distance, y + y_distance]\n",
    "    \n",
    "    def transformLocation(self, x, y):\n",
    "        if not self.base_top_left:\n",
    "            return [64 - x, 64 - y]\n",
    "        \n",
    "        return [x, y]\n",
    "    \n",
    "    def getMeanLocation(self, unitList):\n",
    "        sum_x = 0\n",
    "        sum_y = 0\n",
    "        for unit in unitList:\n",
    "            sum_x += unit.x\n",
    "            sum_y += unit.y\n",
    "        mean_x = sum_x / len(unitList)\n",
    "        mean_y = sum_y / len(unitList)\n",
    "        \n",
    "        return [mean_x, mean_y]\n",
    "    \n",
    "    def splitAction(self, action_id):\n",
    "        smart_action = smart_actions[action_id]\n",
    "            \n",
    "        x = 0\n",
    "        y = 0\n",
    "        if '_' in smart_action:\n",
    "            smart_action, x, y = smart_action.split('_')\n",
    "\n",
    "        return (smart_action, x, y)\n",
    "    \n",
    "    def unit_type_is_selected(self, obs, unit_type):\n",
    "        if (len(obs.observation.single_select) > 0 and\n",
    "            obs.observation.single_select[0].unit_type == unit_type):\n",
    "              return True\n",
    "\n",
    "        if (len(obs.observation.multi_select) > 0 and\n",
    "            obs.observation.multi_select[0].unit_type == unit_type):\n",
    "              return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def get_units_by_type(self, obs, unit_type):\n",
    "        return [unit for unit in obs.observation.feature_units\n",
    "                if unit.unit_type == unit_type]\n",
    "\n",
    "    def can_do(self, obs, action):\n",
    "        return action in obs.observation.available_actions\n",
    "        \n",
    "    def step(self, obs):\n",
    "        super(TerranSparseRewardRLAgent, self).step(obs)\n",
    "\n",
    "        #time.sleep(0.5)\n",
    "        \n",
    "        if obs.last():\n",
    "            reward = obs.reward\n",
    "        \n",
    "            self.qlearn.learn(str(self.previous_state), self.previous_action, reward, 'terminal')\n",
    "            \n",
    "            self.qlearn.q_table.to_pickle(DATA_FILE + '.gz', 'gzip')\n",
    "            \n",
    "            self.previous_action = None\n",
    "            self.previous_state = None\n",
    "            \n",
    "            self.move_number = 0\n",
    "            \n",
    "            return actions.FUNCTIONS.no_op()\n",
    "        \n",
    "        if obs.first():\n",
    "            player_y, player_x = (obs.observation.feature_minimap.player_relative == features.PlayerRelative.SELF).nonzero()\n",
    "            self.base_top_left = 1 if player_y.any() and player_y.mean() <= 31 else 0\n",
    "        \n",
    "        ccs = self.get_units_by_type(obs, units.Terran.CommandCenter)\n",
    "        if len(ccs) > 0:\n",
    "            self.cc_x, self.cc_y = self.getMeanLocation(ccs)\n",
    "            \n",
    "        cc_count = len(ccs)\n",
    "        \n",
    "        supply_depot_count = len(self.get_units_by_type(obs, units.Terran.SupplyDepot))\n",
    "\n",
    "        barracks_count = len(self.get_units_by_type(obs, units.Terran.Barracks))\n",
    "\n",
    "        army_supply = obs.observation.player.food_used\n",
    "        \n",
    "        if self.move_number == 0:\n",
    "            self.move_number += 1\n",
    "            \n",
    "            current_state = np.zeros(8)\n",
    "            current_state[0] = cc_count\n",
    "            current_state[1] = supply_depot_count\n",
    "            current_state[2] = barracks_count\n",
    "            current_state[3] = army_supply\n",
    "    \n",
    "            hot_squares = np.zeros(4)        \n",
    "            enemy_y, enemy_x = (obs.observation.feature_minimap.player_relative == features.PlayerRelative.ENEMY).nonzero()\n",
    "            for i in range(0, len(enemy_y)):\n",
    "                y = int(math.ceil((enemy_y[i] + 1) / 32))\n",
    "                x = int(math.ceil((enemy_x[i] + 1) / 32))\n",
    "                \n",
    "                hot_squares[((y - 1) * 2) + (x - 1)] = 1\n",
    "            \n",
    "            if not self.base_top_left:\n",
    "                hot_squares = hot_squares[::-1]\n",
    "            \n",
    "            for i in range(0, 4):\n",
    "                current_state[i + 4] = hot_squares[i]\n",
    "    \n",
    "            if self.previous_action is not None:\n",
    "                self.qlearn.learn(str(self.previous_state), self.previous_action, 0, str(current_state))\n",
    "        \n",
    "            rl_action = self.qlearn.choose_action(str(current_state))\n",
    "\n",
    "            self.previous_state = current_state\n",
    "            self.previous_action = rl_action\n",
    "        \n",
    "            smart_action, x, y = self.splitAction(self.previous_action)\n",
    "            \n",
    "            if smart_action == ACTION_BUILD_BARRACKS or smart_action == ACTION_BUILD_SUPPLY_DEPOT:\n",
    "                if self.can_do(obs, actions.FUNCTIONS.select_point.id):\n",
    "                    scvs = self.get_units_by_type(obs, units.Terran.SCV)\n",
    "                    if len(scvs) > 0:\n",
    "                        scv = random.choice(scvs)\n",
    "                        if scv.x >= 0 and scv.y >= 0:\n",
    "                            return actions.FUNCTIONS.select_point(\"select\", (scv.x,\n",
    "                                                                              scv.y))\n",
    "                \n",
    "            elif smart_action == ACTION_BUILD_MARINE:\n",
    "                if self.can_do(obs, actions.FUNCTIONS.select_point.id):\n",
    "                    barracks = self.get_units_by_type(obs, units.Terran.Barracks)\n",
    "                    if len(barracks) > 0:\n",
    "                        barrack = random.choice(barracks)\n",
    "                        if barrack.x >= 0 and barrack.y >= 0:\n",
    "                            return actions.FUNCTIONS.select_point(\"select_all_type\", (barrack.x,\n",
    "                                                                                  barrack.y))\n",
    "                \n",
    "            elif smart_action == ACTION_ATTACK:\n",
    "                if self.can_do(obs, actions.FUNCTIONS.select_army.id):\n",
    "                    return actions.FUNCTIONS.select_army(\"select\")\n",
    "                \n",
    "        elif self.move_number == 1:\n",
    "            self.move_number += 1\n",
    "            \n",
    "            smart_action, x, y = self.splitAction(self.previous_action)\n",
    "                \n",
    "            if smart_action == ACTION_BUILD_SUPPLY_DEPOT:\n",
    "                if supply_depot_count < 2 and self.can_do(obs, actions.FUNCTIONS.Build_SupplyDepot_screen.id):\n",
    "                    if len(ccs) > 0:\n",
    "                        if supply_depot_count == 0:\n",
    "                            target = self.transformDistance(self.cc_x, -35, self.cc_y, 0)\n",
    "                        elif supply_depot_count == 1:\n",
    "                            target = self.transformDistance(self.cc_x, -25, self.cc_y, -25)\n",
    "    \n",
    "                        return actions.FUNCTIONS.Build_SupplyDepot_screen(\"now\", target) \n",
    "            \n",
    "            elif smart_action == ACTION_BUILD_BARRACKS:\n",
    "                if barracks_count < 2 and self.can_do(obs, actions.FUNCTIONS.Build_Barracks_screen.id):\n",
    "                    if len(ccs) > 0:\n",
    "                        if  barracks_count == 0:\n",
    "                            target = self.transformDistance(self.cc_x, 15, self.cc_y, -9)\n",
    "                        elif  barracks_count == 1:\n",
    "                            target = self.transformDistance(self.cc_x, 15, self.cc_y, 12)\n",
    "    \n",
    "                        return actions.FUNCTIONS.Build_Barracks_screen(\"now\", target)\n",
    "    \n",
    "            elif smart_action == ACTION_BUILD_MARINE:\n",
    "                if self.can_do(obs, actions.FUNCTIONS.Train_Marine_quick.id):\n",
    "                    return actions.FUNCTIONS.Train_Marine_quick(\"queued\")\n",
    "        \n",
    "            elif smart_action == ACTION_ATTACK:\n",
    "                if self.can_do(obs, actions.FUNCTIONS.Attack_minimap.id):\n",
    "                    x_offset = random.randint(-1, 1)\n",
    "                    y_offset = random.randint(-1, 1)\n",
    "                    \n",
    "                    return actions.FUNCTIONS.Attack_minimap(\"now\", self.transformLocation(int(x) + (x_offset * 8), int(y) + (y_offset * 8)))\n",
    "            \n",
    "        elif self.move_number == 2:\n",
    "            self.move_number = 0\n",
    "            \n",
    "            smart_action, x, y = self.splitAction(self.previous_action)\n",
    "                \n",
    "            if smart_action == ACTION_BUILD_BARRACKS or smart_action == ACTION_BUILD_SUPPLY_DEPOT:\n",
    "                if self.can_do(obs, actions.FUNCTIONS.Harvest_Gather_screen.id):\n",
    "                    mfs = self.get_units_by_type(obs, units.Neutral.MineralField)\n",
    "                    if len(mfs) > 0:\n",
    "                        mf = random.choice(mfs)\n",
    "                        if mf.x >= 0 and mf.y >= 0:\n",
    "                            return actions.FUNCTIONS.Harvest_Gather_screen(\"queued\", (mf.x,mf.y))\n",
    "            \n",
    "        return actions.FUNCTIONS.no_op()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [run code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  app.run(main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Refining \n",
    "- Ignoreing Learing When State Does Not Change : 'Maximization Bias in RL' Problem\n",
    "- Preventing Invalid Actions\n",
    "- Adding Our Unit Locations to the State\n",
    "\n",
    "![Winning rate graph](./images/rlagent_with_sparse_reward_learning_scoreTerran-Terran-350_Eps.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import math\n",
    "import os.path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import deque\n",
    "import pickle\n",
    "\n",
    "from pysc2.agents import base_agent\n",
    "from pysc2.env import sc2_env\n",
    "from pysc2.lib import actions, features, units\n",
    "from absl import app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = 'rlagent_with_sparse_reward_learning_data'\n",
    "SCORE_FILE = 'rlagent_with_sparse_reward_learning_score'\n",
    "\n",
    "ACTION_DO_NOTHING = 'donothing'\n",
    "ACTION_BUILD_SUPPLY_DEPOT = 'buildsupplydepot'\n",
    "ACTION_BUILD_BARRACKS = 'buildbarracks'\n",
    "ACTION_BUILD_MARINE = 'buildmarine'\n",
    "ACTION_ATTACK = 'attack'\n",
    "\n",
    "smart_actions = [\n",
    "    ACTION_DO_NOTHING,\n",
    "    ACTION_BUILD_SUPPLY_DEPOT,\n",
    "    ACTION_BUILD_BARRACKS,\n",
    "    ACTION_BUILD_MARINE,\n",
    "]\n",
    "\n",
    "for mm_x in range(0, 64):\n",
    "    for mm_y in range(0, 64):\n",
    "        if (mm_x + 1) % 32 == 0 and (mm_y + 1) % 32 == 0:\n",
    "            smart_actions.append(ACTION_ATTACK + '_' + str(mm_x - 16) + '_' + str(mm_y - 16))\n",
    "            \n",
    "scores = []                        # list containing scores from each episode\n",
    "scores_window = deque(maxlen=100)  # last 100 scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 첫째, Ignoreing Learing When State Does Not Change : 'Maximization Bias in RL' Problem 해결 방법\n",
    "#### 학습의 초기과정에서 같은 State에 자주 도착하는 경우 덜 가치있는 Action을 가장 가치있는 Action으로 밀어 붙이는 경향이 생깁니다. 이 것이 반복되면 시간이 지남에 따라 완전히 다른 State 대신 동일한 State에 더 자주 도착하면 모든 보상이 0에 가까워 지는 문제가 생긴다.\n",
    "\n",
    "#### 이를 해결하기 위해 아래와 같이 간단한 방법을 추가함.\n",
    "\n",
    "    def learn(self, s, a, r, s_):\n",
    "        if s == s_:\n",
    "            return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 둘째, Preventing Invalid Actions\n",
    "\n",
    "< Invalid Actions 의 예>\n",
    "- supply depot 제한 갯수 2에 도달했거나 supply depot를 건설할 SCV가 없는 경우 에이전트가 supply depot를 건설 할 수 없도로 한다.\n",
    "- supply depot가 없거나 barrack 제한 갯수 2에 도달했거나 barrack 제한 갯수를 지을 SCV이 없다면 에이전트가 barrack를 지을 수 없도록 한다.\n",
    "- barrack가 없거나 supply limit에 도달한 경우 marine을 훈련시키지 않는다.\n",
    "\n",
    "#### 학습과정에서 에이전트가 Invalid Actions을 시도하는 것이 반복적으로 관찰됩니다. 사용가능한 Action의 수가 Available Actions 절반인 경우가 많기 때문에 에이전트는 불필요한 Action에서 학습하는 데 시간을 많이 보내게 됩니다.\n",
    "#### 이러한 Invalid Actions을 필터링하여 에이전트가 State 변경으로 이어지는 Action을 시도하는 데 집중하도록하여 Exploration을 줄이고 학습 시간을 개선 할 수 있습니다.\n",
    "\n",
    "#### 이를 해결하기 위해 아래와 같이 self.disallowed_actions 와 excluded_action 을 활용하는 방법을 추가함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 셋째, Adding Our Unit Locations to the State\n",
    "\n",
    "#### \"아군이 어디에 있는지 모른다면 어떤 위치가 공격하기에 가장 좋은지 알 수 있지 않을까요?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference from https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow\n",
    "class QLearningTable:\n",
    "    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9):\n",
    "        self.actions = actions  # a list\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = reward_decay\n",
    "        self.epsilon = e_greedy\n",
    "        self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64)\n",
    "        self.disallowed_actions = {}\n",
    "        \n",
    "    def choose_action(self, observation, excluded_actions=[]):\n",
    "        self.check_state_exist(observation)\n",
    "        \n",
    "        self.disallowed_actions[observation] = excluded_actions\n",
    "        \n",
    "        #state_action = self.q_table.ix[observation, :]\n",
    "        #state_action = self.q_table.loc[observation, self.q_table.columns[:]]\n",
    "        state_action = self.q_table.loc[observation, :]\n",
    "        \n",
    "        for excluded_action in excluded_actions:\n",
    "            del state_action[excluded_action]\n",
    "        \n",
    "        if np.random.uniform() < self.epsilon:            \n",
    "            # some actions have the same value\n",
    "            state_action = state_action.reindex(np.random.permutation(state_action.index))\n",
    "            \n",
    "            action = state_action.idxmax()\n",
    "        else:\n",
    "            # choose random action\n",
    "            action = np.random.choice(state_action.index)\n",
    "            \n",
    "        return action\n",
    "\n",
    "    def learn(self, s, a, r, s_):\n",
    "        if s == s_:\n",
    "            return\n",
    "        \n",
    "        self.check_state_exist(s_)\n",
    "        self.check_state_exist(s)\n",
    "        \n",
    "        #q_predict = self.q_table.ix[s, a]\n",
    "        q_predict = self.q_table.loc[s, a]\n",
    "        \n",
    "        #s_rewards = self.q_table.ix[s_, :]\n",
    "        #s_rewards = self.q_table.loc[s_, self.q_table.columns[:]]\n",
    "        s_rewards = self.q_table.loc[s_, :]\n",
    "        \n",
    "        if s_ in self.disallowed_actions:\n",
    "            for excluded_action in self.disallowed_actions[s_]:\n",
    "                del s_rewards[excluded_action]\n",
    "        \n",
    "        if s_ != 'terminal':\n",
    "            q_target = r + self.gamma * s_rewards.max()\n",
    "        else:\n",
    "            q_target = r  # next state is terminal\n",
    "            \n",
    "        # update\n",
    "        #self.q_table.ix[s, a] += self.lr * (q_target - q_predict)\n",
    "        self.q_table.loc[s, a] += self.lr * (q_target - q_predict)\n",
    "\n",
    "    def check_state_exist(self, state):\n",
    "        if state not in self.q_table.index:\n",
    "            # append new state to q table\n",
    "            self.q_table = self.q_table.append(pd.Series([0] * len(self.actions), index=self.q_table.columns, name=state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TerranSparseRewardRLAgent(base_agent.BaseAgent):\n",
    "    def __init__(self):\n",
    "        super(TerranSparseRewardRLAgent, self).__init__()\n",
    "        \n",
    "        self.qlearn = QLearningTable(actions=list(range(len(smart_actions))))\n",
    "        \n",
    "        self.previous_action = None\n",
    "        self.previous_state = None\n",
    "        \n",
    "        self.cc_y = None\n",
    "        self.cc_x = None\n",
    "        \n",
    "        self.move_number = 0\n",
    "        \n",
    "        if os.path.isfile(DATA_FILE + '.gz'):\n",
    "            self.qlearn.q_table = pd.read_pickle(DATA_FILE + '.gz', compression='gzip')\n",
    "\n",
    "    def transformDistance(self, x, x_distance, y, y_distance):\n",
    "        if not self.base_top_left:\n",
    "            return [x - x_distance, y - y_distance]\n",
    "        \n",
    "        return [x + x_distance, y + y_distance]\n",
    "    \n",
    "    def transformLocation(self, x, y):\n",
    "        if not self.base_top_left:\n",
    "            return [64 - x, 64 - y]\n",
    "        \n",
    "        return [x, y]\n",
    "    \n",
    "    def getMeanLocation(self, unitList):\n",
    "        sum_x = 0\n",
    "        sum_y = 0\n",
    "        for unit in unitList:\n",
    "            sum_x += unit.x\n",
    "            sum_y += unit.y\n",
    "        mean_x = sum_x / len(unitList)\n",
    "        mean_y = sum_y / len(unitList)\n",
    "        \n",
    "        return [mean_x, mean_y]\n",
    "    \n",
    "    def splitAction(self, action_id):\n",
    "        smart_action = smart_actions[action_id]\n",
    "            \n",
    "        x = 0\n",
    "        y = 0\n",
    "        if '_' in smart_action:\n",
    "            smart_action, x, y = smart_action.split('_')\n",
    "\n",
    "        return (smart_action, x, y)\n",
    "    \n",
    "    def unit_type_is_selected(self, obs, unit_type):\n",
    "        if (len(obs.observation.single_select) > 0 and\n",
    "            obs.observation.single_select[0].unit_type == unit_type):\n",
    "              return True\n",
    "\n",
    "        if (len(obs.observation.multi_select) > 0 and\n",
    "            obs.observation.multi_select[0].unit_type == unit_type):\n",
    "              return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def get_units_by_type(self, obs, unit_type):\n",
    "        return [unit for unit in obs.observation.feature_units\n",
    "                if unit.unit_type == unit_type]\n",
    "\n",
    "    def can_do(self, obs, action):\n",
    "        return action in obs.observation.available_actions\n",
    "        \n",
    "    def step(self, obs):\n",
    "        super(TerranSparseRewardRLAgent, self).step(obs)\n",
    "\n",
    "        #time.sleep(0.5)\n",
    "        \n",
    "        if obs.last():\n",
    "            reward = obs.reward\n",
    "        \n",
    "            self.qlearn.learn(str(self.previous_state), self.previous_action, reward, 'terminal')\n",
    "            \n",
    "            self.qlearn.q_table.to_pickle(DATA_FILE + '.gz', 'gzip')\n",
    "            \n",
    "            scores_window.append(obs.reward)  # save most recent reward\n",
    "            win_rate = scores_window.count(1)/len(scores_window)*100\n",
    "            tie_rate = scores_window.count(0)/len(scores_window)*100\n",
    "            lost_rate = scores_window.count(-1)/len(scores_window)*100\n",
    "            \n",
    "            scores.append([win_rate, tie_rate, lost_rate])  # save most recent score(win_rate, tie_rate, lost_rate)\n",
    "            with open(SCORE_FILE + '.txt', \"wb\") as fp:\n",
    "                pickle.dump(scores, fp)\n",
    "            \n",
    "            self.previous_action = None\n",
    "            self.previous_state = None\n",
    "            \n",
    "            self.move_number = 0\n",
    "            \n",
    "            return actions.FUNCTIONS.no_op()\n",
    "        \n",
    "        if obs.first():\n",
    "            player_y, player_x = (obs.observation.feature_minimap.player_relative == features.PlayerRelative.SELF).nonzero()\n",
    "            self.base_top_left = 1 if player_y.any() and player_y.mean() <= 31 else 0\n",
    "        \n",
    "        ccs = self.get_units_by_type(obs, units.Terran.CommandCenter)\n",
    "        if len(ccs) > 0:\n",
    "            self.cc_x, self.cc_y = self.getMeanLocation(ccs)\n",
    "            \n",
    "        cc_count = len(ccs)\n",
    "        \n",
    "        supply_depot_count = len(self.get_units_by_type(obs, units.Terran.SupplyDepot))\n",
    "\n",
    "        barracks_count = len(self.get_units_by_type(obs, units.Terran.Barracks))\n",
    "        \n",
    "        supply_used = obs.observation.player.food_used\n",
    "        supply_limit = obs.observation.player.food_cap\n",
    "        army_supply = obs.observation.player.food_army\n",
    "        worker_supply = obs.observation.player.food_workers\n",
    "        \n",
    "        supply_free = supply_limit - supply_used\n",
    "        \n",
    "        if self.move_number == 0:\n",
    "            self.move_number += 1\n",
    "            \n",
    "            \n",
    "            current_state = np.zeros(12)\n",
    "            current_state[0] = cc_count\n",
    "            current_state[1] = supply_depot_count\n",
    "            current_state[2] = barracks_count\n",
    "            current_state[3] = army_supply\n",
    "    \n",
    "            hot_squares = np.zeros(4)        \n",
    "            enemy_y, enemy_x = (obs.observation.feature_minimap.player_relative == features.PlayerRelative.ENEMY).nonzero()\n",
    "            for i in range(0, len(enemy_y)):\n",
    "                y = int(math.ceil((enemy_y[i] + 1) / 32))\n",
    "                x = int(math.ceil((enemy_x[i] + 1) / 32))\n",
    "                \n",
    "                hot_squares[((y - 1) * 2) + (x - 1)] = 1\n",
    "            \n",
    "            if not self.base_top_left:\n",
    "                hot_squares = hot_squares[::-1]\n",
    "            \n",
    "            for i in range(0, 4):\n",
    "                current_state[i + 4] = hot_squares[i]\n",
    "                \n",
    "            green_squares = np.zeros(4)        \n",
    "            friendly_y, friendly_x = (obs.observation.feature_minimap.player_relative == features.PlayerRelative.SELF).nonzero()\n",
    "            for i in range(0, len(friendly_y)):\n",
    "                y = int(math.ceil((friendly_y[i] + 1) / 32))\n",
    "                x = int(math.ceil((friendly_x[i] + 1) / 32))\n",
    "                \n",
    "                green_squares[((y - 1) * 2) + (x - 1)] = 1\n",
    "            \n",
    "            if not self.base_top_left:\n",
    "                green_squares = green_squares[::-1]\n",
    "            \n",
    "            for i in range(0, 4):\n",
    "                current_state[i + 8] = green_squares[i]\n",
    "    \n",
    "            if self.previous_action is not None:\n",
    "                self.qlearn.learn(str(self.previous_state), self.previous_action, 0, str(current_state))\n",
    "        \n",
    "            excluded_actions = []\n",
    "            if supply_depot_count == 2 or worker_supply == 0:\n",
    "                excluded_actions.append(1)\n",
    "                \n",
    "            if supply_depot_count == 0 or barracks_count == 2 or worker_supply == 0:\n",
    "                excluded_actions.append(2)\n",
    "\n",
    "            if supply_free == 0 or barracks_count == 0:\n",
    "                excluded_actions.append(3)\n",
    "                \n",
    "            if army_supply == 0:\n",
    "                excluded_actions.append(4)\n",
    "                excluded_actions.append(5)\n",
    "                excluded_actions.append(6)\n",
    "                excluded_actions.append(7)\n",
    "        \n",
    "            rl_action = self.qlearn.choose_action(str(current_state), excluded_actions)\n",
    "\n",
    "            self.previous_state = current_state\n",
    "            self.previous_action = rl_action\n",
    "        \n",
    "            smart_action, x, y = self.splitAction(self.previous_action)\n",
    "            \n",
    "            if smart_action == ACTION_BUILD_BARRACKS or smart_action == ACTION_BUILD_SUPPLY_DEPOT:\n",
    "                if self.can_do(obs, actions.FUNCTIONS.select_point.id):\n",
    "                    scvs = self.get_units_by_type(obs, units.Terran.SCV)\n",
    "                    if len(scvs) > 0:\n",
    "                        scv = random.choice(scvs)\n",
    "                        if scv.x >= 0 and scv.y >= 0:\n",
    "                            return actions.FUNCTIONS.select_point(\"select\", (scv.x,\n",
    "                                                                              scv.y))\n",
    "                \n",
    "            elif smart_action == ACTION_BUILD_MARINE:\n",
    "                if self.can_do(obs, actions.FUNCTIONS.select_point.id):\n",
    "                    barracks = self.get_units_by_type(obs, units.Terran.Barracks)\n",
    "                    if len(barracks) > 0:\n",
    "                        barrack = random.choice(barracks)\n",
    "                        if barrack.x >= 0 and barrack.y >= 0:\n",
    "                            return actions.FUNCTIONS.select_point(\"select_all_type\", (barrack.x,\n",
    "                                                                                  barrack.y))\n",
    "                \n",
    "            elif smart_action == ACTION_ATTACK:\n",
    "                if self.can_do(obs, actions.FUNCTIONS.select_army.id):\n",
    "                    return actions.FUNCTIONS.select_army(\"select\")\n",
    "                \n",
    "        elif self.move_number == 1:\n",
    "            self.move_number += 1\n",
    "            \n",
    "            smart_action, x, y = self.splitAction(self.previous_action)\n",
    "                \n",
    "            if smart_action == ACTION_BUILD_SUPPLY_DEPOT:\n",
    "                if supply_depot_count < 2 and self.can_do(obs, actions.FUNCTIONS.Build_SupplyDepot_screen.id):\n",
    "                    if len(ccs) > 0:\n",
    "                        if supply_depot_count == 0:\n",
    "                            target = self.transformDistance(self.cc_x, -35, self.cc_y, 0)\n",
    "                        elif supply_depot_count == 1:\n",
    "                            target = self.transformDistance(self.cc_x, -25, self.cc_y, -25)\n",
    "    \n",
    "                        return actions.FUNCTIONS.Build_SupplyDepot_screen(\"now\", target) \n",
    "            \n",
    "            elif smart_action == ACTION_BUILD_BARRACKS:\n",
    "                if barracks_count < 2 and self.can_do(obs, actions.FUNCTIONS.Build_Barracks_screen.id):\n",
    "                    if len(ccs) > 0:\n",
    "                        if  barracks_count == 0:\n",
    "                            target = self.transformDistance(self.cc_x, 15, self.cc_y, -9)\n",
    "                        elif  barracks_count == 1:\n",
    "                            target = self.transformDistance(self.cc_x, 15, self.cc_y, 12)\n",
    "    \n",
    "                        return actions.FUNCTIONS.Build_Barracks_screen(\"now\", target)\n",
    "    \n",
    "            elif smart_action == ACTION_BUILD_MARINE:\n",
    "                if self.can_do(obs, actions.FUNCTIONS.Train_Marine_quick.id):\n",
    "                    return actions.FUNCTIONS.Train_Marine_quick(\"queued\")\n",
    "        \n",
    "            elif smart_action == ACTION_ATTACK:\n",
    "                if self.can_do(obs, actions.FUNCTIONS.Attack_minimap.id):\n",
    "                    x_offset = random.randint(-1, 1)\n",
    "                    y_offset = random.randint(-1, 1)\n",
    "                    \n",
    "                    return actions.FUNCTIONS.Attack_minimap(\"now\", self.transformLocation(int(x) + (x_offset * 8), int(y) + (y_offset * 8)))\n",
    "            \n",
    "        elif self.move_number == 2:\n",
    "            self.move_number = 0\n",
    "            \n",
    "            smart_action, x, y = self.splitAction(self.previous_action)\n",
    "                \n",
    "            if smart_action == ACTION_BUILD_BARRACKS or smart_action == ACTION_BUILD_SUPPLY_DEPOT:\n",
    "                if self.can_do(obs, actions.FUNCTIONS.Harvest_Gather_screen.id):\n",
    "                    mfs = self.get_units_by_type(obs, units.Neutral.MineralField)\n",
    "                    if len(mfs) > 0:\n",
    "                        mf = random.choice(mfs)\n",
    "                        if mf.x >= 0 and mf.y >= 0:\n",
    "                            return actions.FUNCTIONS.Harvest_Gather_screen(\"queued\", (mf.x,mf.y))\n",
    "            \n",
    "        return actions.FUNCTIONS.no_op()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [run code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0908 22:54:34.908689 4549250496 sc_process.py:135] Launching SC2: /Applications/StarCraft II/Versions/Base81102/SC2.app/Contents/MacOS/SC2 -listen 127.0.0.1 -port 24748 -dataDir /Applications/StarCraft II/ -tempDir /var/folders/r1/x6k135_915z463fc7lc4hkp40000gn/T/sc-s__1457n/ -displayMode 0 -windowwidth 640 -windowheight 480 -windowx 50 -windowy 50\n",
      "I0908 22:54:35.040081 4549250496 remote_controller.py:167] Connecting to: ws://127.0.0.1:24748/sc2api, attempt: 0, running: True\n",
      "I0908 22:54:36.046929 4549250496 remote_controller.py:167] Connecting to: ws://127.0.0.1:24748/sc2api, attempt: 1, running: True\n",
      "I0908 22:54:37.051900 4549250496 remote_controller.py:167] Connecting to: ws://127.0.0.1:24748/sc2api, attempt: 2, running: True\n",
      "I0908 22:54:38.053664 4549250496 remote_controller.py:167] Connecting to: ws://127.0.0.1:24748/sc2api, attempt: 3, running: True\n",
      "I0908 22:54:39.059527 4549250496 remote_controller.py:167] Connecting to: ws://127.0.0.1:24748/sc2api, attempt: 4, running: True\n",
      "I0908 22:54:40.065847 4549250496 remote_controller.py:167] Connecting to: ws://127.0.0.1:24748/sc2api, attempt: 5, running: True\n",
      "I0908 22:54:41.067157 4549250496 remote_controller.py:167] Connecting to: ws://127.0.0.1:24748/sc2api, attempt: 6, running: True\n",
      "I0908 22:54:42.070942 4549250496 remote_controller.py:167] Connecting to: ws://127.0.0.1:24748/sc2api, attempt: 7, running: True\n",
      "I0908 22:54:43.072805 4549250496 remote_controller.py:167] Connecting to: ws://127.0.0.1:24748/sc2api, attempt: 8, running: True\n",
      "I0908 22:54:44.077894 4549250496 remote_controller.py:167] Connecting to: ws://127.0.0.1:24748/sc2api, attempt: 9, running: True\n",
      "I0908 22:54:45.083070 4549250496 remote_controller.py:167] Connecting to: ws://127.0.0.1:24748/sc2api, attempt: 10, running: True\n",
      "I0908 22:54:46.085251 4549250496 remote_controller.py:167] Connecting to: ws://127.0.0.1:24748/sc2api, attempt: 11, running: True\n",
      "I0908 22:54:47.087372 4549250496 remote_controller.py:167] Connecting to: ws://127.0.0.1:24748/sc2api, attempt: 12, running: True\n",
      "I0908 22:54:48.090584 4549250496 remote_controller.py:167] Connecting to: ws://127.0.0.1:24748/sc2api, attempt: 13, running: True\n",
      "I0908 22:54:49.094665 4549250496 remote_controller.py:167] Connecting to: ws://127.0.0.1:24748/sc2api, attempt: 14, running: True\n",
      "I0908 22:54:50.100687 4549250496 remote_controller.py:167] Connecting to: ws://127.0.0.1:24748/sc2api, attempt: 15, running: True\n",
      "I0908 22:54:51.106812 4549250496 remote_controller.py:167] Connecting to: ws://127.0.0.1:24748/sc2api, attempt: 16, running: True\n",
      "I0908 22:54:52.109431 4549250496 remote_controller.py:167] Connecting to: ws://127.0.0.1:24748/sc2api, attempt: 17, running: True\n",
      "I0908 22:54:53.110558 4549250496 remote_controller.py:167] Connecting to: ws://127.0.0.1:24748/sc2api, attempt: 18, running: True\n",
      "I0908 22:54:54.115744 4549250496 remote_controller.py:167] Connecting to: ws://127.0.0.1:24748/sc2api, attempt: 19, running: True\n",
      "I0908 22:54:55.121906 4549250496 remote_controller.py:167] Connecting to: ws://127.0.0.1:24748/sc2api, attempt: 20, running: True\n",
      "I0908 22:54:56.123043 4549250496 remote_controller.py:167] Connecting to: ws://127.0.0.1:24748/sc2api, attempt: 21, running: True\n",
      "I0908 22:54:57.125100 4549250496 remote_controller.py:167] Connecting to: ws://127.0.0.1:24748/sc2api, attempt: 22, running: True\n",
      "I0908 22:54:58.126527 4549250496 remote_controller.py:167] Connecting to: ws://127.0.0.1:24748/sc2api, attempt: 23, running: True\n",
      "I0908 22:54:59.128649 4549250496 remote_controller.py:167] Connecting to: ws://127.0.0.1:24748/sc2api, attempt: 24, running: True\n",
      "I0908 22:55:00.132603 4549250496 remote_controller.py:167] Connecting to: ws://127.0.0.1:24748/sc2api, attempt: 25, running: True\n",
      "I0908 22:55:01.133924 4549250496 remote_controller.py:167] Connecting to: ws://127.0.0.1:24748/sc2api, attempt: 26, running: True\n",
      "I0908 22:55:02.139726 4549250496 remote_controller.py:167] Connecting to: ws://127.0.0.1:24748/sc2api, attempt: 27, running: True\n",
      "I0908 22:55:03.145208 4549250496 remote_controller.py:167] Connecting to: ws://127.0.0.1:24748/sc2api, attempt: 28, running: True\n",
      "I0908 22:55:13.921795 4549250496 sc2_env.py:314] Environment is ready\n",
      "I0908 22:55:13.932752 4549250496 sc2_env.py:507] Starting episode 1: [terran, terran] on Simple64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0/no_op                                              ()\n",
      "   1/move_camera                                        (1/minimap [64, 64])\n",
      "   2/select_point                                       (6/select_point_act [4]; 0/screen [84, 84])\n",
      "   3/select_rect                                        (7/select_add [2]; 0/screen [84, 84]; 2/screen2 [84, 84])\n",
      "   4/select_control_group                               (4/control_group_act [5]; 5/control_group_id [10])\n",
      " 453/Stop_quick                                         (3/queued [2])\n",
      " 230/Effect_Spray_screen                                (3/queued [2]; 0/screen [84, 84])\n",
      " 549/Effect_Spray_minimap                               (3/queued [2]; 1/minimap [64, 64])\n",
      " 264/Harvest_Gather_screen                              (3/queued [2]; 0/screen [84, 84])\n",
      " 451/Smart_screen                                       (3/queued [2]; 0/screen [84, 84])\n",
      " 452/Smart_minimap                                      (3/queued [2]; 1/minimap [64, 64])\n",
      " 331/Move_screen                                        (3/queued [2]; 0/screen [84, 84])\n",
      " 332/Move_minimap                                       (3/queued [2]; 1/minimap [64, 64])\n",
      " 333/Patrol_screen                                      (3/queued [2]; 0/screen [84, 84])\n",
      " 334/Patrol_minimap                                     (3/queued [2]; 1/minimap [64, 64])\n",
      "  12/Attack_screen                                      (3/queued [2]; 0/screen [84, 84])\n",
      "  13/Attack_minimap                                     (3/queued [2]; 1/minimap [64, 64])\n",
      " 274/HoldPosition_quick                                 (3/queued [2])\n",
      " 220/Effect_Repair_screen                               (3/queued [2]; 0/screen [84, 84])\n",
      " 221/Effect_Repair_autocast                             ()\n",
      " 269/Harvest_Return_quick                               (3/queued [2])\n",
      "  79/Build_Refinery_screen                              (3/queued [2]; 0/screen [84, 84])\n",
      "  91/Build_SupplyDepot_screen                           (3/queued [2]; 0/screen [84, 84])\n",
      " 261/Halt_quick                                         (3/queued [2])\n",
      "  50/Build_EngineeringBay_screen                        (3/queued [2]; 0/screen [84, 84])\n",
      "  42/Build_Barracks_screen                              (3/queued [2]; 0/screen [84, 84])\n",
      " 140/Cancel_quick                                       (3/queued [2])\n",
      " 335/Rally_Units_screen                                 (3/queued [2]; 0/screen [84, 84])\n",
      " 336/Rally_Units_minimap                                (3/queued [2]; 1/minimap [64, 64])\n",
      "   5/select_unit                                        (8/select_unit_act [4]; 9/select_unit_id [500])\n",
      " 281/Lift_quick                                         (3/queued [2])\n",
      " 477/Train_Marine_quick                                 (3/queued [2])\n",
      " 168/Cancel_Last_quick                                  (3/queued [2])\n",
      "   7/select_army                                        (7/select_add [2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0908 22:55:48.199373 4549250496 sc2_env.py:725] Episode 1 finished after 12520 game steps. Outcome: [1], reward: [1], score: [7100]\n",
      "I0908 22:55:52.745210 4549250496 sc2_env.py:507] Starting episode 2: [terran, terran] on Simple64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  43/Build_Bunker_screen                                (3/queued [2]; 0/screen [84, 84])\n",
      "  44/Build_CommandCenter_screen                         (3/queued [2]; 0/screen [84, 84])\n",
      "  11/build_queue                                        (11/build_queue_id [10])\n",
      "   6/select_idle_worker                                 (10/select_worker [4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0908 22:57:13.768297 4549250496 sc2_env.py:725] Episode 2 finished after 22472 game steps. Outcome: [-1], reward: [-1], score: [7215]\n",
      "I0908 22:57:18.352750 4549250496 sc2_env.py:507] Starting episode 3: [terran, terran] on Simple64\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  app.run(main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Winning rate graph]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "SCORE_FILE = 'rlagent_with_sparse_reward_learning_score'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(SCORE_FILE + '.txt', \"rb\") as fp:\n",
    "    scores = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_scores = np.array(scores)\n",
    "np_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(np_scores)), np_scores.T[0], color='r', label='win rate')\n",
    "plt.plot(np.arange(len(np_scores)), np_scores.T[1], color='g', label='tie rate')\n",
    "plt.plot(np.arange(len(np_scores)), np_scores.T[2], color='b', label='lose rate')\n",
    "plt.ylabel('Score %')\n",
    "plt.xlabel('Episode #')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "starcraft2",
   "language": "python",
   "name": "starcraft2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
