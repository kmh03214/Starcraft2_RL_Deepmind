{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 4 - Making DRL PySC2 Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "### unfortunately, PySC2 uses Abseil, which treats python code as if its run like an app\n",
    "# This does not play well with jupyter notebook\n",
    "# So we will need to monkeypatch sys.argv\n",
    "\n",
    "\n",
    "import sys\n",
    "#sys.argv = [\"python\", \"--map\", \"AbyssalReef\"]\n",
    "sys.argv = [\"python\", \"--map\", \"Simple64\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Runnning 'Agent code' on jupyter notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Copyright 2017 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS-IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"Run an agent.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import importlib\n",
    "import threading\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from future.builtins import range  # pylint: disable=redefined-builtin\n",
    "\n",
    "from pysc2 import maps\n",
    "from pysc2.env import available_actions_printer\n",
    "from pysc2.env import run_loop\n",
    "from pysc2.env import sc2_env\n",
    "from pysc2.lib import point_flag\n",
    "from pysc2.lib import stopwatch\n",
    "from pysc2.lib import actions\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "# because of Abseil's horrible design for running code underneath Colabs\n",
    "# We have to pull out this ugly hack from the hat\n",
    "if \"flags_defined\" not in globals():\n",
    "    flags.DEFINE_bool(\"render\", False, \"Whether to render with pygame.\")\n",
    "    point_flag.DEFINE_point(\"feature_screen_size\", \"84\",\n",
    "                            \"Resolution for screen feature layers.\")\n",
    "    point_flag.DEFINE_point(\"feature_minimap_size\", \"64\",\n",
    "                            \"Resolution for minimap feature layers.\")\n",
    "    point_flag.DEFINE_point(\"rgb_screen_size\", None,\n",
    "                            \"Resolution for rendered screen.\")\n",
    "    point_flag.DEFINE_point(\"rgb_minimap_size\", None,\n",
    "                            \"Resolution for rendered minimap.\")\n",
    "    flags.DEFINE_enum(\"action_space\", \"RAW\", sc2_env.ActionSpace._member_names_,  # pylint: disable=protected-access\n",
    "                      \"Which action space to use. Needed if you take both feature \"\n",
    "                      \"and rgb observations.\")\n",
    "    flags.DEFINE_bool(\"use_feature_units\", False,\n",
    "                      \"Whether to include feature units.\")\n",
    "    flags.DEFINE_bool(\"use_raw_units\", True,\n",
    "                      \"Whether to include raw units.\")\n",
    "    flags.DEFINE_integer(\"raw_resolution\", 64, \"Raw Resolution.\")\n",
    "    flags.DEFINE_bool(\"disable_fog\", True, \"Whether to disable Fog of War.\")\n",
    "\n",
    "    flags.DEFINE_integer(\"max_agent_steps\", 0, \"Total agent steps.\")\n",
    "    flags.DEFINE_integer(\"game_steps_per_episode\", None, \"Game steps per episode.\")\n",
    "    flags.DEFINE_integer(\"max_episodes\", 0, \"Total episodes.\")\n",
    "    flags.DEFINE_integer(\"step_mul\", 8, \"Game steps per agent step.\")\n",
    "    flags.DEFINE_float(\"fps\", 22.4, \"Frames per second to run the game.\")\n",
    "\n",
    "    #flags.DEFINE_string(\"agent\", \"sc2.agent.BasicAgent.ZergBasicAgent\",\n",
    "    #                    \"Which agent to run, as a python path to an Agent class.\")\n",
    "    #flags.DEFINE_enum(\"agent_race\", \"zerg\", sc2_env.Race._member_names_,  # pylint: disable=protected-access\n",
    "    #                  \"Agent 1's race.\")\n",
    "    flags.DEFINE_string(\"agent\", \"TerranRLAgentWithRawActsAndRawObs\",\n",
    "                        \"Which agent to run, as a python path to an Agent class.\")\n",
    "    flags.DEFINE_enum(\"agent_race\", \"terran\", sc2_env.Race._member_names_,  # pylint: disable=protected-access\n",
    "                      \"Agent 1's race.\")\n",
    "\n",
    "    flags.DEFINE_string(\"agent2\", \"Bot\", \"Second agent, either Bot or agent class.\")\n",
    "    flags.DEFINE_enum(\"agent2_race\", \"random\", sc2_env.Race._member_names_,  # pylint: disable=protected-access\n",
    "                      \"Agent 2's race.\")\n",
    "    flags.DEFINE_enum(\"difficulty\", \"hard\", sc2_env.Difficulty._member_names_,  # pylint: disable=protected-access\n",
    "                      \"If agent2 is a built-in Bot, it's strength.\")\n",
    "\n",
    "    flags.DEFINE_bool(\"profile\", False, \"Whether to turn on code profiling.\")\n",
    "    flags.DEFINE_bool(\"trace\", False, \"Whether to trace the code execution.\")\n",
    "    flags.DEFINE_integer(\"parallel\", 1, \"How many instances to run in parallel.\")\n",
    "\n",
    "    flags.DEFINE_bool(\"save_replay\", True, \"Whether to save a replay at the end.\")\n",
    "\n",
    "    flags.DEFINE_string(\"map\", None, \"Name of a map to use.\")\n",
    "    flags.mark_flag_as_required(\"map\")\n",
    "\n",
    "flags_defined = True\n",
    "\n",
    "def run_thread(agent_classes, players, map_name, visualize):\n",
    "  \"\"\"Run one thread worth of the environment with agents.\"\"\"\n",
    "  with sc2_env.SC2Env(\n",
    "      map_name=map_name,\n",
    "      players=players,\n",
    "      agent_interface_format=sc2_env.parse_agent_interface_format(\n",
    "        feature_screen=FLAGS.feature_screen_size,\n",
    "        feature_minimap=FLAGS.feature_minimap_size,\n",
    "        rgb_screen=FLAGS.rgb_screen_size,\n",
    "        rgb_minimap=FLAGS.rgb_minimap_size,\n",
    "        action_space=FLAGS.action_space,\n",
    "        use_raw_units=FLAGS.use_raw_units,\n",
    "        raw_resolution=FLAGS.raw_resolution),\n",
    "      step_mul=FLAGS.step_mul,\n",
    "      game_steps_per_episode=FLAGS.game_steps_per_episode,\n",
    "      disable_fog=FLAGS.disable_fog,\n",
    "      visualize=visualize) as env:\n",
    "    #env = available_actions_printer.AvailableActionsPrinter(env)\n",
    "    agents = [agent_cls() for agent_cls in agent_classes]\n",
    "    run_loop.run_loop(agents, env, FLAGS.max_agent_steps, FLAGS.max_episodes)\n",
    "    if FLAGS.save_replay:\n",
    "      env.save_replay(agent_classes[0].__name__)\n",
    "\n",
    "def main(unused_argv):\n",
    "  \"\"\"Run an agent.\"\"\"\n",
    "  #stopwatch.sw.enabled = FLAGS.profile or FLAGS.trace\n",
    "  #stopwatch.sw.trace = FLAGS.trace\n",
    "\n",
    "  map_inst = maps.get(FLAGS.map)\n",
    "\n",
    "  agent_classes = []\n",
    "  players = []\n",
    "\n",
    "  #agent_module, agent_name = FLAGS.agent.rsplit(\".\", 1)\n",
    "  #agent_cls = getattr(importlib.import_module(agent_module), agent_name)\n",
    "  #agent_classes.append(agent_cls)\n",
    "  agent_classes.append(TerranRLAgentWithRawActsAndRawObs)\n",
    "  players.append(sc2_env.Agent(sc2_env.Race[FLAGS.agent_race]))\n",
    "\n",
    "  if map_inst.players >= 2:\n",
    "    if FLAGS.agent2 == \"Bot\":\n",
    "      players.append(sc2_env.Bot(sc2_env.Race[FLAGS.agent2_race],\n",
    "                                 sc2_env.Difficulty[FLAGS.difficulty]))\n",
    "    else:\n",
    "      #agent_module, agent_name = FLAGS.agent2.rsplit(\".\", 1)\n",
    "      #agent_cls = getattr(importlib.import_module(agent_module), agent_name)\n",
    "      agent_classes.append(TerranRandomAgent)\n",
    "      players.append(sc2_env.Agent(sc2_env.Race[FLAGS.agent2_race]))\n",
    "\n",
    "  threads = []\n",
    "  for _ in range(FLAGS.parallel - 1):\n",
    "    t = threading.Thread(target=run_thread,\n",
    "                         args=(agent_classes, players, FLAGS.map, False))\n",
    "    threads.append(t)\n",
    "    t.start()\n",
    "\n",
    "  run_thread(agent_classes, players, FLAGS.map, FLAGS.render)\n",
    "\n",
    "  for t in threads:\n",
    "    t.join()\n",
    "\n",
    "  if FLAGS.profile:\n",
    "    pass\n",
    "    #print(stopwatch.sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creating a PySC2 Agent with Raw Actions & Observations\n",
    "\n",
    "![StarCraft2 PySC2 interfaces](./images/StarCraft2_PySC2_interfaces.png)\n",
    "\n",
    "ref : https://on-demand.gputechconf.com/gtc/2018/presentation/s8739-machine-learning-with-starcraft-II.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### < PySC2 Interfaces 3가지 종류 >\n",
    "\n",
    "### 1st, Rendered\n",
    "* Decomposed :\n",
    "    - Screen, minimap, resources, available actions\n",
    "* Same control as humans :\n",
    "    - Pixel coordinates\n",
    "    - Move camera\n",
    "    - Select unit/rectangle\n",
    "* Great for Deep Learning, but hard\n",
    "\n",
    "### 2nd, Feature Layer\n",
    "* Same actions : still in pixel space\n",
    "* Same decomposed observations, but more abstract\n",
    "    - Orthogonal camera \n",
    "* Layers:\n",
    "    - unit type\n",
    "    - unit owner\n",
    "    - selection\n",
    "    - health\n",
    "    - unit density\n",
    "    - etc\n",
    "    \n",
    "### 3rd, Raw\n",
    "* List of units and state\n",
    "* Control each unit individually in world coordinates\n",
    "* Gives all observable state (no camera)\n",
    "* Great for scripted agents and programmatic replay analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### < Raw Actions & Observations 을 사용하는 이유>\n",
    "* Raw Actions & Observations 은 world cordinates를 사용하므로 전체 Map을 한번에 관찰하고 Camera를 이동하지 않고도 Map 상의 어느 곳에서도 Action을 취할 수 있는 새로운 형태의 Feature 이다.\n",
    "* 이번 과정에 SL(Supervised Learning, 지도학습)을 활용한 학습은 없지만 스타크래프트 2 리플레이를 활용한 SL은 Raw Actions & Observations를 활용한 \"programmatic replay analysis\"가 필요하다.\n",
    "* 인간 플레이어를 이긴 DeepMind의 AlphaStar의 주요 변경사항 중의 하나는 Raw Actions & Observations 의 활용이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DRL 모델의 성능 추이를 보기위해 Reward의 평균 추이를 이용한다. 이때 단순이동평균 보다는 지수이동평균이 적절하다.\n",
    "\n",
    "### 지수이동평균(EMA:Exponential Moving Average) 란?\n",
    "지수이동평균(Exponential Moving Average)은 과거의 모든 기간을 계산대상으로 하며 최근의 데이타에 더 높은 가중치를 두는 일종의 가중이동평균법이다.\n",
    "\n",
    "단순이동평균의 계산법에 비하여 원리가 복잡해 보이지만 실제로 이동평균을 산출하는 방법은 Previous Step의 지수이동평균값과 평활계수(smoothing constant) 그리고 당일의 가격만으로 구할 수 있으므로 Previous Step의 지수이동평균값만 구해진다면 오히려 간단한 편이다.\n",
    "\n",
    "따라서 지수이동평균은 단순이동평균에 비해 몇가지 중요한 강점을 가진다.\n",
    "\n",
    "첫째는 가장 최근의 Step에 가장 큰 가중치를 둠으로 해서 최근의 Episode들을 잘 반영한다는 점이고, 둘째는 단순이동평균에서와 같이 오래된 데이타를 갑자기 제외하지 않고 천천히 그 영향력을 사라지게 한다는 점이다.\n",
    "또한 전 기간의 데이타를 분석대상으로 함으로써 가중이동평균에서 문제되는 특정 기간의 데이타만을 분석대상으로 한다는 단점도 보완하고 있다.\n",
    "\n",
    "### 지수이동평균(EMA:Exponential Moving Average) 계산\n",
    "\n",
    "지수이동평균은 가장 최근의 값에 많은 가중치를 부여하고 오래 된 값에는 적은 가중치를 부여한다. 비록 오래 된 값이라고 할지라도 완전히 무시하지는 않고 적게나마 반영시켜 계산한다는 장점이 있다. 단기 변동성을 포착하려는 것이 목적이다.\n",
    "\n",
    "EMA=Previous Step 지수이동평균+(k∗(Current Step Reward − Previous Step 지수이동평균))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Applying Vanilla DQN to a PySC2 Agent\n",
    "\n",
    "구현된 기능\n",
    "\n",
    "- Implementing 'Experience Replay' : \n",
    "    - 'Maximization Bias' 문제를 발생시키는 원인 중 하나인 'Sample간의 시간적 연관성'을 해결하기 위한 방법\n",
    "    - Online Learning 에서 Batch Learning 으로 학습방법 바뀜 : Online update 는 Batch update 보다 일반적으로 Validation loss 가 더 높게 나타남.\n",
    "    - Reinforcement Learning for Robots. Using Neural Networks. Long -Ji Lin. January 6, 1993. 논문에서 최초로 연구됨 http://isl.anthropomatik.kit.edu/pdf/Lin1993.pdf\n",
    "\n",
    "- Implementing 'Fixed Q-Target' : \n",
    "    - 'Moving Q-Target' 문제 해결하기 위한 방법\n",
    "    - 2015년 Nature 버전 DQN 논문에서 처음 제안됨. https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning \n",
    "\n",
    "\n",
    "구현되지 않은 기능\n",
    "\n",
    "- Implementing 'Sensory Input Feature-Extraction' :\n",
    "    - 게임의 Raw Image 를 Neural Net에 넣기 위한 Preprocessing(전처리) 과정\n",
    "    - Raw Image 의 Sequence중 '최근 4개의 이미지'(과거 정보)를 하나의 새로운 State로 정의하여 non-MDP를 MDP 문제로 바꾸는 Preprocessing 과정 \n",
    "    - CNN(합성곱 신경망)을 활용한 '차원의 저주' 극복"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import math\n",
    "import os.path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import deque\n",
    "import pickle\n",
    "\n",
    "from pysc2.agents import base_agent\n",
    "from pysc2.env import sc2_env\n",
    "from pysc2.lib import actions, features, units, upgrades\n",
    "from absl import app\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from skdrl.pytorch.model.mlp import NaiveMultiLayerPerceptron\n",
    "from skdrl.common.memory.memory import ExperienceReplayMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE_QNET = 'rlagent_with_vanilla_dqn_qnet'\n",
    "DATA_FILE_QNET_TARGET = 'rlagent_with_vanilla_dqn_qnet_target'\n",
    "SCORE_FILE = 'rlagent_with_vanilla_dqn_score'\n",
    "\n",
    "scores = []                        # list containing scores from each episode\n",
    "scores_window = deque(maxlen=100)  # last 100 scores\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaiveMultiLayerPerceptron(\n",
      "  (hidden_act_func): ReLU()\n",
      "  (out_act_func): Identity()\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=10, out_features=20, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=20, out_features=12, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=12, out_features=1, bias=True)\n",
      "    (5): Identity()\n",
      "  )\n",
      ")\n",
      "tensor([[-0.3301],\n",
      "        [-0.4388],\n",
      "        [-0.4118],\n",
      "        [-0.3924],\n",
      "        [-0.3717],\n",
      "        [-0.4231],\n",
      "        [-0.4617],\n",
      "        [-0.3912],\n",
      "        [-0.3118],\n",
      "        [-0.3619],\n",
      "        [-0.3262],\n",
      "        [-0.4291]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class NaiveMultiLayerPerceptron(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_dim: int,\n",
    "                 output_dim: int,\n",
    "                 num_neurons: list = [64, 32],\n",
    "                 hidden_act_func: str = 'ReLU',\n",
    "                 out_act_func: str = 'Identity'):\n",
    "        super(NaiveMultiLayerPerceptron, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_neurons = num_neurons\n",
    "        self.hidden_act_func = getattr(nn, hidden_act_func)()\n",
    "        self.out_act_func = getattr(nn, out_act_func)()\n",
    "\n",
    "        input_dims = [input_dim] + num_neurons\n",
    "        output_dims = num_neurons + [output_dim]\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i, (in_dim, out_dim) in enumerate(zip(input_dims, output_dims)):\n",
    "            is_last = True if i == len(input_dims) - 1 else False\n",
    "            self.layers.append(nn.Linear(in_dim, out_dim))\n",
    "            if is_last:\n",
    "                self.layers.append(self.out_act_func)\n",
    "            else:\n",
    "                self.layers.append(self.hidden_act_func)\n",
    "\n",
    "    def forward(self, xs):\n",
    "        for layer in self.layers:\n",
    "            xs = layer(xs)\n",
    "        return xs\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    net = NaiveMultiLayerPerceptron(10, 1, [20, 12], 'ReLU', 'Identity')\n",
    "    print(net)\n",
    "\n",
    "    xs = torch.randn(size=(12, 10))\n",
    "    ys = net(xs)\n",
    "    print(ys)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-update 공식\n",
    "\n",
    "#### 1. Online Q-learning\n",
    "![Online Q-learning](./images/q-update-experience-replay.png)\n",
    "\n",
    "#### 2. Online Q-learning with Function Approximation\n",
    "![Online Q-learning with Function Approximation](./images/q-update-function-approximation.png)\n",
    "\n",
    "#### 3. Batch Q-learning with Function Approximation & Experience Replay\n",
    "![Batch Q-learning with Function Approximation & Experience Replay](./images/q-update-online.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "\n",
    "\n",
    "class ExperienceReplayMemory:\n",
    "    def __init__(self, max_size):\n",
    "        # deque object that we've used for 'episodic_memory' is not suitable for random sampling\n",
    "        # here, we instead use a fix-size array to implement 'buffer'\n",
    "        self.buffer = [None] * max_size\n",
    "        self.max_size = max_size\n",
    "        self.index = 0\n",
    "        self.size = 0\n",
    "\n",
    "    def push(self, obj):\n",
    "        self.buffer[self.index] = obj\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "        self.index = (self.index + 1) % self.max_size\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        indices = sample(range(self.size), batch_size)\n",
    "        return [self.buffer[index] for index in indices]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving target problem\n",
    "\n",
    "#### 1. Function Approximation을 사용하지 않는 Q-learning 의 경우 : 특정한 Q(s,a) update가 다른 Q(s,a)에 영향을 주지 않는다.\n",
    "![Moving target Q-learning](./images/moving-target_q-learing_case.png)\n",
    "\n",
    "#### 2. Function Approximation을 사용하는 Q-learnig 의 경우 : 특정한 Q(s,a) update가 다른 Q(s,a)에 영향을 준다.\n",
    "![Moving target Q-learning with Function Approximation](./images/moving-target_q-learing_with_function_approximation_case.png)\n",
    "\n",
    "### Moving target 문제는 Deep Neural Network를 사용하는 Function Approximation 기법인 경우 심해지는 경향성이 있음.\n",
    "\n",
    "image ref : Fast Campus RL online courese"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `nn.SmoothL1Loss()` = Huber loss 란?\n",
    "\n",
    "Mean-squared Error (MSE) Loss 는 데이터의 outlier에 매우 취약하다.\n",
    "어떤 이유로 타겟하는 레이블 y (이 경우는 q-learning target)이 noisy 할때를 가정하면, 잘못된 y 값을 맞추기 위해 파라미터들이 너무 sensitive 하게 움직이게 된다.\n",
    "\n",
    "이런 현상은 q-learning 의 학습초기에 매우 빈번해 나타난다. 이러한 문제를 조금이라도 완화하기 위해서 outlier에 덜 민감한 Huber loss 함수를 사용한다.\n",
    "\n",
    "### SmoothL1Loss (aka Huber loss)\n",
    "\n",
    "$$loss(x,y) = \\frac{1}{n}\\sum_i z_i$$\n",
    "$|x_i - y_i| <1$ 일때,\n",
    "$$z_i = 0.5(x_i - y_i)^2$$\n",
    "$|x_i - y_i| \\geq1$ 일때,\n",
    "$$z_i = |x_i - y_i|-0.5$$\n",
    "\n",
    "ref : https://pytorch.org/docs/master/generated/torch.nn.SmoothL1Loss.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 state_dim: int,\n",
    "                 action_dim: int,\n",
    "                 qnet: nn.Module,\n",
    "                 qnet_target: nn.Module,\n",
    "                 lr: float,\n",
    "                 gamma: float,\n",
    "                 epsilon: float):\n",
    "        \"\"\"\n",
    "        :param state_dim: input state dimension\n",
    "        :param action_dim: action dimension\n",
    "        :param qnet: main q network\n",
    "        :param qnet_target: target q network\n",
    "        :param lr: learning rate\n",
    "        :param gamma: discount factor of MDP\n",
    "        :param epsilon: E-greedy factor\n",
    "        \"\"\"\n",
    "\n",
    "        super(DQN, self).__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.qnet = qnet\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.opt = torch.optim.Adam(params=self.qnet.parameters(), lr=lr)\n",
    "        self.register_buffer('epsilon', torch.ones(1) * epsilon)\n",
    "\n",
    "        # target network related\n",
    "        qnet_target.load_state_dict(qnet.state_dict())\n",
    "        self.qnet_target = qnet_target\n",
    "        self.criteria = nn.SmoothL1Loss()\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        qs = self.qnet(state)\n",
    "        #prob = np.random.uniform(0.0, 1.0, 1)\n",
    "        #if torch.from_numpy(prob).float() <= self.epsilon:  # random\n",
    "        if random.random() <= self.epsilon: # random\n",
    "            action = np.random.choice(range(self.action_dim))\n",
    "        else:  # greedy\n",
    "            action = qs.argmax(dim=-1)\n",
    "        return int(action)\n",
    "\n",
    "    def learn(self, state, action, reward, next_state, done):\n",
    "        s, a, r, ns = state, action, reward, next_state\n",
    "#         print(\"state: \", s)\n",
    "#         print(\"action: \", a)\n",
    "#         print(\"reward: \", reward)\n",
    "#         print(\"next_state: \", ns)\n",
    "        \n",
    "\n",
    "        # compute Q-Learning target with 'target network'\n",
    "        with torch.no_grad():\n",
    "            q_max, _ = self.qnet_target(ns).max(dim=-1, keepdims=True)\n",
    "            q_target = r + self.gamma * q_max * (1 - done)\n",
    "\n",
    "        q_val = self.qnet(s).gather(1, a)\n",
    "        loss = self.criteria(q_val, q_target)\n",
    "\n",
    "        self.opt.zero_grad()\n",
    "        loss.backward()\n",
    "        self.opt.step()\n",
    "\n",
    "\n",
    "def prepare_training_inputs(sampled_exps, device='cpu'):\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    next_states = []\n",
    "    dones = []\n",
    "    for sampled_exp in sampled_exps:\n",
    "        states.append(sampled_exp[0])\n",
    "        actions.append(sampled_exp[1])\n",
    "        rewards.append(sampled_exp[2])\n",
    "        next_states.append(sampled_exp[3])\n",
    "        dones.append(sampled_exp[4])\n",
    "\n",
    "    states = torch.cat(states, dim=0).float().to(device)\n",
    "    actions = torch.cat(actions, dim=0).to(device)\n",
    "    rewards = torch.cat(rewards, dim=0).float().to(device)\n",
    "    next_states = torch.cat(next_states, dim=0).float().to(device)\n",
    "    dones = torch.cat(dones, dim=0).float().to(device)\n",
    "    return states, actions, rewards, next_states, dones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Action 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TerranAgentWithRawActsAndRawObs(base_agent.BaseAgent):\n",
    "    # actions 추가 및 함수 정의(hirerachy하게)\n",
    "    \n",
    "    actions = (\"do_nothing\",\n",
    "               \"train_scv\",\n",
    "               \"harvest_minerals\",\n",
    "               \"harvest_gas\",\n",
    "               \"build_commandcenter\",\n",
    "               \n",
    "               \"build_refinery\",\n",
    "               \"build_supply_depot\",\n",
    "               \"build_barracks\",\n",
    "               \"train_marine\",\n",
    "               \n",
    "               \"build_factorys\",\n",
    "               \"build_techlab_factorys\",\n",
    "               \"train_tank\",\n",
    "               \n",
    "               \"build_armorys\",\n",
    "               \n",
    "               \"build_starports\",\n",
    "               \"build_techlab_starports\",\n",
    "               \"train_banshee\",\n",
    "               \n",
    "               \"attack\",\n",
    "               \"attack_all\",\n",
    "               \n",
    "               \"tank_control\"\n",
    "              )\n",
    "\n",
    "\n",
    "    \n",
    "    def unit_type_is_selected(self, obs, unit_type):\n",
    "        if (len(obs.observation.single_select) > 0 and\n",
    "            obs.observation.single_select[0].unit_type == unit_type):\n",
    "              return True\n",
    "\n",
    "        if (len(obs.observation.multi_select) > 0 and\n",
    "            obs.observation.multi_select[0].unit_type == unit_type):\n",
    "              return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def get_my_units_by_type(self, obs, unit_type):\n",
    "        if unit_type == units.Neutral.VespeneGeyser: # 가스 일 때만\n",
    "            return [unit for unit in obs.observation.raw_units\n",
    "                if unit.unit_type == unit_type]\n",
    "        \n",
    "        return [unit for unit in obs.observation.raw_units\n",
    "                if unit.unit_type == unit_type\n",
    "                and unit.alliance == features.PlayerRelative.SELF]\n",
    "\n",
    "    def get_enemy_units_by_type(self, obs, unit_type):\n",
    "        return [unit for unit in obs.observation.raw_units\n",
    "                if unit.unit_type == unit_type\n",
    "                and unit.alliance == features.PlayerRelative.ENEMY]\n",
    "\n",
    "    def get_my_completed_units_by_type(self, obs, unit_type):\n",
    "        return [unit for unit in obs.observation.raw_units\n",
    "                if unit.unit_type == unit_type\n",
    "                and unit.build_progress == 100\n",
    "                and unit.alliance == features.PlayerRelative.SELF]\n",
    "\n",
    "    def get_enemy_completed_units_by_type(self, obs, unit_type):\n",
    "        return [unit for unit in obs.observation.raw_units\n",
    "                if unit.unit_type == unit_type\n",
    "                and unit.build_progress == 100\n",
    "                and unit.alliance == features.PlayerRelative.ENEMY]\n",
    "\n",
    "    def get_distances(self, obs, units, xy):\n",
    "        units_xy = [(unit.x, unit.y) for unit in units]\n",
    "        return np.linalg.norm(np.array(units_xy) - np.array(xy), axis=1)\n",
    "\n",
    "    def step(self, obs):\n",
    "        super(TerranAgentWithRawActsAndRawObs, self).step(obs)\n",
    "        if obs.first():\n",
    "            command_center = self.get_my_units_by_type(\n",
    "                obs, units.Terran.CommandCenter)[0]\n",
    "            self.base_top_left = (command_center.x < 32)\n",
    "            self.top_left_gas_xy = [(14, 25), (21,19), (46,23), (39,16)]\n",
    "            self.bottom_right_gas_xy = [(44, 43), (37,50), (12,46), (19,53)]\n",
    "            \n",
    "            \n",
    "            self.cloaking_flag = 1\n",
    "            \n",
    "            self.TerranVehicleWeaponsLevel1 = False\n",
    "            self.TerranVehicleWeaponsLevel2 = False\n",
    "            self.TerranVehicleWeaponsLevel3 = False\n",
    "            \n",
    "\n",
    "    def do_nothing(self, obs):\n",
    "        return actions.RAW_FUNCTIONS.no_op()\n",
    "    \n",
    "    def train_scv(self, obs):\n",
    "        completed_commandcenterses = self.get_my_completed_units_by_type(\n",
    "            obs, units.Terran.CommandCenter)\n",
    "        \n",
    "        scvs = self.get_my_units_by_type(obs, units.Terran.SCV)\n",
    "        \n",
    "        if (len(completed_commandcenterses) > 0 and obs.observation.player.minerals >= 100\n",
    "                and len(scvs) < 35):\n",
    "            commandcenters = self.get_my_units_by_type(obs, units.Terran.CommandCenter)\n",
    "            \n",
    "            ccs =[commandcenter for commandcenter in commandcenters if commandcenter.assigned_harvesters < 18]\n",
    "            \n",
    "            if ccs:\n",
    "                ccs = ccs[0]\n",
    "                if ccs.order_length < 5:\n",
    "                    return actions.RAW_FUNCTIONS.Train_SCV_quick(\"now\", ccs.tag)\n",
    "        return actions.RAW_FUNCTIONS.no_op()\n",
    "\n",
    "    def harvest_minerals(self, obs):\n",
    "        scvs = self.get_my_units_by_type(obs, units.Terran.SCV)\n",
    "        commandcenters = self.get_my_units_by_type(obs,units.Terran.CommandCenter) # 최적 자원 할당 유닛 구현\n",
    "        \n",
    "        cc = [commandcenter for commandcenter in commandcenters if commandcenter.assigned_harvesters < 18]\n",
    "        \n",
    "        if cc:\n",
    "            cc = cc[0]\n",
    "\n",
    "            idle_scvs = [scv for scv in scvs if scv.order_length == 0]\n",
    "\n",
    "            if len(idle_scvs) > 0 and cc.assigned_harvesters < 18:\n",
    "\n",
    "                mineral_patches = [unit for unit in obs.observation.raw_units\n",
    "                                   if unit.unit_type in [\n",
    "                                       units.Neutral.BattleStationMineralField,\n",
    "                                       units.Neutral.BattleStationMineralField750,\n",
    "                                       units.Neutral.LabMineralField,\n",
    "                                       units.Neutral.LabMineralField750,\n",
    "                                       units.Neutral.MineralField,\n",
    "                                       units.Neutral.MineralField750,\n",
    "                                       units.Neutral.PurifierMineralField,\n",
    "                                       units.Neutral.PurifierMineralField750,\n",
    "                                       units.Neutral.PurifierRichMineralField,\n",
    "                                       units.Neutral.PurifierRichMineralField750,\n",
    "                                       units.Neutral.RichMineralField,\n",
    "                                       units.Neutral.RichMineralField750\n",
    "                                   ]]\n",
    "                scv = random.choice(idle_scvs)\n",
    "                distances = self.get_distances(obs, mineral_patches, (scv.x, scv.y))\n",
    "                mineral_patch = mineral_patches[np.argmin(distances)]\n",
    "                return actions.RAW_FUNCTIONS.Harvest_Gather_unit(\n",
    "                    \"now\", scv.tag, mineral_patch.tag)\n",
    "        return actions.RAW_FUNCTIONS.no_op()\n",
    "    \n",
    "    def harvest_gas(self, obs):\n",
    "        scvs = self.get_my_units_by_type(obs, units.Terran.SCV)\n",
    "        refs = self.get_my_units_by_type(obs, units.Terran.Refinery)\n",
    "        \n",
    "        refs = [refinery for refinery in refs if refinery.assigned_harvesters < 3]\n",
    "        \n",
    "        if refs:\n",
    "            ref = refs[0]\n",
    "            if len(scvs) > 0 and ref.ideal_harvesters:\n",
    "                scv = random.choice(scvs)\n",
    "                distances = self.get_distances(obs, refs, (scv.x, scv.y))\n",
    "                ref = refs[np.argmin(distances)]\n",
    "\n",
    "                return actions.RAW_FUNCTIONS.Harvest_Gather_unit(\n",
    "                    \"now\", scv.tag, ref.tag)\n",
    "        \n",
    "        return actions.RAW_FUNCTIONS.no_op()\n",
    "    \n",
    "    def build_commandcenter(self,obs):\n",
    "        commandcenters = self.get_my_units_by_type(obs,units.Terran.CommandCenter)\n",
    "        \n",
    "        scvs = self.get_my_units_by_type(obs, units.Terran.SCV)\n",
    "        \n",
    "        if len(commandcenters) == 0 and obs.observation.player.minerals >= 400 and len(scvs) > 0:\n",
    "            # 본진 commandcenter가 파괴된 경우\n",
    "            ccs_xy = (19, 23) if self.base_top_left else (39,45)\n",
    "            distances = self.get_distances(obs, scvs, ccs_xy)\n",
    "            scv = scvs[np.argmin(distances)]\n",
    "            return actions.RAW_FUNCTIONS.Build_CommandCenter_pt(\n",
    "                \"now\", scv.tag, ccs_xy)\n",
    "        \n",
    "        if ( len(commandcenters) < 2 and obs.observation.player.minerals >= 400 and\n",
    "                len(scvs) > 0):\n",
    "            ccs_xy = (41, 21) if self.base_top_left else (17, 48)\n",
    "            \n",
    "            if len(commandcenters) == 1 and ( (commandcenters[0].x,commandcenters[0].y) == (41,21) or\n",
    "                                             (commandcenters[0].x,commandcenters[0].y) == (17,48)):\n",
    "                # 본진 commandcenter가 파괴된 경우\n",
    "                ccs_xy = (19, 23) if self.base_top_left else (39,45)\n",
    "            \n",
    "            distances = self.get_distances(obs, scvs, ccs_xy)\n",
    "            scv = scvs[np.argmin(distances)]\n",
    "\n",
    "            return actions.RAW_FUNCTIONS.Build_CommandCenter_pt(\n",
    "                \"now\", scv.tag, ccs_xy)\n",
    "        return actions.RAW_FUNCTIONS.no_op()\n",
    "    \n",
    "    ################################################################################################\n",
    "    ####################################### refinery ###############################################\n",
    "    \n",
    "    def build_refinery(self,obs):\n",
    "        refinerys = self.get_my_units_by_type(obs,units.Terran.Refinery)\n",
    "        scvs = self.get_my_units_by_type(obs, units.Terran.SCV)\n",
    "        \n",
    "        if (obs.observation.player.minerals >= 100 and\n",
    "                len(scvs) > 0):\n",
    "            gas = self.get_my_units_by_type(obs, units.Neutral.VespeneGeyser)[0]\n",
    "            \n",
    "            if self.base_top_left:\n",
    "                gases = self.top_left_gas_xy\n",
    "            else:\n",
    "                gases = self.bottom_right_gas_xy\n",
    "            \n",
    "            rc = np.random.choice([0,1,2,3])\n",
    "            gas_xy = gases[rc]\n",
    "            if (gas.x, gas.y) == gas_xy:\n",
    "                distances = self.get_distances(obs, scvs, gas_xy)\n",
    "                scv = scvs[np.argmin(distances)]\n",
    "\n",
    "                return actions.RAW_FUNCTIONS.Build_Refinery_pt(\n",
    "                    \"now\", scv.tag, gas.tag)\n",
    "        return actions.RAW_FUNCTIONS.no_op()\n",
    "\n",
    "    def build_supply_depot(self, obs):\n",
    "        supply_depots = self.get_my_units_by_type(obs, units.Terran.SupplyDepot)\n",
    "        scvs = self.get_my_units_by_type(obs, units.Terran.SCV)\n",
    "        \n",
    "        free_supply = (obs.observation.player.food_cap -\n",
    "                       obs.observation.player.food_used)\n",
    "        \n",
    "        if (obs.observation.player.minerals >= 100 and\n",
    "            len(scvs) > 0 and free_supply < 8):\n",
    "            \n",
    "            ccs = self.get_my_units_by_type(obs, units.Terran.CommandCenter)\n",
    "            if ccs:\n",
    "                for cc in ccs:\n",
    "                    cc_x, cc_y = cc.x, cc.y\n",
    "                \n",
    "                rand1,rand2 = random.randint(0,10),random.randint(-10,0)\n",
    "                supply_depot_xy = (cc_x + rand1, cc_y + rand2) if self.base_top_left else (cc_x - rand1, cc_y - rand2)\n",
    "                if 0 < supply_depot_xy[0] < 64 and 0 < supply_depot_xy[1] < 64:\n",
    "                    pass\n",
    "                else:\n",
    "                    return actions.RAW_FUNCTIONS.no_op()\n",
    "                    \n",
    "                \n",
    "                distances = self.get_distances(obs, scvs, supply_depot_xy)\n",
    "                scv = scvs[np.argmin(distances)]\n",
    "                \n",
    "                return actions.RAW_FUNCTIONS.Build_SupplyDepot_pt(\n",
    "                    \"now\", scv.tag, supply_depot_xy)\n",
    "                \n",
    "        return actions.RAW_FUNCTIONS.no_op()\n",
    "\n",
    "    def build_barracks(self, obs):\n",
    "        completed_supply_depots = self.get_my_completed_units_by_type(\n",
    "            obs, units.Terran.SupplyDepot)\n",
    "        barrackses = self.get_my_units_by_type(obs, units.Terran.Barracks)\n",
    "        scvs = self.get_my_units_by_type(obs, units.Terran.SCV)\n",
    "        \n",
    "        if (len(completed_supply_depots) > 0 and\n",
    "            obs.observation.player.minerals >= 150 and len(scvs) > 0 and\n",
    "            len(barrackses)< 3):\n",
    "            \n",
    "            brks = self.get_my_units_by_type(obs, units.Terran.SupplyDepot)\n",
    "            \n",
    "            completed_command_center = self.get_my_completed_units_by_type(\n",
    "            obs, units.Terran.CommandCenter)\n",
    "            \n",
    "            if len(barrackses) >= 1 and len(completed_command_center) == 1:\n",
    "                # double commands\n",
    "                    \n",
    "                commandcenters = self.get_my_units_by_type(obs,units.Terran.CommandCenter)\n",
    "                scvs = self.get_my_units_by_type(obs, units.Terran.SCV)\n",
    "\n",
    "                if ( len(commandcenters) < 2 and obs.observation.player.minerals >= 400 and\n",
    "                        len(scvs) > 0):\n",
    "                    ccs_xy = (41, 21) if self.base_top_left else (17, 48)\n",
    "\n",
    "                    distances = self.get_distances(obs, scvs, ccs_xy)\n",
    "                    scv = scvs[np.argmin(distances)]\n",
    "\n",
    "                    return actions.RAW_FUNCTIONS.Build_CommandCenter_pt(\n",
    "                        \"now\", scv.tag, ccs_xy)\n",
    "            \n",
    "            if brks:\n",
    "                for brk in brks:\n",
    "                    brk_x,brk_y = brk.x, brk.y\n",
    "                \n",
    "\n",
    "                rand1, rand2 = random.randint(1,3),random.randint(1,3)\n",
    "                barracks_xy = (brk_x + rand1, brk_y + rand2) if self.base_top_left else (brk_x - rand1, brk_y - rand2)\n",
    "                if 0 < barracks_xy[0] < 64 and 0 < barracks_xy[1] < 64:\n",
    "                    pass\n",
    "                else:\n",
    "                    return actions.RAW_FUNCTIONS.no_op()\n",
    "                    \n",
    "\n",
    "                distances = self.get_distances(obs, scvs, barracks_xy)\n",
    "                scv = scvs[np.argmin(distances)]\n",
    "                return actions.RAW_FUNCTIONS.Build_Barracks_pt(\n",
    "                    \"now\", scv.tag, barracks_xy)\n",
    "        \n",
    "        \n",
    "        \n",
    "        return actions.RAW_FUNCTIONS.no_op()\n",
    "\n",
    "    def train_marine(self, obs):\n",
    "        \n",
    "        ################# 아머리가 완성된 후 부터 토르생산 ######################\n",
    "        completed_barrackses = self.get_my_completed_units_by_type(\n",
    "            obs, units.Terran.Barracks)\n",
    "        \n",
    "        completed_factorys = self.get_my_completed_units_by_type(\n",
    "            obs, units.Terran.Factory)\n",
    "        \n",
    "        completed_armorys = self.get_my_completed_units_by_type(\n",
    "            obs, units.Terran.Armory)\n",
    "        \n",
    "        marines = self.get_my_units_by_type(obs, units.Terran.Marine)\n",
    "        \n",
    "        \n",
    "        free_supply = (obs.observation.player.food_cap -\n",
    "                       obs.observation.player.food_used)\n",
    "        \n",
    "        \n",
    "        if (len(completed_barrackses) > 0 and obs.observation.player.minerals >= 100\n",
    "                and free_supply > 0 and len(completed_armorys) == 0):\n",
    "            barracks = self.get_my_units_by_type(obs, units.Terran.Barracks)[0]\n",
    "            if barracks.order_length < 5:\n",
    "                return actions.RAW_FUNCTIONS.Train_Marine_quick(\"now\", barracks.tag)\n",
    "        \n",
    "        elif free_supply > 0 and len(completed_factorys) > 0 and len(completed_armorys) > 0:\n",
    "            factory = completed_factorys[0]\n",
    "            if factory.order_length < 5:\n",
    "                return actions.RAW_FUNCTIONS.Train_Thor_quick(\"now\", factory.tag)\n",
    "        \n",
    "        return actions.RAW_FUNCTIONS.no_op()\n",
    "    \n",
    "    ###############################################################################################\n",
    "    ###################################### Factorys ###############################################\n",
    "    ###############################################################################################\n",
    "    \n",
    "    def build_factorys(self, obs):\n",
    "        completed_barrackses = self.get_my_completed_units_by_type(\n",
    "            obs, units.Terran.Barracks)\n",
    "        \n",
    "        factorys = self.get_my_units_by_type(obs, units.Terran.Factory)\n",
    "        scvs = self.get_my_units_by_type(obs, units.Terran.SCV)\n",
    "        ref = self.get_my_completed_units_by_type(obs,units.Terran.Refinery)\n",
    "        # print(\"gas: \", obs.observation.player.minerals)\n",
    "        # print(\"gas: \", obs.observation.player.gas)\n",
    "        if (len(completed_barrackses) > 0 and\n",
    "            obs.observation.player.minerals >= 200 and\n",
    "            len(factorys) < 3 and\n",
    "            len(scvs) > 0):\n",
    "            \n",
    "            if len(factorys) >= 1 and len(ref) < 4: # 가스부족시 가스 건설\n",
    "                refinerys = self.get_my_units_by_type(obs,units.Terran.Refinery)\n",
    "                scvs = self.get_my_units_by_type(obs, units.Terran.SCV)\n",
    "\n",
    "                if (obs.observation.player.minerals >= 100 and\n",
    "                        len(scvs) > 0):\n",
    "                    gas = self.get_my_units_by_type(obs, units.Neutral.VespeneGeyser)[0]\n",
    "\n",
    "                    if self.base_top_left:\n",
    "                        gases = self.top_left_gas_xy\n",
    "                    else:\n",
    "                        gases = self.bottom_right_gas_xy\n",
    "\n",
    "                    rc = np.random.choice([0,1,2,3])\n",
    "                    gas_xy = gases[rc]\n",
    "                    if (gas.x, gas.y) == gas_xy:\n",
    "                        distances = self.get_distances(obs, scvs, gas_xy)\n",
    "                        scv = scvs[np.argmin(distances)]\n",
    "\n",
    "                        return actions.RAW_FUNCTIONS.Build_Refinery_pt(\n",
    "                            \"now\", scv.tag, gas.tag)\n",
    "            \n",
    "            if len(factorys) >= 1:\n",
    "                rand1 = random.randint(-5,5)\n",
    "                fx, fy = factorys[0].x, factorys[0].y\n",
    "                factorys_xy = (fx + rand1, fy + rand1) if self.base_top_left else (fx - rand1, fy - rand1)\n",
    "                \n",
    "            else:\n",
    "                rand1, rand2 = random.randint(-2,2), random.randint(-2,2) # x, y\n",
    "                factorys_xy = (39 + rand1, 25 + rand2) if self.base_top_left else (17 - rand1, 40 - rand2)\n",
    "\n",
    "                \n",
    "            if 0 < factorys_xy[0] < 64 and 0 < factorys_xy[1] < 64 and factorys_xy != (17,48) and factorys_xy != (41,21):\n",
    "                pass\n",
    "            else:\n",
    "                return actions.RAW_FUNCTIONS.no_op()\n",
    "\n",
    "\n",
    "            distances = self.get_distances(obs, scvs, factorys_xy)\n",
    "            scv = scvs[np.argmin(distances)]\n",
    "            return actions.RAW_FUNCTIONS.Build_Factory_pt(\n",
    "                \"now\", scv.tag, factorys_xy)\n",
    "        return actions.RAW_FUNCTIONS.no_op()\n",
    "    \n",
    "    def build_techlab_factorys(self, obs):\n",
    "        completed_factorys = self.get_my_completed_units_by_type(\n",
    "            obs, units.Terran.Factory)\n",
    "\n",
    "        \n",
    "        scvs = self.get_my_units_by_type(obs, units.Terran.SCV)\n",
    "        \n",
    "        if (len(completed_factorys) > 0 and \n",
    "            obs.observation.player.minerals >= 200):\n",
    "            \n",
    "            ftrs = self.get_my_units_by_type(obs, units.Terran.Factory)\n",
    "            \n",
    "            if ftrs:\n",
    "                for ftr in ftrs:\n",
    "                    ftr_x,ftr_y = ftr.x, ftr.y\n",
    "            \n",
    "                factorys_xy = (ftr_x,ftr_y)\n",
    "                if 0 < factorys_xy[0] < 64 and 0 < factorys_xy[1] < 64:\n",
    "                    pass\n",
    "                else:\n",
    "                    return actions.RAW_FUNCTIONS.no_op()\n",
    "\n",
    "                return actions.RAW_FUNCTIONS.Build_TechLab_Factory_pt(\n",
    "                    \"now\", ftr.tag, factorys_xy)\n",
    "            \n",
    "        return actions.RAW_FUNCTIONS.no_op()\n",
    "    \n",
    "    def train_tank(self, obs):\n",
    "        completed_factorytechlab = self.get_my_completed_units_by_type(\n",
    "            obs, units.Terran.FactoryTechLab)\n",
    "        \n",
    "        free_supply = (obs.observation.player.food_cap -\n",
    "                       obs.observation.player.food_used)\n",
    "        \n",
    "        if (len(completed_factorytechlab) > 0 and obs.observation.player.minerals >= 200):\n",
    "            \n",
    "            factorys = self.get_my_units_by_type(obs, units.Terran.Factory)[0]\n",
    "            \n",
    "            if factorys.order_length < 5:\n",
    "                return actions.RAW_FUNCTIONS.Train_SiegeTank_quick(\"now\", factorys.tag)\n",
    "        return actions.RAW_FUNCTIONS.no_op()\n",
    "    \n",
    "    ###############################################################################\n",
    "    ############################ Build Armory ##################################\n",
    "    \n",
    "    def build_armorys(self, obs):\n",
    "        completed_factory = self.get_my_completed_units_by_type(\n",
    "            obs, units.Terran.Factory)\n",
    "        \n",
    "        armorys = self.get_my_units_by_type(obs, units.Terran.Armory)\n",
    "        \n",
    "        scvs = self.get_my_units_by_type(obs, units.Terran.SCV)\n",
    "        \n",
    "        if (len(completed_factory) > 0 and\n",
    "            obs.observation.player.minerals >= 200 and\n",
    "            len(armorys) < 2 and\n",
    "            len(scvs) > 0):\n",
    "            \n",
    "            rand1, rand2 = random.randint(-2,2),random.randint(-2,2)\n",
    "            armorys_xy = (36 + rand1, 20 + rand2) if self.base_top_left else ( 20 - rand1, 50 - rand2)\n",
    "            if 0 < armorys_xy[0] < 64 and 0 < armorys_xy[1] < 64:\n",
    "                pass\n",
    "            else:\n",
    "                return actions.RAW_FUNCTIONS.no_op()\n",
    "\n",
    "\n",
    "            distances = self.get_distances(obs, scvs, armorys_xy)\n",
    "            scv = scvs[np.argmin(distances)]\n",
    "            return actions.RAW_FUNCTIONS.Build_Armory_pt(\n",
    "                \"now\", scv.tag, armorys_xy)\n",
    "\n",
    "        elif (len(completed_factory) > 0 and\n",
    "            obs.observation.player.minerals >= 200 and\n",
    "            1 <= len(armorys) and\n",
    "            len(scvs) > 0):\n",
    "            # armory upgrade 추가\n",
    "            armory = armorys[0]\n",
    "            \n",
    "            armory_xy = (armory.x, armory.y)\n",
    "            #cloak_field = self.get_my_units_by_type(obs, upgrades.Upgrades.CloakingField)[0]\n",
    "            if self.TerranVehicleWeaponsLevel1 == False:\n",
    "                self.TerranVehicleWeaponsLevel1 = True\n",
    "                return actions.RAW_FUNCTIONS.Research_TerranVehicleWeapons_quick(\"now\", armory.tag)\n",
    "            \n",
    "            elif self.TerranVehicleWeaponsLevel1 == True and self.TerranVehicleWeaponsLevel2 == False:\n",
    "                self.TerranVehicleWeaponsLevel2 = True\n",
    "                return actions.RAW_FUNCTIONS.Research_TerranVehicleWeaponsLevel2_quick(\"now\", armory.tag)\n",
    "            \n",
    "            elif self.TerranVehicleWeaponsLevel1 == True and self.TerranVehicleWeaponsLevel2 == True and self.TerranVehicleWeaponsLevel3 == False:\n",
    "                self.TerranVehicleWeaponsLevel3 = True\n",
    "                return actions.RAW_FUNCTIONS.Research_TerranVehicleWeaponsLevel3_quick(\"now\", armory.tag)\n",
    "            \n",
    "            \n",
    "            \n",
    "        return actions.RAW_FUNCTIONS.no_op()\n",
    "    \n",
    "    \n",
    "    ############################################################################################\n",
    "    #################################### StarPort ##############################################\n",
    "    def build_starports(self, obs):\n",
    "        completed_factorys = self.get_my_completed_units_by_type(\n",
    "            obs, units.Terran.Factory)\n",
    "        \n",
    "        starports = self.get_my_units_by_type(obs, units.Terran.Starport)\n",
    "        scvs = self.get_my_units_by_type(obs, units.Terran.SCV)\n",
    "        \n",
    "        if (len(completed_factorys) > 0 and\n",
    "            obs.observation.player.minerals >= 200 and \n",
    "            len(starports) < 1 and\n",
    "            len(scvs) > 0):\n",
    "            \n",
    "            # stp_x,stp_y = (22,22), (36,46) # minerals기준 중앙부쪽 좌표\n",
    "            \n",
    "            if len(starports) >= 1:\n",
    "                rand1 = random.randint(-5,5)\n",
    "                sx, sy = starports[0].x, starports[0].y\n",
    "                starport_xy = (sx + rand1, sy + rand1) if self.base_top_left else (sx - rand1, sy - rand1)\n",
    "            else:\n",
    "                rand1, rand2 = random.randint(-5,5),random.randint(-5,5)\n",
    "                starport_xy = (22 + rand1, 22 + rand2) if self.base_top_left else (36 - rand1, 46 - rand2)\n",
    "\n",
    "            \n",
    "            if 0 < starport_xy[0] < 64 and 0 < starport_xy[1] < 64:\n",
    "                pass\n",
    "            else:\n",
    "                return actions.RAW_FUNCTIONS.no_op()\n",
    "\n",
    "            distances = self.get_distances(obs, scvs, starport_xy)\n",
    "            scv = scvs[np.argmin(distances)]\n",
    "            return actions.RAW_FUNCTIONS.Build_Starport_pt(\n",
    "                \"now\", scv.tag, starport_xy)\n",
    "        \n",
    "        ####################### 스타포트 건설 후 팩토리 증설 #########################\n",
    "        elif (len(starports) >= 1 and obs.observation.player.minerals >= 200 and\n",
    "              len(completed_factorys) < 4 and len(scvs) > 0):\n",
    "            \n",
    "            if len(starports) >= 1:\n",
    "                rand1 = random.randint(-5,5)\n",
    "                sx, sy = starports[0].x, starports[0].y\n",
    "                factory_xy = (sx + rand1, sy + rand1) if self.base_top_left else (sx - rand1, sy - rand1)\n",
    "            else:\n",
    "                rand1, rand2 = random.randint(-5,5),random.randint(-5,5)\n",
    "                factory_xy = (22 + rand1, 22 + rand2) if self.base_top_left else (36 - rand1, 46 - rand2)\n",
    "\n",
    "            \n",
    "            if 0 < factory_xy[0] < 64 and 0 < factory_xy[1] < 64:\n",
    "                pass\n",
    "            else:\n",
    "                return actions.RAW_FUNCTIONS.no_op()\n",
    "\n",
    "            distances = self.get_distances(obs, scvs, factory_xy)\n",
    "            scv = scvs[np.argmin(distances)]\n",
    "            return actions.RAW_FUNCTIONS.Build_Factory_pt(\n",
    "                \"now\", scv.tag, factory_xy)\n",
    "        \n",
    "        else:\n",
    "            completed_barrackses = self.get_my_completed_units_by_type(\n",
    "            obs, units.Terran.Barracks)\n",
    "            marines = self.get_my_units_by_type(obs, units.Terran.Marine)\n",
    "\n",
    "            free_supply = (obs.observation.player.food_cap -\n",
    "                           obs.observation.player.food_used)\n",
    "\n",
    "            if (len(completed_barrackses) > 0 and obs.observation.player.minerals >= 100\n",
    "                    and free_supply > 0):\n",
    "                barracks = self.get_my_units_by_type(obs, units.Terran.Barracks)[0]\n",
    "                if barracks.order_length < 5:\n",
    "                    return actions.RAW_FUNCTIONS.Train_Marine_quick(\"now\", barracks.tag)\n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "        return actions.RAW_FUNCTIONS.no_op()\n",
    "    \n",
    "    def build_techlab_starports(self, obs):\n",
    "        completed_starports = self.get_my_completed_units_by_type(\n",
    "            obs, units.Terran.Starport)\n",
    "        \n",
    "        completed_starport_techlab = self.get_my_completed_units_by_type(\n",
    "            obs, units.Terran.StarportTechLab)\n",
    "                \n",
    "        if (len(completed_starports) < 3 and \n",
    "            obs.observation.player.minerals >= 200):\n",
    "            stps = self.get_my_units_by_type(obs, units.Terran.Starport)\n",
    "            \n",
    "            if stps:\n",
    "                for stp in stps:\n",
    "                    stp_x,stp_y = stp.x, stp.y\n",
    "                    \n",
    "                starport_xy = (stp_x,stp_y)\n",
    "\n",
    "                return actions.RAW_FUNCTIONS.Build_TechLab_Starport_pt(\n",
    "                    \"now\", stp.tag, starport_xy)\n",
    "            \n",
    "        ############ Cloak upgrade #########################\n",
    "        if len(completed_starport_techlab) > 0 and self.cloaking_flag:\n",
    "            # self.cloaking_flag = 0\n",
    "            cloaking = upgrades.Upgrades.CloakingField\n",
    "            \n",
    "            stp_techlab = self.get_my_units_by_type(obs, units.Terran.StarportTechLab)\n",
    "            if stp_techlab:\n",
    "                stp_tech_xy = (stp_techlab[0].x, stp_techlab[0].y)\n",
    "                cloak_field = self.get_my_units_by_type(obs, upgrades.Upgrades.CloakingField)[0]\n",
    "                \n",
    "#                 print(\"stp_tech_xy: \", stp_tech_xy)\n",
    "#                 print(\"cloaking upgrade: \",cloak_field.tag)\n",
    "                return actions.FUNCTIONS.Research_BansheeCloakingField_quick(\"now\", cloaking )\n",
    "                \n",
    "        return actions.RAW_FUNCTIONS.no_op()\n",
    "    \n",
    "    def train_banshee(self, obs):\n",
    "        completed_starporttechlab = self.get_my_completed_units_by_type(\n",
    "            obs, units.Terran.StarportTechLab)\n",
    "        \n",
    "        ravens = self.get_my_units_by_type(obs, units.Terran.Raven)\n",
    "        \n",
    "        free_supply = (obs.observation.player.food_cap -\n",
    "                       obs.observation.player.food_used)\n",
    "        \n",
    "        \n",
    "        if (len(completed_starporttechlab) > 0 and obs.observation.player.minerals >= 200\n",
    "                and free_supply > 3):\n",
    "            \n",
    "            starports = self.get_my_units_by_type(obs, units.Terran.Starport)[0]\n",
    "            \n",
    "            ############################### cloaking detecting을 위한 Raven 생산 #######################\n",
    "            if starports.order_length < 2 and len(ravens) < 3 :\n",
    "                return actions.RAW_FUNCTIONS.Train_Raven_quick(\"now\", starports.tag)\n",
    "            \n",
    "            #########################################################################################\n",
    "            \n",
    "            if starports.order_length < 5:\n",
    "                return actions.RAW_FUNCTIONS.Train_Banshee_quick(\"now\", starports.tag)\n",
    "        return actions.RAW_FUNCTIONS.no_op()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ############################################################################################\n",
    "   \n",
    "    def attack(self, obs):\n",
    "        marines = self.get_my_units_by_type(obs, units.Terran.Marine)\n",
    "        if 20 < len(marines):\n",
    "            \n",
    "            flag = random.randint(0,2)\n",
    "            if flag == 1:\n",
    "                attack_xy = (38, 44) if self.base_top_left else (19, 23)\n",
    "            else:\n",
    "                attack_xy = (16, 45) if self.base_top_left else (42, 19)\n",
    "            \n",
    "            \n",
    "            distances = self.get_distances(obs, marines, attack_xy)\n",
    "            marine = marines[np.argmax(distances)]\n",
    "            #marine = marines\n",
    "            \n",
    "            x_offset = random.randint(-5, 5)\n",
    "            y_offset = random.randint(-5, 5)\n",
    "            return actions.RAW_FUNCTIONS.Attack_pt(\n",
    "                \"now\", marine.tag, (attack_xy[0] + x_offset, attack_xy[1] + y_offset))\n",
    "        else:\n",
    "            barracks = self.get_my_units_by_type(obs, units.Terran.Barracks)\n",
    "            if len(barracks) > 0:\n",
    "                barracks = barracks[0]\n",
    "                if barracks.order_length < 5:\n",
    "                    return actions.RAW_FUNCTIONS.Train_Marine_quick(\"now\", barracks.tag)\n",
    "\n",
    "        return actions.RAW_FUNCTIONS.no_op()\n",
    "    \n",
    "    def attack_all(self,obs):\n",
    "        # 추가 유닛 생길 때 마다 추가\n",
    "        marines = self.get_my_units_by_type(obs, units.Terran.Marine)\n",
    "        tanks = self.get_my_units_by_type(obs, units.Terran.SiegeTank)\n",
    "        banshees = self.get_my_units_by_type(obs, units.Terran.Banshee)\n",
    "        raven = self.get_my_units_by_type(obs, units.Terran.Raven)\n",
    "        thor = self.get_my_units_by_type(obs, units.Terran.Thor)\n",
    "        \n",
    "        sieged_tanks = self.get_my_units_by_type(obs, units.Terran.SiegeTankSieged)\n",
    "        total_tanks = tanks + sieged_tanks\n",
    "        \n",
    "        all_units = marines + total_tanks + banshees + raven + thor\n",
    "        \n",
    "        if 25 < len(all_units):\n",
    "            \n",
    "            flag = random.randint(0,1000)\n",
    "            \n",
    "            if flag%4 == 0:\n",
    "                attack_xy = (39, 45) if self.base_top_left else (19, 23)\n",
    "            elif flag%4 == 1:\n",
    "                \n",
    "                attack_xy = (39, 45) if self.base_top_left else (19, 23)\n",
    "                \n",
    "                if len(tanks) > 0:\n",
    "                    distances = self.get_distances(obs, tanks, attack_xy)\n",
    "                    tank = tanks[np.argmax(distances)]\n",
    "                    x_offset = random.randint(-1, 1)\n",
    "                    y_offset = random.randint(-1, 1)\n",
    "                    return actions.RAW_FUNCTIONS.Morph_SiegeMode_quick(\n",
    "                        \"now\", tank.tag, (attack_xy[0] + x_offset, attack_xy[1] + y_offset))\n",
    "            \n",
    "            elif flag%4 == 2:\n",
    "                attack_xy = (39, 45) if self.base_top_left else (19, 23)\n",
    "                #### siegeMode 제거 ####\n",
    "                if len(total_tanks) > 0:\n",
    "                    all_tanks_tag = [tank.tag for tank in total_tanks]\n",
    "\n",
    "                    return actions.RAW_FUNCTIONS.Morph_Unsiege_quick(\n",
    "                        \"now\", all_tanks_tag)\n",
    "            \n",
    "            else:\n",
    "                attack_xy = (17, 48) if self.base_top_left else (41, 21)\n",
    "            \n",
    "            x_offset = random.randint(-5, 5)\n",
    "            y_offset = random.randint(-5, 5)\n",
    "            \n",
    "            all_tag = [unit.tag for unit in all_units]\n",
    "            \n",
    "            return actions.RAW_FUNCTIONS.Attack_pt(\n",
    "                \"now\", all_tag, (attack_xy[0] + x_offset, attack_xy[1] + y_offset))\n",
    "        else:\n",
    "            flag = random.randint(0,1000)\n",
    "            if flag%4 == 0:\n",
    "                attack_xy = (35, 25) if self.base_top_left else (25, 40)\n",
    "            elif flag%4 == 1:\n",
    "                attack_xy = (35, 25) if self.base_top_left else (25, 40)\n",
    "\n",
    "                if len(tanks) > 0:\n",
    "                    tanks_tag = [tank.tag for tank in tanks]\n",
    "                    x_offset = random.randint(-1, 1)\n",
    "                    y_offset = random.randint(-1, 1)\n",
    "                    return actions.RAW_FUNCTIONS.Morph_SiegeMode_quick(\n",
    "                        \"now\", tanks_tag, (attack_xy[0] + x_offset, attack_xy[1] + y_offset))\n",
    "                \n",
    "                \n",
    "            elif flag%4 == 2:\n",
    "                attack_xy = (35, 25) if self.base_top_left else (25, 40)\n",
    "                \n",
    "            else:\n",
    "                attack_xy = (30, 25) if self.base_top_left else (33, 40)\n",
    "            \n",
    "            x_offset = random.randint(-1, 1)\n",
    "            y_offset = random.randint(-1, 1)\n",
    "            \n",
    "            \n",
    "            \n",
    "            all_units = marines + banshees + raven + thor\n",
    "            all_tag = [unit.tag for unit in all_units]\n",
    "            if all_tag:\n",
    "                return actions.RAW_FUNCTIONS.Attack_pt(\n",
    "                    \"now\", all_tag, (attack_xy[0] + x_offset, attack_xy[1] + y_offset))\n",
    "            \n",
    "        return actions.RAW_FUNCTIONS.no_op()\n",
    "    \n",
    "    ###################################################################################\n",
    "    ############################### Unit Controls #####################################\n",
    "    \n",
    "    def tank_control(self, obs):\n",
    "        tanks = self.get_my_units_by_type(obs, units.Terran.SiegeTank)\n",
    "        sieged_tanks = self.get_my_units_by_type(obs, units.Terran.SiegeTankSieged)\n",
    "        \n",
    "        total_tanks = tanks + sieged_tanks\n",
    "        \n",
    "        if len(total_tanks) < 8:\n",
    "            \n",
    "            if tanks:\n",
    "            \n",
    "                attack_xy = (40, 25) if self.base_top_left else (25, 40)\n",
    "\n",
    "                distances = self.get_distances(obs, tanks, attack_xy)\n",
    "                distances.sort()\n",
    "                \n",
    "                tank_tag = [t.tag for t in tanks[:4]]\n",
    "\n",
    "                x_offset = random.randint(-5, 5)\n",
    "                y_offset = random.randint(-5, 5)\n",
    "                return actions.RAW_FUNCTIONS.Morph_SiegeMode_quick(\n",
    "                    \"now\", tank_tag, (attack_xy[0] + x_offset, attack_xy[1] + y_offset))\n",
    "        else:\n",
    "            #### siegeMode 제거 ####\n",
    "            all_tanks_tag = [tank.tag for tank in total_tanks]\n",
    "            return actions.RAW_FUNCTIONS.Morph_Unsiege_quick(\n",
    "                \"now\", all_tanks_tag)\n",
    "        \n",
    "        return actions.RAW_FUNCTIONS.no_op()\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TerranRandomAgent(TerranAgentWithRawActsAndRawObs):\n",
    "    def step(self, obs):\n",
    "        super(TerranRandomAgent, self).step(obs)\n",
    "        action = random.choice(self.actions)\n",
    "        \n",
    "        return getattr(self, action)(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter\n",
    "\n",
    "하이퍼파라미터는 심층강화학습 알고리즘에서 성능에 매우 큰 영향을 미칩니다.\n",
    "이 실험에 쓰인 하이퍼파라미터는 https://github.com/chucnorrisful/dqn 실험에서 제안된 값들을 참고하였습니다.\n",
    "\n",
    "\n",
    "- self.s_dim = 21\n",
    "- self.a_dim = 6\n",
    "\n",
    "- self.lr = 1e-4 * 1\n",
    "- self.batch_size = 32\n",
    "- self.gamma = 0.99\n",
    "- self.memory_size = 200000\n",
    "- self.eps_max = 1.0\n",
    "- self.eps_min = 0.01\n",
    "- self.epsilon = 1.0\n",
    "- self.init_sampling = 4000\n",
    "- self.target_update_interval = 10\n",
    "\n",
    "- self.epsilon = max(self.eps_min, self.eps_max - self.eps_min * (self.episode_count / 50))\n",
    "\n",
    "\n",
    "![Winning rate graph](./images/rlagent_with_vanilla_dqn_score-Terran-Terran-495_Eps.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TerranRLAgentWithRawActsAndRawObs(TerranAgentWithRawActsAndRawObs):\n",
    "    def __init__(self):\n",
    "        super(TerranRLAgentWithRawActsAndRawObs, self).__init__()\n",
    "\n",
    "        self.s_dim = 21\n",
    "        self.a_dim = 19\n",
    "        \n",
    "        self.lr = 1e-4 * 1\n",
    "        self.batch_size = 32\n",
    "        self.gamma = 0.99\n",
    "        self.memory_size = 200000\n",
    "        self.eps_max = 1.0\n",
    "        self.eps_min = 0.01\n",
    "        self.epsilon = 1.0\n",
    "        self.init_sampling = 4000\n",
    "        self.target_update_interval = 10\n",
    "\n",
    "        self.data_file_qnet = DATA_FILE_QNET\n",
    "        self.data_file_qnet_target = DATA_FILE_QNET_TARGET\n",
    "        self.score_file = SCORE_FILE\n",
    "        \n",
    "        self.qnetwork = NaiveMultiLayerPerceptron(input_dim=self.s_dim,\n",
    "                           output_dim=self.a_dim,\n",
    "                           num_neurons=[128],\n",
    "                           hidden_act_func='ReLU',\n",
    "                           out_act_func='Identity').to(device)\n",
    "        \n",
    "        self.qnetwork_target = NaiveMultiLayerPerceptron(input_dim=self.s_dim,\n",
    "                           output_dim=self.a_dim,\n",
    "                           num_neurons=[128],\n",
    "                           hidden_act_func='ReLU',\n",
    "                           out_act_func='Identity').to(device)\n",
    "        \n",
    "        ############################################ qnet 로드하면 이전 모델이라 학습모델 인풋 아웃풋차원이 바뀜 #########\n",
    "        if os.path.isfile(self.data_file_qnet + '.pt'):\n",
    "           self.qnetwork.load_state_dict(torch.load(self.data_file_qnet + '.pt'))\n",
    "            \n",
    "        if os.path.isfile(self.data_file_qnet_target + '.pt'):\n",
    "           self.qnetwork_target.load_state_dict(torch.load(self.data_file_qnet_target + '.pt'))\n",
    "        \n",
    "        # initialize target network same as the main network.\n",
    "        self.qnetwork_target.load_state_dict(self.qnetwork.state_dict())\n",
    "\n",
    "        self.dqn = DQN(state_dim=self.s_dim,\n",
    "                             action_dim=self.a_dim,\n",
    "                             qnet=self.qnetwork,\n",
    "                             qnet_target=self.qnetwork_target,\n",
    "                             lr=self.lr,\n",
    "                             gamma=self.gamma,\n",
    "                             epsilon=self.epsilon).to(device)\n",
    "        \n",
    "        self.memory = ExperienceReplayMemory(self.memory_size)\n",
    "        \n",
    "        self.print_every = 1\n",
    "        self.cum_reward = 0\n",
    "        self.cum_loss = 0\n",
    "        self.episode_count = 0\n",
    "        \n",
    "        self.new_game()\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        super(TerranRLAgentWithRawActsAndRawObs, self).reset()\n",
    "        self.new_game()\n",
    "\n",
    "    def new_game(self):\n",
    "        self.base_top_left = None\n",
    "        self.previous_state = None\n",
    "        self.previous_action = None\n",
    "        self.cum_reward = 0\n",
    "        self.cum_loss = 0\n",
    "        \n",
    "        # epsilon scheduling\n",
    "        # slowly decaying_epsilon\n",
    "        self.epsilon = max(self.eps_min, self.eps_max - self.eps_min * (self.episode_count / 50))\n",
    "        self.dqn.epsilon = torch.tensor(self.epsilon).to(device)\n",
    "        \n",
    "\n",
    "    def get_state(self, obs):\n",
    "        scvs = self.get_my_units_by_type(obs, units.Terran.SCV)\n",
    "        idle_scvs = [scv for scv in scvs if scv.order_length == 0]\n",
    "        command_centers = self.get_my_units_by_type(obs, units.Terran.CommandCenter)\n",
    "        supply_depots = self.get_my_units_by_type(obs, units.Terran.SupplyDepot)\n",
    "        completed_supply_depots = self.get_my_completed_units_by_type(\n",
    "            obs, units.Terran.SupplyDepot)\n",
    "        barrackses = self.get_my_units_by_type(obs, units.Terran.Barracks)\n",
    "        completed_barrackses = self.get_my_completed_units_by_type(\n",
    "            obs, units.Terran.Barracks)\n",
    "        marines = self.get_my_units_by_type(obs, units.Terran.Marine)\n",
    "\n",
    "        queued_marines = (completed_barrackses[0].order_length\n",
    "        if len(completed_barrackses) > 0 else 0)\n",
    "\n",
    "        free_supply = (obs.observation.player.food_cap -\n",
    "                       obs.observation.player.food_used)\n",
    "        can_afford_supply_depot = obs.observation.player.minerals >= 100\n",
    "        can_afford_barracks = obs.observation.player.minerals >= 150\n",
    "        can_afford_marine = obs.observation.player.minerals >= 100\n",
    "\n",
    "        enemy_scvs = self.get_enemy_units_by_type(obs, units.Terran.SCV)\n",
    "        enemy_idle_scvs = [scv for scv in enemy_scvs if scv.order_length == 0]\n",
    "        enemy_command_centers = self.get_enemy_units_by_type(\n",
    "            obs, units.Terran.CommandCenter)\n",
    "        enemy_supply_depots = self.get_enemy_units_by_type(\n",
    "            obs, units.Terran.SupplyDepot)\n",
    "        enemy_completed_supply_depots = self.get_enemy_completed_units_by_type(\n",
    "            obs, units.Terran.SupplyDepot)\n",
    "        enemy_barrackses = self.get_enemy_units_by_type(obs, units.Terran.Barracks)\n",
    "        enemy_completed_barrackses = self.get_enemy_completed_units_by_type(\n",
    "            obs, units.Terran.Barracks)\n",
    "        enemy_marines = self.get_enemy_units_by_type(obs, units.Terran.Marine)\n",
    "\n",
    "        return (len(command_centers),\n",
    "                len(scvs),\n",
    "                len(idle_scvs),\n",
    "                len(supply_depots),\n",
    "                len(completed_supply_depots),\n",
    "                len(barrackses),\n",
    "                len(completed_barrackses),\n",
    "                len(marines),\n",
    "                queued_marines,\n",
    "                free_supply,\n",
    "                can_afford_supply_depot,\n",
    "                can_afford_barracks,\n",
    "                can_afford_marine,\n",
    "                len(enemy_command_centers),\n",
    "                len(enemy_scvs),\n",
    "                len(enemy_idle_scvs),\n",
    "                len(enemy_supply_depots),\n",
    "                len(enemy_completed_supply_depots),\n",
    "                len(enemy_barrackses),\n",
    "                len(enemy_completed_barrackses),\n",
    "                len(enemy_marines))\n",
    "\n",
    "    def step(self, obs):\n",
    "        super(TerranRLAgentWithRawActsAndRawObs, self).step(obs)\n",
    "        \n",
    "        #time.sleep(0.5)\n",
    "        \n",
    "        state = self.get_state(obs)\n",
    "        state = torch.tensor(state).float().view(1, self.s_dim).to(device)\n",
    "        action_idx = self.dqn.choose_action(state)\n",
    "        action = self.actions[action_idx]\n",
    "        done = True if obs.last() else False\n",
    "\n",
    "        if self.previous_action is not None:\n",
    "            experience = (self.previous_state.to(device),\n",
    "                          torch.tensor(self.previous_action).view(1, 1).to(device),\n",
    "                          torch.tensor(obs.reward).view(1, 1).to(device),\n",
    "                          state.to(device),\n",
    "                          torch.tensor(done).view(1, 1).to(device))\n",
    "            self.memory.push(experience)\n",
    "        \n",
    "        self.cum_reward += obs.reward\n",
    "        self.previous_state = state\n",
    "        self.previous_action = action_idx\n",
    "        \n",
    "        if obs.last():\n",
    "            self.episode_count = self.episode_count + 1\n",
    "            \n",
    "            if len(self.memory) >= self.init_sampling:\n",
    "                # training dqn\n",
    "                sampled_exps = self.memory.sample(self.batch_size)\n",
    "                sampled_exps = prepare_training_inputs(sampled_exps, device)\n",
    "                self.dqn.learn(*sampled_exps)\n",
    "\n",
    "            if self.episode_count % self.target_update_interval == 0:\n",
    "                self.dqn.qnet_target.load_state_dict(self.dqn.qnet.state_dict())\n",
    "\n",
    "            if self.episode_count % self.print_every == 0:\n",
    "                msg = (self.episode_count, self.cum_reward, self.epsilon)\n",
    "                print(\"Episode : {:4.0f} | Cumulative Reward : {:4.0f} | Epsilon : {:.3f}\".format(*msg))\n",
    "            \n",
    "            torch.save(self.dqn.qnet.state_dict(), self.data_file_qnet + '.pt')\n",
    "            torch.save(self.dqn.qnet_target.state_dict(), self.data_file_qnet_target + '.pt')\n",
    "\n",
    "            scores_window.append(obs.reward)  # save most recent reward\n",
    "            win_rate = scores_window.count(1)/len(scores_window)*100\n",
    "            tie_rate = scores_window.count(0)/len(scores_window)*100\n",
    "            lost_rate = scores_window.count(-1)/len(scores_window)*100\n",
    "            \n",
    "            scores.append([win_rate, tie_rate, lost_rate])  # save most recent score(win_rate, tie_rate, lost_rate)\n",
    "            with open(self.score_file + '.txt', \"wb\") as fp:\n",
    "                pickle.dump(scores, fp)\n",
    "            \n",
    "            #writer.add_scalar(\"Loss/train\", self.cum_loss/obs.observation.game_loop, self.episode_count)\n",
    "            writer.add_scalar(\"Score\", self.cum_reward, self.episode_count)\n",
    "\n",
    "        return getattr(self, action)(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0922 23:23:02.756312 4616515008 sc_process.py:135] Launching SC2: /Applications/StarCraft II/Versions/Base81102/SC2.app/Contents/MacOS/SC2 -listen 127.0.0.1 -port 19112 -dataDir /Applications/StarCraft II/ -tempDir /var/folders/r1/x6k135_915z463fc7lc4hkp40000gn/T/sc-m9gntgxu/ -displayMode 0 -windowwidth 640 -windowheight 480 -windowx 50 -windowy 50\n",
      "I0922 23:23:02.777318 4616515008 remote_controller.py:167] Connecting to: ws://127.0.0.1:19112/sc2api, attempt: 0, running: True\n",
      "I0922 23:23:03.782161 4616515008 remote_controller.py:167] Connecting to: ws://127.0.0.1:19112/sc2api, attempt: 1, running: True\n",
      "I0922 23:23:04.785537 4616515008 remote_controller.py:167] Connecting to: ws://127.0.0.1:19112/sc2api, attempt: 2, running: True\n",
      "I0922 23:23:05.792152 4616515008 remote_controller.py:167] Connecting to: ws://127.0.0.1:19112/sc2api, attempt: 3, running: True\n",
      "I0922 23:23:06.797986 4616515008 remote_controller.py:167] Connecting to: ws://127.0.0.1:19112/sc2api, attempt: 4, running: True\n",
      "I0922 23:23:07.802450 4616515008 remote_controller.py:167] Connecting to: ws://127.0.0.1:19112/sc2api, attempt: 5, running: True\n",
      "I0922 23:23:08.808930 4616515008 remote_controller.py:167] Connecting to: ws://127.0.0.1:19112/sc2api, attempt: 6, running: True\n",
      "I0922 23:23:09.812354 4616515008 remote_controller.py:167] Connecting to: ws://127.0.0.1:19112/sc2api, attempt: 7, running: True\n",
      "I0922 23:23:10.816801 4616515008 remote_controller.py:167] Connecting to: ws://127.0.0.1:19112/sc2api, attempt: 8, running: True\n",
      "I0922 23:23:11.818795 4616515008 remote_controller.py:167] Connecting to: ws://127.0.0.1:19112/sc2api, attempt: 9, running: True\n",
      "I0922 23:23:12.820305 4616515008 remote_controller.py:167] Connecting to: ws://127.0.0.1:19112/sc2api, attempt: 10, running: True\n",
      "I0922 23:23:13.825726 4616515008 remote_controller.py:167] Connecting to: ws://127.0.0.1:19112/sc2api, attempt: 11, running: True\n",
      "I0922 23:23:14.828522 4616515008 remote_controller.py:167] Connecting to: ws://127.0.0.1:19112/sc2api, attempt: 12, running: True\n",
      "I0922 23:23:15.832547 4616515008 remote_controller.py:167] Connecting to: ws://127.0.0.1:19112/sc2api, attempt: 13, running: True\n",
      "I0922 23:23:16.834920 4616515008 remote_controller.py:167] Connecting to: ws://127.0.0.1:19112/sc2api, attempt: 14, running: True\n",
      "I0922 23:23:17.841595 4616515008 remote_controller.py:167] Connecting to: ws://127.0.0.1:19112/sc2api, attempt: 15, running: True\n",
      "I0922 23:23:18.845139 4616515008 remote_controller.py:167] Connecting to: ws://127.0.0.1:19112/sc2api, attempt: 16, running: True\n",
      "I0922 23:23:19.846853 4616515008 remote_controller.py:167] Connecting to: ws://127.0.0.1:19112/sc2api, attempt: 17, running: True\n",
      "I0922 23:23:20.848951 4616515008 remote_controller.py:167] Connecting to: ws://127.0.0.1:19112/sc2api, attempt: 18, running: True\n",
      "I0922 23:23:21.852277 4616515008 remote_controller.py:167] Connecting to: ws://127.0.0.1:19112/sc2api, attempt: 19, running: True\n",
      "I0922 23:23:22.856023 4616515008 remote_controller.py:167] Connecting to: ws://127.0.0.1:19112/sc2api, attempt: 20, running: True\n",
      "I0922 23:23:23.861757 4616515008 remote_controller.py:167] Connecting to: ws://127.0.0.1:19112/sc2api, attempt: 21, running: True\n",
      "I0922 23:23:33.645176 4616515008 sc2_env.py:314] Environment is ready\n",
      "I0922 23:23:33.653440 4616515008 sc2_env.py:507] Starting episode 1: [terran, random] on Simple64\n",
      "I0922 23:23:35.562937 4616515008 sc2_env.py:752] Environment Close\n",
      "I0922 23:25:39.113759 4616515008 sc2_env.py:725] Episode 1 finished after 15944 game steps. Outcome: [1], reward: [1], score: [11323]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode :    1 | Cumulative Reward :    1 | Epsilon : 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0922 23:25:43.969074 4616515008 sc2_env.py:507] Starting episode 2: [terran, random] on Simple64\n",
      "I0922 23:27:26.205519 4616515008 sc2_env.py:725] Episode 2 finished after 13624 game steps. Outcome: [1], reward: [1], score: [14085]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode :    2 | Cumulative Reward :    1 | Epsilon : 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0922 23:27:31.392760 4616515008 sc2_env.py:507] Starting episode 3: [terran, random] on Simple64\n",
      "I0922 23:29:50.744581 4616515008 sc2_env.py:725] Episode 3 finished after 17584 game steps. Outcome: [1], reward: [1], score: [16837]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode :    3 | Cumulative Reward :    1 | Epsilon : 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0922 23:29:55.972882 4616515008 sc2_env.py:507] Starting episode 4: [terran, random] on Simple64\n",
      "I0922 23:33:15.206000 4616515008 sc2_env.py:725] Episode 4 finished after 21608 game steps. Outcome: [1], reward: [1], score: [20204]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode :    4 | Cumulative Reward :    1 | Epsilon : 0.999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0922 23:33:20.697720 4616515008 sc2_env.py:507] Starting episode 5: [terran, random] on Simple64\n",
      "I0922 23:34:37.038655 4616515008 sc2_env.py:725] Episode 5 finished after 10144 game steps. Outcome: [1], reward: [1], score: [7631]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode :    5 | Cumulative Reward :    1 | Epsilon : 0.999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0922 23:34:42.260922 4616515008 sc2_env.py:507] Starting episode 6: [terran, random] on Simple64\n",
      "I0922 23:37:49.513190 4616515008 sc2_env.py:725] Episode 6 finished after 22728 game steps. Outcome: [1], reward: [1], score: [10307]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode :    6 | Cumulative Reward :    1 | Epsilon : 0.999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0922 23:37:54.725817 4616515008 sc2_env.py:507] Starting episode 7: [terran, random] on Simple64\n",
      "I0922 23:40:53.685406 4616515008 sc2_env.py:725] Episode 7 finished after 18912 game steps. Outcome: [1], reward: [1], score: [17154]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode :    7 | Cumulative Reward :    1 | Epsilon : 0.999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0922 23:40:59.228914 4616515008 sc2_env.py:507] Starting episode 8: [terran, random] on Simple64\n",
      "I0922 23:42:45.233356 4616515008 sc2_env.py:725] Episode 8 finished after 12920 game steps. Outcome: [1], reward: [1], score: [13930]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode :    8 | Cumulative Reward :    1 | Epsilon : 0.999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0922 23:42:50.265546 4616515008 sc2_env.py:507] Starting episode 9: [terran, random] on Simple64\n",
      "I0922 23:45:57.774312 4616515008 sc2_env.py:725] Episode 9 finished after 18968 game steps. Outcome: [1], reward: [1], score: [17073]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode :    9 | Cumulative Reward :    1 | Epsilon : 0.998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0922 23:46:02.923837 4616515008 sc2_env.py:507] Starting episode 10: [terran, random] on Simple64\n",
      "I0922 23:48:04.067924 4616515008 sc2_env.py:725] Episode 10 finished after 15400 game steps. Outcome: [1], reward: [1], score: [12114]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode :   10 | Cumulative Reward :    1 | Epsilon : 0.998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0922 23:48:09.402554 4616515008 sc2_env.py:507] Starting episode 11: [terran, random] on Simple64\n",
      "I0922 23:50:27.675535 4616515008 sc2_env.py:725] Episode 11 finished after 16352 game steps. Outcome: [1], reward: [1], score: [14944]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode :   11 | Cumulative Reward :    1 | Epsilon : 0.998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0922 23:50:32.610768 4616515008 sc2_env.py:507] Starting episode 12: [terran, random] on Simple64\n",
      "I0922 23:53:05.086646 4616515008 sc2_env.py:725] Episode 12 finished after 16920 game steps. Outcome: [1], reward: [1], score: [14927]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode :   12 | Cumulative Reward :    1 | Epsilon : 0.998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0922 23:53:10.053215 4616515008 sc2_env.py:507] Starting episode 13: [terran, random] on Simple64\n",
      "I0922 23:55:55.594042 4616515008 sc2_env.py:725] Episode 13 finished after 16592 game steps. Outcome: [1], reward: [1], score: [14637]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode :   13 | Cumulative Reward :    1 | Epsilon : 0.998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0922 23:56:02.144268 4616515008 sc2_env.py:507] Starting episode 14: [terran, random] on Simple64\n",
      "I0922 23:58:38.942584 4616515008 sc2_env.py:725] Episode 14 finished after 17280 game steps. Outcome: [1], reward: [1], score: [16656]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode :   14 | Cumulative Reward :    1 | Epsilon : 0.997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0922 23:58:44.627360 4616515008 sc2_env.py:507] Starting episode 15: [terran, random] on Simple64\n",
      "I0923 00:02:04.062047 4616515008 sc2_env.py:725] Episode 15 finished after 19544 game steps. Outcome: [1], reward: [1], score: [16921]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode :   15 | Cumulative Reward :    1 | Epsilon : 0.997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0923 00:02:09.639254 4616515008 sc2_env.py:507] Starting episode 16: [terran, random] on Simple64\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  app.run(main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Winning rate graph]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages (3.0.3)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: numpy>=1.10.0 in /Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages (from matplotlib) (1.18.5)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages (from kiwisolver>=1.0.1->matplotlib) (47.1.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.3 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.5/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "SCORE_FILE = 'rlagent_with_vanilla_dqn_score'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(SCORE_FILE + '.txt', \"rb\") as fp:\n",
    "    scores = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[100.        ,   0.        ,   0.        ],\n",
       "       [100.        ,   0.        ,   0.        ],\n",
       "       [100.        ,   0.        ,   0.        ],\n",
       "       [100.        ,   0.        ,   0.        ],\n",
       "       [ 80.        ,   0.        ,  20.        ],\n",
       "       [ 66.66666667,  16.66666667,  16.66666667],\n",
       "       [ 71.42857143,  14.28571429,  14.28571429],\n",
       "       [ 62.5       ,  25.        ,  12.5       ],\n",
       "       [ 66.66666667,  22.22222222,  11.11111111],\n",
       "       [ 70.        ,  20.        ,  10.        ],\n",
       "       [ 72.72727273,  18.18181818,   9.09090909],\n",
       "       [ 75.        ,  16.66666667,   8.33333333],\n",
       "       [ 76.92307692,  15.38461538,   7.69230769],\n",
       "       [ 78.57142857,  14.28571429,   7.14285714],\n",
       "       [ 73.33333333,  20.        ,   6.66666667],\n",
       "       [ 75.        ,  18.75      ,   6.25      ],\n",
       "       [ 76.47058824,  17.64705882,   5.88235294],\n",
       "       [ 77.77777778,  16.66666667,   5.55555556],\n",
       "       [ 78.94736842,  15.78947368,   5.26315789],\n",
       "       [ 80.        ,  15.        ,   5.        ],\n",
       "       [ 80.95238095,  14.28571429,   4.76190476],\n",
       "       [ 81.81818182,  13.63636364,   4.54545455],\n",
       "       [ 82.60869565,  13.04347826,   4.34782609],\n",
       "       [ 83.33333333,  12.5       ,   4.16666667],\n",
       "       [ 84.        ,  12.        ,   4.        ],\n",
       "       [ 84.61538462,  11.53846154,   3.84615385],\n",
       "       [ 81.48148148,  14.81481481,   3.7037037 ],\n",
       "       [ 82.14285714,  14.28571429,   3.57142857],\n",
       "       [ 82.75862069,  13.79310345,   3.44827586],\n",
       "       [ 83.33333333,  13.33333333,   3.33333333],\n",
       "       [ 80.64516129,  16.12903226,   3.22580645],\n",
       "       [ 81.25      ,  15.625     ,   3.125     ],\n",
       "       [ 81.81818182,  15.15151515,   3.03030303],\n",
       "       [ 79.41176471,  14.70588235,   5.88235294],\n",
       "       [ 80.        ,  14.28571429,   5.71428571],\n",
       "       [ 80.55555556,  13.88888889,   5.55555556],\n",
       "       [ 81.08108108,  13.51351351,   5.40540541],\n",
       "       [ 81.57894737,  13.15789474,   5.26315789],\n",
       "       [ 79.48717949,  15.38461538,   5.12820513],\n",
       "       [ 77.5       ,  15.        ,   7.5       ],\n",
       "       [ 78.04878049,  14.63414634,   7.31707317],\n",
       "       [ 76.19047619,  16.66666667,   7.14285714],\n",
       "       [ 74.41860465,  16.27906977,   9.30232558],\n",
       "       [ 72.72727273,  15.90909091,  11.36363636],\n",
       "       [ 71.11111111,  15.55555556,  13.33333333],\n",
       "       [ 71.73913043,  15.2173913 ,  13.04347826],\n",
       "       [ 72.34042553,  14.89361702,  12.76595745],\n",
       "       [ 72.91666667,  14.58333333,  12.5       ],\n",
       "       [ 71.42857143,  14.28571429,  14.28571429],\n",
       "       [ 72.        ,  14.        ,  14.        ],\n",
       "       [ 72.54901961,  13.7254902 ,  13.7254902 ],\n",
       "       [ 73.07692308,  13.46153846,  13.46153846],\n",
       "       [ 73.58490566,  13.20754717,  13.20754717],\n",
       "       [ 74.07407407,  12.96296296,  12.96296296],\n",
       "       [ 74.54545455,  12.72727273,  12.72727273],\n",
       "       [ 75.        ,  12.5       ,  12.5       ],\n",
       "       [ 75.43859649,  12.28070175,  12.28070175],\n",
       "       [ 75.86206897,  12.06896552,  12.06896552],\n",
       "       [ 74.57627119,  11.86440678,  13.55932203],\n",
       "       [ 75.        ,  11.66666667,  13.33333333],\n",
       "       [ 75.40983607,  11.47540984,  13.1147541 ],\n",
       "       [ 75.80645161,  11.29032258,  12.90322581],\n",
       "       [ 76.19047619,  11.11111111,  12.6984127 ],\n",
       "       [ 75.        ,  10.9375    ,  14.0625    ],\n",
       "       [ 73.84615385,  12.30769231,  13.84615385],\n",
       "       [ 74.24242424,  12.12121212,  13.63636364],\n",
       "       [ 74.62686567,  11.94029851,  13.43283582],\n",
       "       [ 75.        ,  11.76470588,  13.23529412],\n",
       "       [ 75.36231884,  11.5942029 ,  13.04347826],\n",
       "       [ 75.71428571,  11.42857143,  12.85714286],\n",
       "       [ 76.05633803,  11.26760563,  12.67605634],\n",
       "       [ 76.38888889,  11.11111111,  12.5       ],\n",
       "       [ 76.71232877,  10.95890411,  12.32876712],\n",
       "       [ 77.02702703,  10.81081081,  12.16216216],\n",
       "       [ 77.33333333,  10.66666667,  12.        ],\n",
       "       [ 77.63157895,  10.52631579,  11.84210526],\n",
       "       [ 76.62337662,  11.68831169,  11.68831169],\n",
       "       [ 76.92307692,  11.53846154,  11.53846154],\n",
       "       [ 77.21518987,  11.39240506,  11.39240506],\n",
       "       [ 77.5       ,  11.25      ,  11.25      ],\n",
       "       [ 77.77777778,  11.11111111,  11.11111111],\n",
       "       [ 76.82926829,  12.19512195,  10.97560976],\n",
       "       [ 77.10843373,  12.04819277,  10.84337349],\n",
       "       [ 77.38095238,  11.9047619 ,  10.71428571],\n",
       "       [ 77.64705882,  11.76470588,  10.58823529],\n",
       "       [ 77.90697674,  11.62790698,  10.46511628],\n",
       "       [ 77.01149425,  11.49425287,  11.49425287],\n",
       "       [ 77.27272727,  11.36363636,  11.36363636],\n",
       "       [ 77.52808989,  11.23595506,  11.23595506],\n",
       "       [ 76.66666667,  12.22222222,  11.11111111],\n",
       "       [ 76.92307692,  12.08791209,  10.98901099],\n",
       "       [ 76.08695652,  11.95652174,  11.95652174],\n",
       "       [ 76.34408602,  11.82795699,  11.82795699],\n",
       "       [ 76.59574468,  11.70212766,  11.70212766],\n",
       "       [ 76.84210526,  11.57894737,  11.57894737],\n",
       "       [ 77.08333333,  11.45833333,  11.45833333],\n",
       "       [ 77.31958763,  11.34020619,  11.34020619],\n",
       "       [ 77.55102041,  11.2244898 ,  11.2244898 ],\n",
       "       [ 77.77777778,  11.11111111,  11.11111111],\n",
       "       [ 78.        ,  11.        ,  11.        ],\n",
       "       [ 78.        ,  11.        ,  11.        ],\n",
       "       [ 78.        ,  11.        ,  11.        ],\n",
       "       [ 78.        ,  11.        ,  11.        ],\n",
       "       [ 78.        ,  11.        ,  11.        ],\n",
       "       [ 79.        ,  11.        ,  10.        ],\n",
       "       [ 80.        ,  10.        ,  10.        ],\n",
       "       [ 80.        ,  10.        ,  10.        ],\n",
       "       [ 81.        ,   9.        ,  10.        ],\n",
       "       [ 81.        ,   9.        ,  10.        ],\n",
       "       [ 81.        ,   9.        ,  10.        ],\n",
       "       [ 81.        ,   9.        ,  10.        ],\n",
       "       [ 80.        ,  10.        ,  10.        ],\n",
       "       [ 80.        ,  10.        ,  10.        ],\n",
       "       [ 80.        ,  10.        ,  10.        ],\n",
       "       [ 80.        ,  10.        ,  10.        ],\n",
       "       [ 79.        ,  11.        ,  10.        ],\n",
       "       [ 79.        ,  11.        ,  10.        ],\n",
       "       [ 78.        ,  12.        ,  10.        ],\n",
       "       [ 78.        ,  12.        ,  10.        ],\n",
       "       [ 78.        ,  12.        ,  10.        ],\n",
       "       [ 78.        ,  12.        ,  10.        ],\n",
       "       [ 78.        ,  12.        ,  10.        ],\n",
       "       [ 78.        ,  12.        ,  10.        ],\n",
       "       [ 78.        ,  12.        ,  10.        ],\n",
       "       [ 77.        ,  12.        ,  11.        ],\n",
       "       [ 77.        ,  12.        ,  11.        ],\n",
       "       [ 77.        ,  11.        ,  12.        ],\n",
       "       [ 77.        ,  11.        ,  12.        ],\n",
       "       [ 77.        ,  11.        ,  12.        ],\n",
       "       [ 76.        ,  11.        ,  13.        ],\n",
       "       [ 77.        ,  10.        ,  13.        ],\n",
       "       [ 77.        ,  10.        ,  13.        ],\n",
       "       [ 77.        ,  10.        ,  13.        ],\n",
       "       [ 78.        ,  10.        ,  12.        ],\n",
       "       [ 78.        ,  10.        ,  12.        ],\n",
       "       [ 78.        ,  10.        ,  12.        ],\n",
       "       [ 78.        ,  10.        ,  12.        ],\n",
       "       [ 78.        ,  10.        ,  12.        ],\n",
       "       [ 78.        ,  10.        ,  12.        ],\n",
       "       [ 79.        ,  10.        ,  11.        ],\n",
       "       [ 78.        ,  11.        ,  11.        ],\n",
       "       [ 79.        ,  10.        ,  11.        ],\n",
       "       [ 80.        ,  10.        ,  10.        ],\n",
       "       [ 81.        ,  10.        ,   9.        ],\n",
       "       [ 82.        ,  10.        ,   8.        ],\n",
       "       [ 81.        ,  10.        ,   9.        ],\n",
       "       [ 81.        ,  10.        ,   9.        ],\n",
       "       [ 81.        ,  10.        ,   9.        ],\n",
       "       [ 82.        ,  10.        ,   8.        ],\n",
       "       [ 82.        ,  10.        ,   8.        ],\n",
       "       [ 81.        ,  10.        ,   9.        ],\n",
       "       [ 81.        ,  10.        ,   9.        ],\n",
       "       [ 80.        ,  10.        ,  10.        ],\n",
       "       [ 79.        ,  10.        ,  11.        ],\n",
       "       [ 78.        ,  11.        ,  11.        ],\n",
       "       [ 78.        ,  11.        ,  11.        ],\n",
       "       [ 77.        ,  11.        ,  12.        ],\n",
       "       [ 77.        ,  11.        ,  12.        ],\n",
       "       [ 78.        ,  11.        ,  11.        ],\n",
       "       [ 77.        ,  12.        ,  11.        ],\n",
       "       [ 76.        ,  13.        ,  11.        ],\n",
       "       [ 75.        ,  13.        ,  12.        ],\n",
       "       [ 74.        ,  14.        ,  12.        ],\n",
       "       [ 75.        ,  14.        ,  11.        ],\n",
       "       [ 76.        ,  13.        ,  11.        ],\n",
       "       [ 75.        ,  14.        ,  11.        ],\n",
       "       [ 75.        ,  14.        ,  11.        ],\n",
       "       [ 75.        ,  14.        ,  11.        ],\n",
       "       [ 75.        ,  14.        ,  11.        ],\n",
       "       [ 75.        ,  14.        ,  11.        ],\n",
       "       [ 75.        ,  14.        ,  11.        ],\n",
       "       [ 75.        ,  14.        ,  11.        ],\n",
       "       [ 75.        ,  14.        ,  11.        ],\n",
       "       [ 75.        ,  14.        ,  11.        ],\n",
       "       [ 74.        ,  14.        ,  12.        ],\n",
       "       [ 73.        ,  15.        ,  12.        ],\n",
       "       [ 74.        ,  14.        ,  12.        ],\n",
       "       [ 74.        ,  14.        ,  12.        ],\n",
       "       [ 74.        ,  14.        ,  12.        ],\n",
       "       [ 74.        ,  14.        ,  12.        ],\n",
       "       [ 73.        ,  14.        ,  13.        ],\n",
       "       [ 74.        ,  13.        ,  13.        ],\n",
       "       [ 74.        ,  13.        ,  13.        ],\n",
       "       [ 73.        ,  14.        ,  13.        ],\n",
       "       [ 73.        ,  14.        ,  13.        ],\n",
       "       [ 73.        ,  14.        ,  13.        ],\n",
       "       [ 74.        ,  14.        ,  12.        ],\n",
       "       [ 74.        ,  14.        ,  12.        ],\n",
       "       [ 74.        ,  14.        ,  12.        ],\n",
       "       [ 75.        ,  13.        ,  12.        ],\n",
       "       [ 75.        ,  13.        ,  12.        ],\n",
       "       [ 76.        ,  13.        ,  11.        ],\n",
       "       [ 76.        ,  13.        ,  11.        ],\n",
       "       [ 76.        ,  13.        ,  11.        ],\n",
       "       [ 76.        ,  13.        ,  11.        ],\n",
       "       [ 76.        ,  13.        ,  11.        ],\n",
       "       [ 76.        ,  13.        ,  11.        ],\n",
       "       [ 76.        ,  13.        ,  11.        ],\n",
       "       [ 76.        ,  13.        ,  11.        ],\n",
       "       [ 76.        ,  13.        ,  11.        ],\n",
       "       [ 76.        ,  13.        ,  11.        ],\n",
       "       [ 76.        ,  13.        ,  11.        ],\n",
       "       [ 76.        ,  13.        ,  11.        ],\n",
       "       [ 76.        ,  13.        ,  11.        ],\n",
       "       [ 76.        ,  13.        ,  11.        ],\n",
       "       [ 76.        ,  13.        ,  11.        ],\n",
       "       [ 76.        ,  13.        ,  11.        ],\n",
       "       [ 76.        ,  13.        ,  11.        ],\n",
       "       [ 76.        ,  13.        ,  11.        ],\n",
       "       [ 76.        ,  13.        ,  11.        ],\n",
       "       [ 76.        ,  13.        ,  11.        ],\n",
       "       [ 77.        ,  12.        ,  11.        ],\n",
       "       [ 77.        ,  12.        ,  11.        ],\n",
       "       [ 77.        ,  12.        ,  11.        ],\n",
       "       [ 78.        ,  11.        ,  11.        ],\n",
       "       [ 78.        ,  10.        ,  12.        ],\n",
       "       [ 77.        ,  11.        ,  12.        ],\n",
       "       [ 77.        ,  11.        ,  12.        ],\n",
       "       [ 77.        ,  11.        ,  12.        ],\n",
       "       [ 77.        ,  11.        ,  12.        ],\n",
       "       [ 77.        ,  11.        ,  12.        ],\n",
       "       [ 77.        ,  11.        ,  12.        ],\n",
       "       [ 76.        ,  11.        ,  13.        ],\n",
       "       [ 76.        ,  11.        ,  13.        ],\n",
       "       [ 76.        ,  12.        ,  12.        ],\n",
       "       [ 76.        ,  12.        ,  12.        ],\n",
       "       [ 77.        ,  12.        ,  11.        ],\n",
       "       [ 77.        ,  12.        ,  11.        ],\n",
       "       [ 77.        ,  12.        ,  11.        ],\n",
       "       [ 78.        ,  12.        ,  10.        ],\n",
       "       [ 78.        ,  12.        ,  10.        ],\n",
       "       [ 77.        ,  12.        ,  11.        ],\n",
       "       [ 77.        ,  12.        ,  11.        ],\n",
       "       [ 77.        ,  12.        ,  11.        ],\n",
       "       [ 77.        ,  12.        ,  11.        ],\n",
       "       [ 77.        ,  12.        ,  11.        ],\n",
       "       [ 76.        ,  13.        ,  11.        ],\n",
       "       [ 75.        ,  13.        ,  12.        ],\n",
       "       [ 76.        ,  12.        ,  12.        ],\n",
       "       [ 76.        ,  12.        ,  12.        ],\n",
       "       [ 77.        ,  11.        ,  12.        ],\n",
       "       [ 77.        ,  11.        ,  12.        ],\n",
       "       [ 77.        ,  11.        ,  12.        ],\n",
       "       [ 76.        ,  12.        ,  12.        ],\n",
       "       [ 76.        ,  12.        ,  12.        ],\n",
       "       [ 77.        ,  12.        ,  11.        ],\n",
       "       [ 77.        ,  12.        ,  11.        ],\n",
       "       [ 77.        ,  12.        ,  11.        ],\n",
       "       [ 77.        ,  12.        ,  11.        ],\n",
       "       [ 77.        ,  12.        ,  11.        ],\n",
       "       [ 78.        ,  12.        ,  10.        ],\n",
       "       [ 78.        ,  12.        ,  10.        ],\n",
       "       [ 79.        ,  12.        ,   9.        ],\n",
       "       [ 80.        ,  12.        ,   8.        ],\n",
       "       [ 81.        ,  11.        ,   8.        ],\n",
       "       [ 81.        ,  11.        ,   8.        ]])"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_scores = np.array(scores)\n",
    "np_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(np_scores)), np_scores.T[0], color='r', label='win rate')\n",
    "plt.plot(np.arange(len(np_scores)), np_scores.T[1], color='g', label='tie rate')\n",
    "plt.plot(np.arange(len(np_scores)), np_scores.T[2], color='b', label='lose rate')\n",
    "plt.ylabel('Score %')\n",
    "plt.xlabel('Episode #')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-b8811b42e2df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'file' is not defined"
     ]
    }
   ],
   "source": [
    "f = file.open()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "starcraft2",
   "language": "python",
   "name": "starcraft2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
