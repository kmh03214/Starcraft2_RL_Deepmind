{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 4 - Making DRL PySC2 Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "### unfortunately, PySC2 uses Abseil, which treats python code as if its run like an app\n",
    "# This does not play well with jupyter notebook\n",
    "# So we will need to monkeypatch sys.argv\n",
    "\n",
    "\n",
    "import sys\n",
    "#sys.argv = [\"python\", \"--map\", \"AbyssalReef\"]\n",
    "sys.argv = [\"python\", \"--map\", \"Simple64\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Runnning 'Agent code' on jupyter notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Copyright 2017 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS-IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"Run an agent.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import importlib\n",
    "import threading\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from future.builtins import range  # pylint: disable=redefined-builtin\n",
    "\n",
    "from pysc2 import maps\n",
    "from pysc2.env import available_actions_printer\n",
    "from pysc2.env import run_loop\n",
    "from pysc2.env import sc2_env\n",
    "from pysc2.lib import point_flag\n",
    "from pysc2.lib import stopwatch\n",
    "from pysc2.lib import actions\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "# because of Abseil's horrible design for running code underneath Colabs\n",
    "# We have to pull out this ugly hack from the hat\n",
    "if \"flags_defined\" not in globals():\n",
    "    flags.DEFINE_bool(\"render\", False, \"Whether to render with pygame.\")\n",
    "    point_flag.DEFINE_point(\"feature_screen_size\", \"84\",\n",
    "                            \"Resolution for screen feature layers.\")\n",
    "    point_flag.DEFINE_point(\"feature_minimap_size\", \"64\",\n",
    "                            \"Resolution for minimap feature layers.\")\n",
    "    point_flag.DEFINE_point(\"rgb_screen_size\", None,\n",
    "                            \"Resolution for rendered screen.\")\n",
    "    point_flag.DEFINE_point(\"rgb_minimap_size\", None,\n",
    "                            \"Resolution for rendered minimap.\")\n",
    "    flags.DEFINE_enum(\"action_space\", \"RAW\", sc2_env.ActionSpace._member_names_,  # pylint: disable=protected-access\n",
    "                      \"Which action space to use. Needed if you take both feature \"\n",
    "                      \"and rgb observations.\")\n",
    "    flags.DEFINE_bool(\"use_feature_units\", False,\n",
    "                      \"Whether to include feature units.\")\n",
    "    flags.DEFINE_bool(\"use_raw_units\", True,\n",
    "                      \"Whether to include raw units.\")\n",
    "    flags.DEFINE_integer(\"raw_resolution\", 64, \"Raw Resolution.\")\n",
    "    flags.DEFINE_bool(\"disable_fog\", True, \"Whether to disable Fog of War.\")\n",
    "\n",
    "    flags.DEFINE_integer(\"max_agent_steps\", 0, \"Total agent steps.\")\n",
    "    flags.DEFINE_integer(\"game_steps_per_episode\", None, \"Game steps per episode.\")\n",
    "    flags.DEFINE_integer(\"max_episodes\", 0, \"Total episodes.\")\n",
    "    flags.DEFINE_integer(\"step_mul\", 8, \"Game steps per agent step.\")\n",
    "    flags.DEFINE_float(\"fps\", 22.4, \"Frames per second to run the game.\")\n",
    "\n",
    "    #flags.DEFINE_string(\"agent\", \"sc2.agent.BasicAgent.ZergBasicAgent\",\n",
    "    #                    \"Which agent to run, as a python path to an Agent class.\")\n",
    "    #flags.DEFINE_enum(\"agent_race\", \"zerg\", sc2_env.Race._member_names_,  # pylint: disable=protected-access\n",
    "    #                  \"Agent 1's race.\")\n",
    "    flags.DEFINE_string(\"agent\", \"TerranRLAgentWithRawActsAndRawObs\",\n",
    "                        \"Which agent to run, as a python path to an Agent class.\")\n",
    "    flags.DEFINE_enum(\"agent_race\", \"terran\", sc2_env.Race._member_names_,  # pylint: disable=protected-access\n",
    "                      \"Agent 1's race.\")\n",
    "\n",
    "    flags.DEFINE_string(\"agent2\", \"Bot\", \"Second agent, either Bot or agent class.\")\n",
    "    flags.DEFINE_enum(\"agent2_race\", \"random\", sc2_env.Race._member_names_,  # pylint: disable=protected-access\n",
    "                      \"Agent 2's race.\")\n",
    "    flags.DEFINE_enum(\"difficulty\", \"hard\", sc2_env.Difficulty._member_names_,  # pylint: disable=protected-access\n",
    "                      \"If agent2 is a built-in Bot, it's strength.\")\n",
    "\n",
    "    flags.DEFINE_bool(\"profile\", False, \"Whether to turn on code profiling.\")\n",
    "    flags.DEFINE_bool(\"trace\", False, \"Whether to trace the code execution.\")\n",
    "    flags.DEFINE_integer(\"parallel\", 1, \"How many instances to run in parallel.\")\n",
    "\n",
    "    flags.DEFINE_bool(\"save_replay\", True, \"Whether to save a replay at the end.\")\n",
    "\n",
    "    flags.DEFINE_string(\"map\", None, \"Name of a map to use.\")\n",
    "    flags.mark_flag_as_required(\"map\")\n",
    "\n",
    "flags_defined = True\n",
    "\n",
    "def run_thread(agent_classes, players, map_name, visualize):\n",
    "  \"\"\"Run one thread worth of the environment with agents.\"\"\"\n",
    "  with sc2_env.SC2Env(\n",
    "      map_name=map_name,\n",
    "      players=players,\n",
    "      agent_interface_format=sc2_env.parse_agent_interface_format(\n",
    "        feature_screen=FLAGS.feature_screen_size,\n",
    "        feature_minimap=FLAGS.feature_minimap_size,\n",
    "        rgb_screen=FLAGS.rgb_screen_size,\n",
    "        rgb_minimap=FLAGS.rgb_minimap_size,\n",
    "        action_space=FLAGS.action_space,\n",
    "        use_raw_units=FLAGS.use_raw_units,\n",
    "        raw_resolution=FLAGS.raw_resolution),\n",
    "      step_mul=FLAGS.step_mul,\n",
    "      game_steps_per_episode=FLAGS.game_steps_per_episode,\n",
    "      disable_fog=FLAGS.disable_fog,\n",
    "      visualize=visualize) as env:\n",
    "    #env = available_actions_printer.AvailableActionsPrinter(env)\n",
    "    agents = [agent_cls() for agent_cls in agent_classes]\n",
    "    run_loop.run_loop(agents, env, FLAGS.max_agent_steps, FLAGS.max_episodes)\n",
    "    if FLAGS.save_replay:\n",
    "      env.save_replay(agent_classes[0].__name__)\n",
    "\n",
    "def main(unused_argv):\n",
    "  \"\"\"Run an agent.\"\"\"\n",
    "  #stopwatch.sw.enabled = FLAGS.profile or FLAGS.trace\n",
    "  #stopwatch.sw.trace = FLAGS.trace\n",
    "\n",
    "  map_inst = maps.get(FLAGS.map)\n",
    "\n",
    "  agent_classes = []\n",
    "  players = []\n",
    "\n",
    "  #agent_module, agent_name = FLAGS.agent.rsplit(\".\", 1)\n",
    "  #agent_cls = getattr(importlib.import_module(agent_module), agent_name)\n",
    "  #agent_classes.append(agent_cls)\n",
    "  agent_classes.append(TerranRLAgentWithRawActsAndRawObs)\n",
    "  players.append(sc2_env.Agent(sc2_env.Race[FLAGS.agent_race]))\n",
    "\n",
    "  if map_inst.players >= 2:\n",
    "    if FLAGS.agent2 == \"Bot\":\n",
    "      players.append(sc2_env.Bot(sc2_env.Race[FLAGS.agent2_race],\n",
    "                                 sc2_env.Difficulty[FLAGS.difficulty]))\n",
    "    else:\n",
    "      #agent_module, agent_name = FLAGS.agent2.rsplit(\".\", 1)\n",
    "      #agent_cls = getattr(importlib.import_module(agent_module), agent_name)\n",
    "      agent_classes.append(TerranRandomAgent)\n",
    "      players.append(sc2_env.Agent(sc2_env.Race[FLAGS.agent2_race]))\n",
    "\n",
    "  threads = []\n",
    "  for _ in range(FLAGS.parallel - 1):\n",
    "    t = threading.Thread(target=run_thread,\n",
    "                         args=(agent_classes, players, FLAGS.map, False))\n",
    "    threads.append(t)\n",
    "    t.start()\n",
    "\n",
    "  run_thread(agent_classes, players, FLAGS.map, FLAGS.render)\n",
    "\n",
    "  for t in threads:\n",
    "    t.join()\n",
    "\n",
    "  if FLAGS.profile:\n",
    "    pass\n",
    "    #print(stopwatch.sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creating a PySC2 Agent with Raw Actions & Observations\n",
    "\n",
    "![StarCraft2 PySC2 interfaces](./images/StarCraft2_PySC2_interfaces.png)\n",
    "\n",
    "ref : https://on-demand.gputechconf.com/gtc/2018/presentation/s8739-machine-learning-with-starcraft-II.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### < PySC2 Interfaces 3가지 종류 >\n",
    "\n",
    "### 1st, Rendered\n",
    "* Decomposed :\n",
    "    - Screen, minimap, resources, available actions\n",
    "* Same control as humans :\n",
    "    - Pixel coordinates\n",
    "    - Move camera\n",
    "    - Select unit/rectangle\n",
    "* Great for Deep Learning, but hard\n",
    "\n",
    "### 2nd, Feature Layer\n",
    "* Same actions : still in pixel space\n",
    "* Same decomposed observations, but more abstract\n",
    "    - Orthogonal camera \n",
    "* Layers:\n",
    "    - unit type\n",
    "    - unit owner\n",
    "    - selection\n",
    "    - health\n",
    "    - unit density\n",
    "    - etc\n",
    "    \n",
    "### 3rd, Raw\n",
    "* List of units and state\n",
    "* Control each unit individually in world coordinates\n",
    "* Gives all observable state (no camera)\n",
    "* Great for scripted agents and programmatic replay analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### < Raw Actions & Observations 을 사용하는 이유>\n",
    "* Raw Actions & Observations 은 world cordinates를 사용하므로 전체 Map을 한번에 관찰하고 Camera를 이동하지 않고도 Map 상의 어느 곳에서도 Action을 취할 수 있는 새로운 형태의 Feature 이다.\n",
    "* 이번 과정에 SL(Supervised Learning, 지도학습)을 활용한 학습은 없지만 스타크래프트 2 리플레이를 활용한 SL은 Raw Actions & Observations를 활용한 \"programmatic replay analysis\"가 필요하다.\n",
    "* 인간 플레이어를 이긴 DeepMind의 AlphaStar의 주요 변경사항 중의 하나는 Raw Actions & Observations 의 활용이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DRL 모델의 성능 추이를 보기위해 Reward의 평균 추이를 이용한다. 이때 단순이동평균 보다는 지수이동평균이 적절하다.\n",
    "\n",
    "### 지수이동평균(EMA:Exponential Moving Average) 란?\n",
    "지수이동평균(Exponential Moving Average)은 과거의 모든 기간을 계산대상으로 하며 최근의 데이타에 더 높은 가중치를 두는 일종의 가중이동평균법이다.\n",
    "\n",
    "단순이동평균의 계산법에 비하여 원리가 복잡해 보이지만 실제로 이동평균을 산출하는 방법은 Previous Step의 지수이동평균값과 평활계수(smoothing constant) 그리고 당일의 가격만으로 구할 수 있으므로 Previous Step의 지수이동평균값만 구해진다면 오히려 간단한 편이다.\n",
    "\n",
    "따라서 지수이동평균은 단순이동평균에 비해 몇가지 중요한 강점을 가진다.\n",
    "\n",
    "첫째는 가장 최근의 Step에 가장 큰 가중치를 둠으로 해서 최근의 Episode들을 잘 반영한다는 점이고, 둘째는 단순이동평균에서와 같이 오래된 데이타를 갑자기 제외하지 않고 천천히 그 영향력을 사라지게 한다는 점이다.\n",
    "또한 전 기간의 데이타를 분석대상으로 함으로써 가중이동평균에서 문제되는 특정 기간의 데이타만을 분석대상으로 한다는 단점도 보완하고 있다.\n",
    "\n",
    "### 지수이동평균(EMA:Exponential Moving Average) 계산\n",
    "\n",
    "지수이동평균은 가장 최근의 값에 많은 가중치를 부여하고 오래 된 값에는 적은 가중치를 부여한다. 비록 오래 된 값이라고 할지라도 완전히 무시하지는 않고 적게나마 반영시켜 계산한다는 장점이 있다. 단기 변동성을 포착하려는 것이 목적이다.\n",
    "\n",
    "EMA=Previous Step 지수이동평균+(k∗(Current Step Reward − Previous Step 지수이동평균))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Applying Vanilla DQN to a PySC2 Agent\n",
    "\n",
    "구현된 기능\n",
    "\n",
    "- Implementing 'Experience Replay' : \n",
    "    - 'Maximization Bias' 문제를 발생시키는 원인 중 하나인 'Sample간의 시간적 연관성'을 해결하기 위한 방법\n",
    "    - Online Learning 에서 Batch Learning 으로 학습방법 바뀜 : Online update 는 Batch update 보다 일반적으로 Validation loss 가 더 높게 나타남.\n",
    "    - Reinforcement Learning for Robots. Using Neural Networks. Long -Ji Lin. January 6, 1993. 논문에서 최초로 연구됨 http://isl.anthropomatik.kit.edu/pdf/Lin1993.pdf\n",
    "\n",
    "- Implementing 'Fixed Q-Target' : \n",
    "    - 'Moving Q-Target' 문제 해결하기 위한 방법\n",
    "    - 2015년 Nature 버전 DQN 논문에서 처음 제안됨. https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning \n",
    "\n",
    "\n",
    "구현되지 않은 기능\n",
    "\n",
    "- Implementing 'Sensory Input Feature-Extraction' :\n",
    "    - 게임의 Raw Image 를 Neural Net에 넣기 위한 Preprocessing(전처리) 과정\n",
    "    - Raw Image 의 Sequence중 '최근 4개의 이미지'(과거 정보)를 하나의 새로운 State로 정의하여 non-MDP를 MDP 문제로 바꾸는 Preprocessing 과정 \n",
    "    - CNN(합성곱 신경망)을 활용한 '차원의 저주' 극복"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import math\n",
    "import os.path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import deque\n",
    "import pickle\n",
    "\n",
    "from pysc2.agents import base_agent\n",
    "from pysc2.env import sc2_env\n",
    "from pysc2.lib import actions, features, units, upgrades\n",
    "from absl import app\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from skdrl.pytorch.model.mlp import NaiveMultiLayerPerceptron\n",
    "from skdrl.common.memory.memory import ExperienceReplayMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE_QNET = 'rlagent_with_vanilla_dqn_qnet'\n",
    "DATA_FILE_QNET_TARGET = 'rlagent_with_vanilla_dqn_qnet_target'\n",
    "SCORE_FILE = 'rlagent_with_vanilla_dqn_score'\n",
    "\n",
    "scores = []                        # list containing scores from each episode\n",
    "scores_window = deque(maxlen=100)  # last 100 scores\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-update 공식\n",
    "\n",
    "#### 1. Online Q-learning\n",
    "![Online Q-learning](./images/q-update-experience-replay.png)\n",
    "\n",
    "#### 2. Online Q-learning with Function Approximation\n",
    "![Online Q-learning with Function Approximation](./images/q-update-function-approximation.png)\n",
    "\n",
    "#### 3. Batch Q-learning with Function Approximation & Experience Replay\n",
    "![Batch Q-learning with Function Approximation & Experience Replay](./images/q-update-online.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving target problem\n",
    "\n",
    "#### 1. Function Approximation을 사용하지 않는 Q-learning 의 경우 : 특정한 Q(s,a) update가 다른 Q(s,a)에 영향을 주지 않는다.\n",
    "![Moving target Q-learning](./images/moving-target_q-learing_case.png)\n",
    "\n",
    "#### 2. Function Approximation을 사용하는 Q-learnig 의 경우 : 특정한 Q(s,a) update가 다른 Q(s,a)에 영향을 준다.\n",
    "![Moving target Q-learning with Function Approximation](./images/moving-target_q-learing_with_function_approximation_case.png)\n",
    "\n",
    "### Moving target 문제는 Deep Neural Network를 사용하는 Function Approximation 기법인 경우 심해지는 경향성이 있음.\n",
    "\n",
    "image ref : Fast Campus RL online courese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `nn.SmoothL1Loss()` = Huber loss 란?\n",
    "\n",
    "Mean-squared Error (MSE) Loss 는 데이터의 outlier에 매우 취약하다.\n",
    "어떤 이유로 타겟하는 레이블 y (이 경우는 q-learning target)이 noisy 할때를 가정하면, 잘못된 y 값을 맞추기 위해 파라미터들이 너무 sensitive 하게 움직이게 된다.\n",
    "\n",
    "이런 현상은 q-learning 의 학습초기에 매우 빈번해 나타난다. 이러한 문제를 조금이라도 완화하기 위해서 outlier에 덜 민감한 Huber loss 함수를 사용한다.\n",
    "\n",
    "### SmoothL1Loss (aka Huber loss)\n",
    "\n",
    "$$loss(x,y) = \\frac{1}{n}\\sum_i z_i$$\n",
    "$|x_i - y_i| <1$ 일때,\n",
    "$$z_i = 0.5(x_i - y_i)^2$$\n",
    "$|x_i - y_i| \\geq1$ 일때,\n",
    "$$z_i = |x_i - y_i|-0.5$$\n",
    "\n",
    "ref : https://pytorch.org/docs/master/generated/torch.nn.SmoothL1Loss.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 state_dim: int,\n",
    "                 action_dim: int,\n",
    "                 qnet: nn.Module,\n",
    "                 qnet_target: nn.Module,\n",
    "                 lr: float,\n",
    "                 gamma: float,\n",
    "                 epsilon: float):\n",
    "        \"\"\"\n",
    "        :param state_dim: input state dimension\n",
    "        :param action_dim: action dimension\n",
    "        :param qnet: main q network\n",
    "        :param qnet_target: target q network\n",
    "        :param lr: learning rate\n",
    "        :param gamma: discount factor of MDP\n",
    "        :param epsilon: E-greedy factor\n",
    "        \"\"\"\n",
    "\n",
    "        super(DQN, self).__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.qnet = qnet\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.opt = torch.optim.Adam(params=self.qnet.parameters(), lr=lr)\n",
    "        self.register_buffer('epsilon', torch.ones(1) * epsilon)\n",
    "\n",
    "        # target network related\n",
    "        qnet_target.load_state_dict(qnet.state_dict())\n",
    "        self.qnet_target = qnet_target\n",
    "        self.criteria = nn.SmoothL1Loss()\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        qs = self.qnet(state)\n",
    "        #prob = np.random.uniform(0.0, 1.0, 1)\n",
    "        #if torch.from_numpy(prob).float() <= self.epsilon:  # random\n",
    "        if random.random() <= self.epsilon: # random\n",
    "            action = np.random.choice(range(self.action_dim))\n",
    "        else:  # greedy\n",
    "            action = qs.argmax(dim=-1)\n",
    "        return int(action)\n",
    "\n",
    "    def learn(self, state, action, reward, next_state, done):\n",
    "        s, a, r, ns = state, action, reward, next_state\n",
    "#         print(\"state: \", s)\n",
    "#         print(\"action: \", a)\n",
    "#         print(\"reward: \", reward)\n",
    "#         print(\"next_state: \", ns)\n",
    "        \n",
    "\n",
    "        # compute Q-Learning target with 'target network'\n",
    "        with torch.no_grad():\n",
    "            q_max, _ = self.qnet_target(ns).max(dim=-1, keepdims=True)\n",
    "            q_target = r + self.gamma * q_max * (1 - done)\n",
    "\n",
    "        q_val = self.qnet(s).gather(1, a)\n",
    "        loss = self.criteria(q_val, q_target)\n",
    "\n",
    "        self.opt.zero_grad()\n",
    "        loss.backward()\n",
    "        self.opt.step()\n",
    "\n",
    "\n",
    "def prepare_training_inputs(sampled_exps, device='cpu'):\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    next_states = []\n",
    "    dones = []\n",
    "    for sampled_exp in sampled_exps:\n",
    "        states.append(sampled_exp[0])\n",
    "        actions.append(sampled_exp[1])\n",
    "        rewards.append(sampled_exp[2])\n",
    "        next_states.append(sampled_exp[3])\n",
    "        dones.append(sampled_exp[4])\n",
    "\n",
    "    states = torch.cat(states, dim=0).float().to(device)\n",
    "    actions = torch.cat(actions, dim=0).to(device)\n",
    "    rewards = torch.cat(rewards, dim=0).float().to(device)\n",
    "    next_states = torch.cat(next_states, dim=0).float().to(device)\n",
    "    dones = torch.cat(dones, dim=0).float().to(device)\n",
    "    return states, actions, rewards, next_states, dones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Action 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TerranAgentWithRawActsAndRawObs(base_agent.BaseAgent):\n",
    "    # actions 추가 및 함수 정의(hirerachy하게)\n",
    "    \n",
    "    actions = (\"do_nothing\",\n",
    "               \"train_scv\",\n",
    "               \"harvest_minerals\",\n",
    "               \"harvest_gas\",\n",
    "               \"build_commandcenter\",\n",
    "               \n",
    "               \"build_refinery\",\n",
    "               \"build_supply_depot\",\n",
    "               \"build_barracks\",\n",
    "               \"train_marine\",\n",
    "               \n",
    "               \"build_factorys\",\n",
    "               \"build_techlab_factorys\",\n",
    "               \"train_tank\",\n",
    "               \n",
    "               \"build_armorys\",\n",
    "               \n",
    "               \"build_starports\",\n",
    "               \"build_techlab_starports\",\n",
    "               \"train_banshee\",\n",
    "               \n",
    "               \"attack\",\n",
    "               \"attack_all\",\n",
    "               \n",
    "               \"tank_control\"\n",
    "              )\n",
    "\n",
    "\n",
    "    \n",
    "    def unit_type_is_selected(self, obs, unit_type):\n",
    "        if (len(obs.observation.single_select) > 0 and\n",
    "            obs.observation.single_select[0].unit_type == unit_type):\n",
    "              return True\n",
    "\n",
    "        if (len(obs.observation.multi_select) > 0 and\n",
    "            obs.observation.multi_select[0].unit_type == unit_type):\n",
    "              return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def get_my_units_by_type(self, obs, unit_type):\n",
    "        if unit_type == units.Neutral.VespeneGeyser: # 가스 일 때만\n",
    "            return [unit for unit in obs.observation.raw_units\n",
    "                if unit.unit_type == unit_type]\n",
    "        \n",
    "        return [unit for unit in obs.observation.raw_units\n",
    "                if unit.unit_type == unit_type\n",
    "                and unit.alliance == features.PlayerRelative.SELF]\n",
    "\n",
    "    def get_enemy_units_by_type(self, obs, unit_type):\n",
    "        return [unit for unit in obs.observation.raw_units\n",
    "                if unit.unit_type == unit_type\n",
    "                and unit.alliance == features.PlayerRelative.ENEMY]\n",
    "\n",
    "    def get_my_completed_units_by_type(self, obs, unit_type):\n",
    "        return [unit for unit in obs.observation.raw_units\n",
    "                if unit.unit_type == unit_type\n",
    "                and unit.build_progress == 100\n",
    "                and unit.alliance == features.PlayerRelative.SELF]\n",
    "\n",
    "    def get_enemy_completed_units_by_type(self, obs, unit_type):\n",
    "        return [unit for unit in obs.observation.raw_units\n",
    "                if unit.unit_type == unit_type\n",
    "                and unit.build_progress == 100\n",
    "                and unit.alliance == features.PlayerRelative.ENEMY]\n",
    "\n",
    "    def get_distances(self, obs, units, xy):\n",
    "        units_xy = [(unit.x, unit.y) for unit in units]\n",
    "        return np.linalg.norm(np.array(units_xy) - np.array(xy), axis=1)\n",
    "\n",
    "    def step(self, obs):\n",
    "        super(TerranAgentWithRawActsAndRawObs, self).step(obs)\n",
    "        if obs.first():\n",
    "            command_center = self.get_my_units_by_type(\n",
    "                obs, units.Terran.CommandCenter)[0]\n",
    "            self.base_top_left = (command_center.x < 32)\n",
    "            self.top_left_gas_xy = [(14, 25), (21,19), (46,23), (39,16)]\n",
    "            self.bottom_right_gas_xy = [(44, 43), (37,50), (12,46), (19,53)]\n",
    "            \n",
    "            \n",
    "            self.cloaking_flag = 1\n",
    "            \n",
    "            self.TerranVehicleWeaponsLevel1 = False\n",
    "            self.TerranVehicleWeaponsLevel2 = False\n",
    "            self.TerranVehicleWeaponsLevel3 = False\n",
    "            \n",
    "\n",
    "    def do_nothing(self, obs):\n",
    "        return actions.RAW_FUNCTIONS.no_op()\n",
    "    \n",
    "    def train_scv(self, obs):\n",
    "        completed_commandcenterses = self.get_my_completed_units_by_type(\n",
    "            obs, units.Terran.CommandCenter)\n",
    "        \n",
    "        scvs = self.get_my_units_by_type(obs, units.Terran.SCV)\n",
    "        \n",
    "        if (len(completed_commandcenterses) > 0 and obs.observation.player.minerals >= 100\n",
    "                and len(scvs) < 35):\n",
    "            commandcenters = self.get_my_units_by_type(obs, units.Terran.CommandCenter)\n",
    "            \n",
    "            ccs =[commandcenter for commandcenter in commandcenters if commandcenter.assigned_harvesters < 18]\n",
    "            \n",
    "            if ccs:\n",
    "                ccs = ccs[0]\n",
    "                if ccs.order_length < 5:\n",
    "                    return actions.RAW_FUNCTIONS.Train_SCV_quick(\"now\", ccs.tag)\n",
    "        return actions.RAW_FUNCTIONS.no_op()\n",
    "\n",
    "    def harvest_minerals(self, obs):\n",
    "        scvs = self.get_my_units_by_type(obs, units.Terran.SCV)\n",
    "        commandcenters = self.get_my_units_by_type(obs,units.Terran.CommandCenter) # 최적 자원 할당 유닛 구현\n",
    "        \n",
    "        cc = [commandcenter for commandcenter in commandcenters if commandcenter.assigned_harvesters < 18]\n",
    "        \n",
    "        if cc:\n",
    "            cc = cc[0]\n",
    "\n",
    "            idle_scvs = [scv for scv in scvs if scv.order_length == 0]\n",
    "\n",
    "            if len(idle_scvs) > 0 and cc.assigned_harvesters < 18:\n",
    "\n",
    "                mineral_patches = [unit for unit in obs.observation.raw_units\n",
    "                                   if unit.unit_type in [\n",
    "                                       units.Neutral.BattleStationMineralField,\n",
    "                                       units.Neutral.BattleStationMineralField750,\n",
    "                                       units.Neutral.LabMineralField,\n",
    "                                       units.Neutral.LabMineralField750,\n",
    "                                       units.Neutral.MineralField,\n",
    "                                       units.Neutral.MineralField750,\n",
    "                                       units.Neutral.PurifierMineralField,\n",
    "                                       units.Neutral.PurifierMineralField750,\n",
    "                                       units.Neutral.PurifierRichMineralField,\n",
    "                                       units.Neutral.PurifierRichMineralField750,\n",
    "                                       units.Neutral.RichMineralField,\n",
    "                                       units.Neutral.RichMineralField750\n",
    "                                   ]]\n",
    "                scv = random.choice(idle_scvs)\n",
    "                distances = self.get_distances(obs, mineral_patches, (scv.x, scv.y))\n",
    "                mineral_patch = mineral_patches[np.argmin(distances)]\n",
    "                return actions.RAW_FUNCTIONS.Harvest_Gather_unit(\n",
    "                    \"now\", scv.tag, mineral_patch.tag)\n",
    "        return actions.RAW_FUNCTIONS.no_op()\n",
    "    \n",
    "    def harvest_gas(self, obs):\n",
    "        scvs = self.get_my_units_by_type(obs, units.Terran.SCV)\n",
    "        refs = self.get_my_units_by_type(obs, units.Terran.Refinery)\n",
    "        \n",
    "        refs = [refinery for refinery in refs if refinery.assigned_harvesters < 3]\n",
    "        \n",
    "        if refs:\n",
    "            ref = refs[0]\n",
    "            if len(scvs) > 0 and ref.ideal_harvesters:\n",
    "                scv = random.choice(scvs)\n",
    "                distances = self.get_distances(obs, refs, (scv.x, scv.y))\n",
    "                ref = refs[np.argmin(distances)]\n",
    "\n",
    "                return actions.RAW_FUNCTIONS.Harvest_Gather_unit(\n",
    "                    \"now\", scv.tag, ref.tag)\n",
    "        \n",
    "        return actions.RAW_FUNCTIONS.no_op()\n",
    "    \n",
    "    def build_commandcenter(self,obs):\n",
    "        commandcenters = self.get_my_units_by_type(obs,units.Terran.CommandCenter)\n",
    "        scvs = self.get_my_units_by_type(obs, units.Terran.SCV)\n",
    "        \n",
    "        if ( len(commandcenters) < 2 and obs.observation.player.minerals >= 400 and\n",
    "                len(scvs) > 0):\n",
    "            ccs_xy = (41, 21) if self.base_top_left else (17, 48)\n",
    "            \n",
    "            distances = self.get_distances(obs, scvs, ccs_xy)\n",
    "            scv = scvs[np.argmin(distances)]\n",
    "\n",
    "            return actions.RAW_FUNCTIONS.Build_CommandCenter_pt(\n",
    "                \"now\", scv.tag, ccs_xy)\n",
    "        return actions.RAW_FUNCTIONS.no_op()\n",
    "    \n",
    "    ################################################################################################\n",
    "    ####################################### refinery ###############################################\n",
    "    \n",
    "    def build_refinery(self,obs):\n",
    "        refinerys = self.get_my_units_by_type(obs,units.Terran.Refinery)\n",
    "        scvs = self.get_my_units_by_type(obs, units.Terran.SCV)\n",
    "        \n",
    "        if (obs.observation.player.minerals >= 100 and\n",
    "                len(scvs) > 0):\n",
    "            gas = self.get_my_units_by_type(obs, units.Neutral.VespeneGeyser)[0]\n",
    "            \n",
    "            if self.base_top_left:\n",
    "                gases = self.top_left_gas_xy\n",
    "            else:\n",
    "                gases = self.bottom_right_gas_xy\n",
    "            \n",
    "            rc = np.random.choice([0,1,2,3])\n",
    "            gas_xy = gases[rc]\n",
    "            if (gas.x, gas.y) == gas_xy:\n",
    "                distances = self.get_distances(obs, scvs, gas_xy)\n",
    "                scv = scvs[np.argmin(distances)]\n",
    "\n",
    "                return actions.RAW_FUNCTIONS.Build_Refinery_pt(\n",
    "                    \"now\", scv.tag, gas.tag)\n",
    "        return actions.RAW_FUNCTIONS.no_op()\n",
    "\n",
    "    def build_supply_depot(self, obs):\n",
    "        supply_depots = self.get_my_units_by_type(obs, units.Terran.SupplyDepot)\n",
    "        scvs = self.get_my_units_by_type(obs, units.Terran.SCV)\n",
    "        \n",
    "        free_supply = (obs.observation.player.food_cap -\n",
    "                       obs.observation.player.food_used)\n",
    "        \n",
    "        if (obs.observation.player.minerals >= 100 and\n",
    "            len(scvs) > 0 and free_supply < 8):\n",
    "            \n",
    "            ccs = self.get_my_units_by_type(obs, units.Terran.CommandCenter)\n",
    "            if ccs:\n",
    "                for cc in ccs:\n",
    "                    cc_x, cc_y = cc.x, cc.y\n",
    "                \n",
    "                rand1,rand2 = random.randint(-5,10),random.randint(-5,10)\n",
    "                supply_depot_xy = (cc_x + rand1, cc_y + rand2) if self.base_top_left else (cc_x - rand1, cc_y - rand2)\n",
    "                if 0 < supply_depot_xy[0] < 64 and 0 < supply_depot_xy[1] < 64:\n",
    "                    pass\n",
    "                else:\n",
    "                    return actions.RAW_FUNCTIONS.no_op()\n",
    "                    \n",
    "                \n",
    "                distances = self.get_distances(obs, scvs, supply_depot_xy)\n",
    "                scv = scvs[np.argmin(distances)]\n",
    "                \n",
    "                return actions.RAW_FUNCTIONS.Build_SupplyDepot_pt(\n",
    "                    \"now\", scv.tag, supply_depot_xy)\n",
    "                \n",
    "        return actions.RAW_FUNCTIONS.no_op()\n",
    "\n",
    "    def build_barracks(self, obs):\n",
    "        completed_supply_depots = self.get_my_completed_units_by_type(\n",
    "            obs, units.Terran.SupplyDepot)\n",
    "        barrackses = self.get_my_units_by_type(obs, units.Terran.Barracks)\n",
    "        scvs = self.get_my_units_by_type(obs, units.Terran.SCV)\n",
    "        \n",
    "        if (len(completed_supply_depots) > 0 and\n",
    "            obs.observation.player.minerals >= 150 and len(scvs) > 0 and\n",
    "            len(barrackses)< 3):\n",
    "            \n",
    "            brks = self.get_my_units_by_type(obs, units.Terran.SupplyDepot)\n",
    "            \n",
    "            completed_command_center = self.get_my_completed_units_by_type(\n",
    "            obs, units.Terran.CommandCenter)\n",
    "            \n",
    "            if len(barrackses) >= 1 and len(completed_command_center) == 1:\n",
    "                # double commands\n",
    "                    \n",
    "                commandcenters = self.get_my_units_by_type(obs,units.Terran.CommandCenter)\n",
    "                scvs = self.get_my_units_by_type(obs, units.Terran.SCV)\n",
    "\n",
    "                if ( len(commandcenters) < 2 and obs.observation.player.minerals >= 400 and\n",
    "                        len(scvs) > 0):\n",
    "                    ccs_xy = (41, 21) if self.base_top_left else (17, 48)\n",
    "\n",
    "                    distances = self.get_distances(obs, scvs, ccs_xy)\n",
    "                    scv = scvs[np.argmin(distances)]\n",
    "\n",
    "                    return actions.RAW_FUNCTIONS.Build_CommandCenter_pt(\n",
    "                        \"now\", scv.tag, ccs_xy)\n",
    "            \n",
    "            if brks:\n",
    "                for brk in brks:\n",
    "                    brk_x,brk_y = brk.x, brk.y\n",
    "                \n",
    "\n",
    "                rand1, rand2 = random.randint(1,3),random.randint(1,3)\n",
    "                barracks_xy = (brk_x + rand1, brk_y + rand2) if self.base_top_left else (brk_x - rand1, brk_y - rand2)\n",
    "                if 0 < barracks_xy[0] < 64 and 0 < barracks_xy[1] < 64:\n",
    "                    pass\n",
    "                else:\n",
    "                    return actions.RAW_FUNCTIONS.no_op()\n",
    "                    \n",
    "\n",
    "                distances = self.get_distances(obs, scvs, barracks_xy)\n",
    "                scv = scvs[np.argmin(distances)]\n",
    "                return actions.RAW_FUNCTIONS.Build_Barracks_pt(\n",
    "                    \"now\", scv.tag, barracks_xy)\n",
    "        return actions.RAW_FUNCTIONS.no_op()\n",
    "\n",
    "    def train_marine(self, obs):\n",
    "        completed_barrackses = self.get_my_completed_units_by_type(\n",
    "            obs, units.Terran.Barracks)\n",
    "        free_supply = (obs.observation.player.food_cap -\n",
    "                       obs.observation.player.food_used)\n",
    "        if (len(completed_barrackses) > 0 and obs.observation.player.minerals >= 100\n",
    "                and free_supply > 0):\n",
    "            barracks = self.get_my_units_by_type(obs, units.Terran.Barracks)[0]\n",
    "            if barracks.order_length < 5:\n",
    "                return actions.RAW_FUNCTIONS.Train_Marine_quick(\"now\", barracks.tag)\n",
    "        return actions.RAW_FUNCTIONS.no_op()\n",
    "    \n",
    "    ###############################################################################################\n",
    "    ###################################### Factorys ###############################################\n",
    "    ###############################################################################################\n",
    "    \n",
    "    def build_factorys(self, obs):\n",
    "        completed_barrackses = self.get_my_completed_units_by_type(\n",
    "            obs, units.Terran.Barracks)\n",
    "        \n",
    "        factorys = self.get_my_units_by_type(obs, units.Terran.Factory)\n",
    "        scvs = self.get_my_units_by_type(obs, units.Terran.SCV)\n",
    "        ref = self.get_my_completed_units_by_type(obs,units.Terran.Refinery)\n",
    "        # print(\"gas: \", obs.observation.player.minerals)\n",
    "        # print(\"gas: \", obs.observation.player.gas)\n",
    "        if (len(completed_barrackses) > 0 and\n",
    "            obs.observation.player.minerals >= 200 and\n",
    "            len(factorys) < 3 and\n",
    "            len(scvs) > 0):\n",
    "            \n",
    "            if len(factorys) >= 1 and len(ref) < 4: # 가스부족시 가스 건설\n",
    "                refinerys = self.get_my_units_by_type(obs,units.Terran.Refinery)\n",
    "                scvs = self.get_my_units_by_type(obs, units.Terran.SCV)\n",
    "\n",
    "                if (obs.observation.player.minerals >= 100 and\n",
    "                        len(scvs) > 0):\n",
    "                    gas = self.get_my_units_by_type(obs, units.Neutral.VespeneGeyser)[0]\n",
    "\n",
    "                    if self.base_top_left:\n",
    "                        gases = self.top_left_gas_xy\n",
    "                    else:\n",
    "                        gases = self.bottom_right_gas_xy\n",
    "\n",
    "                    rc = np.random.choice([0,1,2,3])\n",
    "                    gas_xy = gases[rc]\n",
    "                    if (gas.x, gas.y) == gas_xy:\n",
    "                        distances = self.get_distances(obs, scvs, gas_xy)\n",
    "                        scv = scvs[np.argmin(distances)]\n",
    "\n",
    "                        return actions.RAW_FUNCTIONS.Build_Refinery_pt(\n",
    "                            \"now\", scv.tag, gas.tag)\n",
    "            \n",
    "            if len(factorys) >= 1:\n",
    "                rand1 = random.randint(-5,5)\n",
    "                fx, fy = factorys[0].x, factorys[0].y\n",
    "                factorys_xy = (fx + rand1, fy + rand1) if self.base_top_left else (fx - rand1, fy - rand1)\n",
    "                \n",
    "            else:\n",
    "                rand1, rand2 = random.randint(-2,2), random.randint(-2,2) # x, y\n",
    "                factorys_xy = (39 + rand1, 25 + rand2) if self.base_top_left else (17 - rand1, 40 - rand2)\n",
    "\n",
    "                \n",
    "            if 0 < factorys_xy[0] < 64 and 0 < factorys_xy[1] < 64 and factorys_xy != (17,48) and factorys_xy != (41,21):\n",
    "                pass\n",
    "            else:\n",
    "                return actions.RAW_FUNCTIONS.no_op()\n",
    "\n",
    "\n",
    "            distances = self.get_distances(obs, scvs, factorys_xy)\n",
    "            scv = scvs[np.argmin(distances)]\n",
    "            return actions.RAW_FUNCTIONS.Build_Factory_pt(\n",
    "                \"now\", scv.tag, factorys_xy)\n",
    "        return actions.RAW_FUNCTIONS.no_op()\n",
    "    \n",
    "    def build_techlab_factorys(self, obs):\n",
    "        completed_factorys = self.get_my_completed_units_by_type(\n",
    "            obs, units.Terran.Factory)\n",
    "        \n",
    "        scvs = self.get_my_units_by_type(obs, units.Terran.SCV)\n",
    "        \n",
    "        if (len(completed_factorys) > 0 and \n",
    "            obs.observation.player.minerals >= 200):\n",
    "            \n",
    "            ftrs = self.get_my_units_by_type(obs, units.Terran.Factory)\n",
    "            \n",
    "            if ftrs:\n",
    "                for ftr in ftrs:\n",
    "                    ftr_x,ftr_y = ftr.x, ftr.y\n",
    "            \n",
    "                factorys_xy = (ftr_x,ftr_y)\n",
    "                if 0 < factorys_xy[0] < 64 and 0 < factorys_xy[1] < 64:\n",
    "                    pass\n",
    "                else:\n",
    "                    return actions.RAW_FUNCTIONS.no_op()\n",
    "\n",
    "                return actions.RAW_FUNCTIONS.Build_TechLab_Factory_pt(\n",
    "                    \"now\", ftr.tag, factorys_xy)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        return actions.RAW_FUNCTIONS.no_op()\n",
    "    \n",
    "    def train_tank(self, obs):\n",
    "        completed_factorytechlab = self.get_my_completed_units_by_type(\n",
    "            obs, units.Terran.FactoryTechLab)\n",
    "        \n",
    "        free_supply = (obs.observation.player.food_cap -\n",
    "                       obs.observation.player.food_used)\n",
    "        \n",
    "        if (len(completed_factorytechlab) > 0 and obs.observation.player.minerals >= 200\n",
    "                and free_supply > 3):\n",
    "            \n",
    "            factorys = self.get_my_units_by_type(obs, units.Terran.Factory)[0]\n",
    "            \n",
    "            if factorys.order_length < 5:\n",
    "                return actions.RAW_FUNCTIONS.Train_SiegeTank_quick(\"now\", factorys.tag)\n",
    "        return actions.RAW_FUNCTIONS.no_op()\n",
    "    \n",
    "    ###############################################################################\n",
    "    ############################ Build Armory ##################################\n",
    "    \n",
    "    def build_armorys(self, obs):\n",
    "        completed_factory = self.get_my_completed_units_by_type(\n",
    "            obs, units.Terran.Factory)\n",
    "        \n",
    "        armorys = self.get_my_units_by_type(obs, units.Terran.Armory)\n",
    "        \n",
    "        scvs = self.get_my_units_by_type(obs, units.Terran.SCV)\n",
    "        \n",
    "        if (len(completed_factory) > 0 and\n",
    "            obs.observation.player.minerals >= 200 and\n",
    "            len(armorys) < 2 and\n",
    "            len(scvs) > 0):\n",
    "            \n",
    "            rand1, rand2 = random.randint(-2,2),random.randint(-2,2)\n",
    "            armorys_xy = (36 + rand1, 20 + rand2) if self.base_top_left else ( 20 - rand1, 50 - rand2)\n",
    "            if 0 < armorys_xy[0] < 64 and 0 < armorys_xy[1] < 64:\n",
    "                pass\n",
    "            else:\n",
    "                return actions.RAW_FUNCTIONS.no_op()\n",
    "\n",
    "\n",
    "            distances = self.get_distances(obs, scvs, armorys_xy)\n",
    "            scv = scvs[np.argmin(distances)]\n",
    "            return actions.RAW_FUNCTIONS.Build_Armory_pt(\n",
    "                \"now\", scv.tag, armorys_xy)\n",
    "        \n",
    "        elif (len(completed_factory) > 0 and\n",
    "            obs.observation.player.minerals >= 200 and\n",
    "            2 <= len(armorys) and\n",
    "            len(scvs) > 0):\n",
    "            # armory upgrade 추가\n",
    "            armory = armorys[0]\n",
    "            \n",
    "            armory_xy = (armory.x, armory.y)\n",
    "            #cloak_field = self.get_my_units_by_type(obs, upgrades.Upgrades.CloakingField)[0]\n",
    "            if self.TerranVehicleWeaponsLevel1 == False:\n",
    "                self.TerranVehicleWeaponsLevel1 = True\n",
    "                return actions.RAW_FUNCTIONS.Research_TerranVehicleWeaponsLevel1_quick(\"now\", armory_xy )\n",
    "            \n",
    "            elif self.TerranVehicleWeaponsLevel1 == True and self.TerranVehicleWeaponsLevel2 == False:\n",
    "                self.TerranVehicleWeaponsLevel2 = True\n",
    "                return actions.RAW_FUNCTIONS.Research_TerranVehicleWeaponsLevel2_quick(\"now\", armory_xy )\n",
    "            \n",
    "            elif self.TerranVehicleWeaponsLevel1 == True and self.TerranVehicleWeaponsLevel2 == True and self.TerranVehicleWeaponsLevel3 == False:\n",
    "                self.TerranVehicleWeaponsLevel3 = True\n",
    "                return actions.RAW_FUNCTIONS.Research_TerranVehicleWeaponsLevel3_quick(\"now\", armory_xy )\n",
    "            \n",
    "            \n",
    "            \n",
    "        return actions.RAW_FUNCTIONS.no_op()\n",
    "    \n",
    "    \n",
    "    ############################################################################################\n",
    "    #################################### StarPort ##############################################\n",
    "    def build_starports(self, obs):\n",
    "        completed_factorys = self.get_my_completed_units_by_type(\n",
    "            obs, units.Terran.Factory)\n",
    "        \n",
    "        starports = self.get_my_units_by_type(obs, units.Terran.Starport)\n",
    "        scvs = self.get_my_units_by_type(obs, units.Terran.SCV)\n",
    "        \n",
    "        if (len(completed_factorys) > 0 and\n",
    "            obs.observation.player.minerals >= 200 and \n",
    "            len(starports) < 2 and\n",
    "            len(scvs) > 0):\n",
    "            \n",
    "            # stp_x,stp_y = (22,22), (36,46) # minerals기준 중앙부쪽 좌표\n",
    "            \n",
    "            if len(starports) >= 1:\n",
    "                rand1 = random.randint(-5,5)\n",
    "                sx, sy = starports[0].x, starports[0].y\n",
    "                factorys_xy = (sx + rand1, sy + rand1) if self.base_top_left else (sx - rand1, sy - rand1)\n",
    "            else:\n",
    "                rand1, rand2 = random.randint(-5,5),random.randint(-5,5)\n",
    "                starport_xy = (22 + rand1, 22 + rand2) if self.base_top_left else (36 - rand1, 46 - rand2)\n",
    "\n",
    "            \n",
    "            if 0 < starport_xy[0] < 64 and 0 < starport_xy[1] < 64:\n",
    "                pass\n",
    "            else:\n",
    "                return actions.RAW_FUNCTIONS.no_op()\n",
    "\n",
    "            distances = self.get_distances(obs, scvs, starport_xy)\n",
    "            scv = scvs[np.argmin(distances)]\n",
    "            return actions.RAW_FUNCTIONS.Build_Starport_pt(\n",
    "                \"now\", scv.tag, starport_xy)\n",
    "        return actions.RAW_FUNCTIONS.no_op()\n",
    "    \n",
    "    def build_techlab_starports(self, obs):\n",
    "        completed_starports = self.get_my_completed_units_by_type(\n",
    "            obs, units.Terran.Starport)\n",
    "        \n",
    "        completed_starport_techlab = self.get_my_completed_units_by_type(\n",
    "            obs, units.Terran.StarportTechLab)\n",
    "                \n",
    "        if (len(completed_starports) < 3 and \n",
    "            obs.observation.player.minerals >= 200):\n",
    "            stps = self.get_my_units_by_type(obs, units.Terran.Starport)\n",
    "            \n",
    "            if stps:\n",
    "                for stp in stps:\n",
    "                    stp_x,stp_y = stp.x, stp.y\n",
    "                    \n",
    "                starport_xy = (stp_x,stp_y)\n",
    "\n",
    "                return actions.RAW_FUNCTIONS.Build_TechLab_Starport_pt(\n",
    "                    \"now\", stp.tag, starport_xy)\n",
    "            \n",
    "        ############ Cloak upgrade #########################\n",
    "        if len(completed_starport_techlab) > 0 and self.cloaking_flag:\n",
    "            # self.cloaking_flag = 0\n",
    "            cloaking = upgrades.Upgrades.CloakingField\n",
    "            \n",
    "            stp_techlab = self.get_my_units_by_type(obs, units.Terran.StarportTechLab)\n",
    "            if stp_techlab:\n",
    "                stp_tech_xy = (stp_techlab[0].x, stp_techlab[0].y)\n",
    "                cloak_field = self.get_my_units_by_type(obs, upgrades.Upgrades.CloakingField)[0]\n",
    "                \n",
    "#                 print(\"stp_tech_xy: \", stp_tech_xy)\n",
    "#                 print(\"cloaking upgrade: \",cloak_field.tag)\n",
    "                return actions.FUNCTIONS.Research_BansheeCloakingField_quick(\"now\", cloaking )\n",
    "                \n",
    "        return actions.RAW_FUNCTIONS.no_op()\n",
    "    \n",
    "    def train_banshee(self, obs):\n",
    "        completed_starporttechlab = self.get_my_completed_units_by_type(\n",
    "            obs, units.Terran.StarportTechLab)\n",
    "        \n",
    "        ravens = self.get_my_units_by_type(obs, units.Terran.Raven)\n",
    "        \n",
    "        free_supply = (obs.observation.player.food_cap -\n",
    "                       obs.observation.player.food_used)\n",
    "        \n",
    "        \n",
    "        if (len(completed_starporttechlab) > 0 and obs.observation.player.minerals >= 200\n",
    "                and free_supply > 3):\n",
    "            \n",
    "            starports = self.get_my_units_by_type(obs, units.Terran.Starport)[0]\n",
    "            \n",
    "            ############################### cloaking detecting을 위한 Raven 생산 #######################\n",
    "            if starports.order_length < 2 and len(ravens) < 3 :\n",
    "                return actions.RAW_FUNCTIONS.Train_Raven_quick(\"now\", starports.tag)\n",
    "            \n",
    "            #########################################################################################\n",
    "            \n",
    "            if starports.order_length < 5:\n",
    "                return actions.RAW_FUNCTIONS.Train_Banshee_quick(\"now\", starports.tag)\n",
    "        return actions.RAW_FUNCTIONS.no_op()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ############################################################################################\n",
    "   \n",
    "    def attack(self, obs):\n",
    "        marines = self.get_my_units_by_type(obs, units.Terran.Marine)\n",
    "        if 20 < len(marines):\n",
    "            \n",
    "            flag = random.randint(0,2)\n",
    "            if flag == 1:\n",
    "                attack_xy = (38, 44) if self.base_top_left else (19, 23)\n",
    "            else:\n",
    "                attack_xy = (16, 45) if self.base_top_left else (42, 19)\n",
    "            \n",
    "            \n",
    "            distances = self.get_distances(obs, marines, attack_xy)\n",
    "            marine = marines[np.argmax(distances)]\n",
    "            #marine = marines\n",
    "            \n",
    "            x_offset = random.randint(-5, 5)\n",
    "            y_offset = random.randint(-5, 5)\n",
    "            return actions.RAW_FUNCTIONS.Attack_pt(\n",
    "                \"now\", marine.tag, (attack_xy[0] + x_offset, attack_xy[1] + y_offset))\n",
    "        return actions.RAW_FUNCTIONS.no_op()\n",
    "    \n",
    "    def attack_all(self,obs):\n",
    "        # 추가 유닛 생길 때 마다 추가\n",
    "        marines = self.get_my_units_by_type(obs, units.Terran.Marine)\n",
    "        tanks = self.get_my_units_by_type(obs, units.Terran.SiegeTank)\n",
    "        banshees = self.get_my_units_by_type(obs, units.Terran.Banshee)\n",
    "        raven = self.get_my_units_by_type(obs, units.Terran.Raven)\n",
    "        \n",
    "        all_units = marines + tanks + banshees + raven\n",
    "        \n",
    "        if 30 < len(all_units):\n",
    "            \n",
    "            flag = random.randint(0,2)\n",
    "            if flag == 1:\n",
    "                attack_xy = (44, 50) if self.base_top_left else (14, 19)\n",
    "            else:\n",
    "                attack_xy = (16, 45) if self.base_top_left else (42, 19)\n",
    "            \n",
    "            x_offset = random.randint(-5, 5)\n",
    "            y_offset = random.randint(-5, 5)\n",
    "            \n",
    "            all_tag = [unit.tag for unit in all_units]\n",
    "            \n",
    "            return actions.RAW_FUNCTIONS.Attack_pt(\n",
    "                \"now\", all_tag, (attack_xy[0] + x_offset, attack_xy[1] + y_offset))\n",
    "        else:\n",
    "            flag = random.randint(0,4)\n",
    "            if flag == 1:\n",
    "                attack_xy = (40, 25) if self.base_top_left else (25, 36)\n",
    "            elif flag == 2:\n",
    "                attack_xy = (40, 25) if self.base_top_left else (25, 36)\n",
    "            elif flag == 3:\n",
    "                attack_xy = (40, 25) if self.base_top_left else (25, 36)\n",
    "            else:\n",
    "                attack_xy = (21, 25) if self.base_top_left else (37, 45)\n",
    "            \n",
    "            x_offset = random.randint(-5, 5)\n",
    "            y_offset = random.randint(-5, 5)\n",
    "            \n",
    "            all_tag = [unit.tag for unit in all_units]\n",
    "            if all_tag:\n",
    "                return actions.RAW_FUNCTIONS.Attack_pt(\n",
    "                    \"now\", all_tag, (attack_xy[0] + x_offset, attack_xy[1] + y_offset))\n",
    "            \n",
    "        return actions.RAW_FUNCTIONS.no_op()\n",
    "    \n",
    "    ###################################################################################\n",
    "    ############################### Unit Controls #####################################\n",
    "    \n",
    "    def tank_control(self, obs):\n",
    "        tanks = self.get_my_units_by_type(obs, units.Terran.SiegeTank)\n",
    "        sieged_tanks = self.get_my_units_by_type(obs, units.Terran.SiegeTankSieged)\n",
    "        \n",
    "        total_tanks = tanks + sieged_tanks\n",
    "        \n",
    "        if len(total_tanks) < 8:\n",
    "            \n",
    "            if tanks:\n",
    "            \n",
    "                attack_xy = (40, 25) if self.base_top_left else (25, 40)\n",
    "\n",
    "                distances = self.get_distances(obs, tanks, attack_xy)\n",
    "                tank = tanks[np.argmax(distances)]\n",
    "\n",
    "                x_offset = random.randint(-5, 5)\n",
    "                y_offset = random.randint(-5, 5)\n",
    "                return actions.RAW_FUNCTIONS.Morph_SiegeMode_quick(\n",
    "                    \"now\", tank.tag, (attack_xy[0] + x_offset, attack_xy[1] + y_offset))\n",
    "        else:\n",
    "            #### siegeMode 제거 ####\n",
    "            all_tanks_tag = [tank.tag for tank in total_tanks]\n",
    "            return actions.RAW_FUNCTIONS.Morph_Unsiege_quick(\n",
    "                \"now\", all_tanks_tag)\n",
    "        \n",
    "        return actions.RAW_FUNCTIONS.no_op()\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TerranRandomAgent(TerranAgentWithRawActsAndRawObs):\n",
    "    def step(self, obs):\n",
    "        super(TerranRandomAgent, self).step(obs)\n",
    "        action = random.choice(self.actions)\n",
    "        \n",
    "        return getattr(self, action)(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter\n",
    "\n",
    "하이퍼파라미터는 심층강화학습 알고리즘에서 성능에 매우 큰 영향을 미칩니다.\n",
    "이 실험에 쓰인 하이퍼파라미터는 https://github.com/chucnorrisful/dqn 실험에서 제안된 값들을 참고하였습니다.\n",
    "\n",
    "\n",
    "- self.s_dim = 21\n",
    "- self.a_dim = 6\n",
    "\n",
    "- self.lr = 1e-4 * 1\n",
    "- self.batch_size = 32\n",
    "- self.gamma = 0.99\n",
    "- self.memory_size = 200000\n",
    "- self.eps_max = 1.0\n",
    "- self.eps_min = 0.01\n",
    "- self.epsilon = 1.0\n",
    "- self.init_sampling = 4000\n",
    "- self.target_update_interval = 10\n",
    "\n",
    "- self.epsilon = max(self.eps_min, self.eps_max - self.eps_min * (self.episode_count / 50))\n",
    "\n",
    "\n",
    "![Winning rate graph](./images/rlagent_with_vanilla_dqn_score-Terran-Terran-495_Eps.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TerranRLAgentWithRawActsAndRawObs(TerranAgentWithRawActsAndRawObs):\n",
    "    def __init__(self):\n",
    "        super(TerranRLAgentWithRawActsAndRawObs, self).__init__()\n",
    "\n",
    "        self.s_dim = 21\n",
    "        self.a_dim = 19\n",
    "        \n",
    "        self.lr = 1e-4 * 1\n",
    "        self.batch_size = 32\n",
    "        self.gamma = 0.99\n",
    "        self.memory_size = 200000\n",
    "        self.eps_max = 1.0\n",
    "        self.eps_min = 0.01\n",
    "        self.epsilon = 1.0\n",
    "        self.init_sampling = 4000\n",
    "        self.target_update_interval = 10\n",
    "\n",
    "        self.data_file_qnet = DATA_FILE_QNET\n",
    "        self.data_file_qnet_target = DATA_FILE_QNET_TARGET\n",
    "        self.score_file = SCORE_FILE\n",
    "        \n",
    "        self.qnetwork = NaiveMultiLayerPerceptron(input_dim=self.s_dim,\n",
    "                           output_dim=self.a_dim,\n",
    "                           num_neurons=[128],\n",
    "                           hidden_act_func='ReLU',\n",
    "                           out_act_func='Identity').to(device)\n",
    "        \n",
    "        self.qnetwork_target = NaiveMultiLayerPerceptron(input_dim=self.s_dim,\n",
    "                           output_dim=self.a_dim,\n",
    "                           num_neurons=[128],\n",
    "                           hidden_act_func='ReLU',\n",
    "                           out_act_func='Identity').to(device)\n",
    "        \n",
    "        ############################################ qnet 로드하면 이전 모델이라 학습모델 인풋 아웃풋차원이 바뀜 #########\n",
    "        if os.path.isfile(self.data_file_qnet + '.pt'):\n",
    "           self.qnetwork.load_state_dict(torch.load(self.data_file_qnet + '.pt'))\n",
    "            \n",
    "        if os.path.isfile(self.data_file_qnet_target + '.pt'):\n",
    "           self.qnetwork_target.load_state_dict(torch.load(self.data_file_qnet_target + '.pt'))\n",
    "        \n",
    "        # initialize target network same as the main network.\n",
    "        self.qnetwork_target.load_state_dict(self.qnetwork.state_dict())\n",
    "\n",
    "        self.dqn = DQN(state_dim=self.s_dim,\n",
    "                             action_dim=self.a_dim,\n",
    "                             qnet=self.qnetwork,\n",
    "                             qnet_target=self.qnetwork_target,\n",
    "                             lr=self.lr,\n",
    "                             gamma=self.gamma,\n",
    "                             epsilon=self.epsilon).to(device)\n",
    "        \n",
    "        self.memory = ExperienceReplayMemory(self.memory_size)\n",
    "        \n",
    "        self.print_every = 1\n",
    "        self.cum_reward = 0\n",
    "        self.cum_loss = 0\n",
    "        self.episode_count = 0\n",
    "        \n",
    "        self.new_game()\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        super(TerranRLAgentWithRawActsAndRawObs, self).reset()\n",
    "        self.new_game()\n",
    "\n",
    "    def new_game(self):\n",
    "        self.base_top_left = None\n",
    "        self.previous_state = None\n",
    "        self.previous_action = None\n",
    "        self.cum_reward = 0\n",
    "        self.cum_loss = 0\n",
    "        \n",
    "        # epsilon scheduling\n",
    "        # slowly decaying_epsilon\n",
    "        self.epsilon = max(self.eps_min, self.eps_max - self.eps_min * (self.episode_count / 50))\n",
    "        self.dqn.epsilon = torch.tensor(self.epsilon).to(device)\n",
    "        \n",
    "\n",
    "    def get_state(self, obs):\n",
    "        scvs = self.get_my_units_by_type(obs, units.Terran.SCV)\n",
    "        idle_scvs = [scv for scv in scvs if scv.order_length == 0]\n",
    "        command_centers = self.get_my_units_by_type(obs, units.Terran.CommandCenter)\n",
    "        supply_depots = self.get_my_units_by_type(obs, units.Terran.SupplyDepot)\n",
    "        completed_supply_depots = self.get_my_completed_units_by_type(\n",
    "            obs, units.Terran.SupplyDepot)\n",
    "        barrackses = self.get_my_units_by_type(obs, units.Terran.Barracks)\n",
    "        completed_barrackses = self.get_my_completed_units_by_type(\n",
    "            obs, units.Terran.Barracks)\n",
    "        marines = self.get_my_units_by_type(obs, units.Terran.Marine)\n",
    "\n",
    "        queued_marines = (completed_barrackses[0].order_length\n",
    "        if len(completed_barrackses) > 0 else 0)\n",
    "\n",
    "        free_supply = (obs.observation.player.food_cap -\n",
    "                       obs.observation.player.food_used)\n",
    "        can_afford_supply_depot = obs.observation.player.minerals >= 100\n",
    "        can_afford_barracks = obs.observation.player.minerals >= 150\n",
    "        can_afford_marine = obs.observation.player.minerals >= 100\n",
    "\n",
    "        enemy_scvs = self.get_enemy_units_by_type(obs, units.Terran.SCV)\n",
    "        enemy_idle_scvs = [scv for scv in enemy_scvs if scv.order_length == 0]\n",
    "        enemy_command_centers = self.get_enemy_units_by_type(\n",
    "            obs, units.Terran.CommandCenter)\n",
    "        enemy_supply_depots = self.get_enemy_units_by_type(\n",
    "            obs, units.Terran.SupplyDepot)\n",
    "        enemy_completed_supply_depots = self.get_enemy_completed_units_by_type(\n",
    "            obs, units.Terran.SupplyDepot)\n",
    "        enemy_barrackses = self.get_enemy_units_by_type(obs, units.Terran.Barracks)\n",
    "        enemy_completed_barrackses = self.get_enemy_completed_units_by_type(\n",
    "            obs, units.Terran.Barracks)\n",
    "        enemy_marines = self.get_enemy_units_by_type(obs, units.Terran.Marine)\n",
    "\n",
    "        return (len(command_centers),\n",
    "                len(scvs),\n",
    "                len(idle_scvs),\n",
    "                len(supply_depots),\n",
    "                len(completed_supply_depots),\n",
    "                len(barrackses),\n",
    "                len(completed_barrackses),\n",
    "                len(marines),\n",
    "                queued_marines,\n",
    "                free_supply,\n",
    "                can_afford_supply_depot,\n",
    "                can_afford_barracks,\n",
    "                can_afford_marine,\n",
    "                len(enemy_command_centers),\n",
    "                len(enemy_scvs),\n",
    "                len(enemy_idle_scvs),\n",
    "                len(enemy_supply_depots),\n",
    "                len(enemy_completed_supply_depots),\n",
    "                len(enemy_barrackses),\n",
    "                len(enemy_completed_barrackses),\n",
    "                len(enemy_marines))\n",
    "\n",
    "    def step(self, obs):\n",
    "        super(TerranRLAgentWithRawActsAndRawObs, self).step(obs)\n",
    "        \n",
    "        #time.sleep(0.5)\n",
    "        \n",
    "        state = self.get_state(obs)\n",
    "        state = torch.tensor(state).float().view(1, self.s_dim).to(device)\n",
    "        action_idx = self.dqn.choose_action(state)\n",
    "        action = self.actions[action_idx]\n",
    "        done = True if obs.last() else False\n",
    "\n",
    "        if self.previous_action is not None:\n",
    "            experience = (self.previous_state.to(device),\n",
    "                          torch.tensor(self.previous_action).view(1, 1).to(device),\n",
    "                          torch.tensor(obs.reward).view(1, 1).to(device),\n",
    "                          state.to(device),\n",
    "                          torch.tensor(done).view(1, 1).to(device))\n",
    "            self.memory.push(experience)\n",
    "        \n",
    "        self.cum_reward += obs.reward\n",
    "        self.previous_state = state\n",
    "        self.previous_action = action_idx\n",
    "        \n",
    "        if obs.last():\n",
    "            self.episode_count = self.episode_count + 1\n",
    "            \n",
    "            if len(self.memory) >= self.init_sampling:\n",
    "                # training dqn\n",
    "                sampled_exps = self.memory.sample(self.batch_size)\n",
    "                sampled_exps = prepare_training_inputs(sampled_exps, device)\n",
    "                self.dqn.learn(*sampled_exps)\n",
    "\n",
    "            if self.episode_count % self.target_update_interval == 0:\n",
    "                self.dqn.qnet_target.load_state_dict(self.dqn.qnet.state_dict())\n",
    "\n",
    "            if self.episode_count % self.print_every == 0:\n",
    "                msg = (self.episode_count, self.cum_reward, self.epsilon)\n",
    "                print(\"Episode : {:4.0f} | Cumulative Reward : {:4.0f} | Epsilon : {:.3f}\".format(*msg))\n",
    "            \n",
    "            torch.save(self.dqn.qnet.state_dict(), self.data_file_qnet + '.pt')\n",
    "            torch.save(self.dqn.qnet_target.state_dict(), self.data_file_qnet_target + '.pt')\n",
    "\n",
    "            scores_window.append(obs.reward)  # save most recent reward\n",
    "            win_rate = scores_window.count(1)/len(scores_window)*100\n",
    "            tie_rate = scores_window.count(0)/len(scores_window)*100\n",
    "            lost_rate = scores_window.count(-1)/len(scores_window)*100\n",
    "            \n",
    "            scores.append([win_rate, tie_rate, lost_rate])  # save most recent score(win_rate, tie_rate, lost_rate)\n",
    "            with open(self.score_file + '.txt', \"wb\") as fp:\n",
    "                pickle.dump(scores, fp)\n",
    "            \n",
    "            #writer.add_scalar(\"Loss/train\", self.cum_loss/obs.observation.game_loop, self.episode_count)\n",
    "            writer.add_scalar(\"Score\", self.cum_reward, self.episode_count)\n",
    "\n",
    "        return getattr(self, action)(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0922 02:57:58.120776 4616515008 sc_process.py:135] Launching SC2: /Applications/StarCraft II/Versions/Base81102/SC2.app/Contents/MacOS/SC2 -listen 127.0.0.1 -port 20189 -dataDir /Applications/StarCraft II/ -tempDir /var/folders/r1/x6k135_915z463fc7lc4hkp40000gn/T/sc-1oquez7h/ -displayMode 0 -windowwidth 640 -windowheight 480 -windowx 50 -windowy 50\n",
      "I0922 02:57:58.132446 4616515008 remote_controller.py:167] Connecting to: ws://127.0.0.1:20189/sc2api, attempt: 0, running: True\n",
      "I0922 02:57:59.139338 4616515008 remote_controller.py:167] Connecting to: ws://127.0.0.1:20189/sc2api, attempt: 1, running: True\n",
      "I0922 02:58:00.145560 4616515008 remote_controller.py:167] Connecting to: ws://127.0.0.1:20189/sc2api, attempt: 2, running: True\n",
      "I0922 02:58:01.152811 4616515008 remote_controller.py:167] Connecting to: ws://127.0.0.1:20189/sc2api, attempt: 3, running: True\n",
      "I0922 02:58:02.160143 4616515008 remote_controller.py:167] Connecting to: ws://127.0.0.1:20189/sc2api, attempt: 4, running: True\n",
      "I0922 02:58:03.163006 4616515008 remote_controller.py:167] Connecting to: ws://127.0.0.1:20189/sc2api, attempt: 5, running: True\n",
      "I0922 02:58:04.165038 4616515008 remote_controller.py:167] Connecting to: ws://127.0.0.1:20189/sc2api, attempt: 6, running: True\n",
      "I0922 02:58:05.171226 4616515008 remote_controller.py:167] Connecting to: ws://127.0.0.1:20189/sc2api, attempt: 7, running: True\n",
      "I0922 02:58:06.175367 4616515008 remote_controller.py:167] Connecting to: ws://127.0.0.1:20189/sc2api, attempt: 8, running: True\n",
      "I0922 02:58:07.179664 4616515008 remote_controller.py:167] Connecting to: ws://127.0.0.1:20189/sc2api, attempt: 9, running: True\n",
      "I0922 02:58:08.186416 4616515008 remote_controller.py:167] Connecting to: ws://127.0.0.1:20189/sc2api, attempt: 10, running: True\n",
      "I0922 02:58:09.188346 4616515008 remote_controller.py:167] Connecting to: ws://127.0.0.1:20189/sc2api, attempt: 11, running: True\n",
      "I0922 02:58:10.191205 4616515008 remote_controller.py:167] Connecting to: ws://127.0.0.1:20189/sc2api, attempt: 12, running: True\n",
      "I0922 02:58:11.196767 4616515008 remote_controller.py:167] Connecting to: ws://127.0.0.1:20189/sc2api, attempt: 13, running: True\n",
      "I0922 02:58:12.198444 4616515008 remote_controller.py:167] Connecting to: ws://127.0.0.1:20189/sc2api, attempt: 14, running: True\n",
      "I0922 02:58:13.199834 4616515008 remote_controller.py:167] Connecting to: ws://127.0.0.1:20189/sc2api, attempt: 15, running: True\n",
      "I0922 02:58:21.431921 4616515008 sc2_env.py:314] Environment is ready\n",
      "I0922 02:58:21.437120 4616515008 sc2_env.py:507] Starting episode 1: [terran, random] on Simple64\n",
      "I0922 03:09:22.040234 4616515008 sc2_env.py:752] Environment Close\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 660.603 seconds for 1087 steps: 1.645 fps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0922 03:09:22.348584 4616515008 sc_process.py:232] Shutdown gracefully.\n",
      "I0922 03:09:22.349338 4616515008 sc_process.py:210] Shutdown with return code: -15\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'starport_xy' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-146-60174cc17dd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/starcraft2/lib/python3.6/site-packages/absl/app.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv, flags_parser)\u001b[0m\n\u001b[1;32m    297\u001b[0m       \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m       \u001b[0m_run_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mUsageError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m       \u001b[0musage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshorthelp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetailed_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexitcode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexitcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/starcraft2/lib/python3.6/site-packages/absl/app.py\u001b[0m in \u001b[0;36m_run_main\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-83-416804353d2b>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(unused_argv)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m   \u001b[0mrun_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthreads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-83-416804353d2b>\u001b[0m in \u001b[0;36mrun_thread\u001b[0;34m(agent_classes, players, map_name, visualize)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;31m#env = available_actions_printer.AvailableActionsPrinter(env)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0magents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0magent_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0magent_cls\u001b[0m \u001b[0;32min\u001b[0m \u001b[0magent_classes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m     \u001b[0mrun_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_agent_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_replay\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m       \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_replay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent_classes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/starcraft2/lib/python3.6/site-packages/pysc2/env/run_loop.py\u001b[0m in \u001b[0;36mrun_loop\u001b[0;34m(agents, env, max_frames, max_episodes)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mtotal_frames\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         actions = [agent.step(timestep)\n\u001b[0;32m---> 43\u001b[0;31m                    for agent, timestep in zip(agents, timesteps)]\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmax_frames\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtotal_frames\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mmax_frames\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m           \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/starcraft2/lib/python3.6/site-packages/pysc2/env/run_loop.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mtotal_frames\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         actions = [agent.step(timestep)\n\u001b[0;32m---> 43\u001b[0;31m                    for agent, timestep in zip(agents, timesteps)]\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmax_frames\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtotal_frames\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mmax_frames\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m           \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-145-b1e40b0df1af>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Score\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcum_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-143-41ae521764d7>\u001b[0m in \u001b[0;36mbuild_starports\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mstarport_xy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m64\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mstarport_xy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'starport_xy' referenced before assignment"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  app.run(main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Winning rate graph]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages (3.0.3)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: numpy>=1.10.0 in /Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages (from matplotlib) (1.18.5)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages (from kiwisolver>=1.0.1->matplotlib) (47.1.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.3 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.5/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "SCORE_FILE = 'rlagent_with_vanilla_dqn_score'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(SCORE_FILE + '.txt', \"rb\") as fp:\n",
    "    scores = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[100.        ,   0.        ,   0.        ],\n",
       "       [100.        ,   0.        ,   0.        ],\n",
       "       [100.        ,   0.        ,   0.        ],\n",
       "       [100.        ,   0.        ,   0.        ],\n",
       "       [ 80.        ,   0.        ,  20.        ],\n",
       "       [ 66.66666667,  16.66666667,  16.66666667],\n",
       "       [ 71.42857143,  14.28571429,  14.28571429],\n",
       "       [ 62.5       ,  25.        ,  12.5       ],\n",
       "       [ 66.66666667,  22.22222222,  11.11111111],\n",
       "       [ 70.        ,  20.        ,  10.        ],\n",
       "       [ 72.72727273,  18.18181818,   9.09090909],\n",
       "       [ 75.        ,  16.66666667,   8.33333333],\n",
       "       [ 76.92307692,  15.38461538,   7.69230769],\n",
       "       [ 78.57142857,  14.28571429,   7.14285714],\n",
       "       [ 73.33333333,  20.        ,   6.66666667],\n",
       "       [ 75.        ,  18.75      ,   6.25      ],\n",
       "       [ 76.47058824,  17.64705882,   5.88235294],\n",
       "       [ 77.77777778,  16.66666667,   5.55555556],\n",
       "       [ 78.94736842,  15.78947368,   5.26315789],\n",
       "       [ 80.        ,  15.        ,   5.        ],\n",
       "       [ 80.95238095,  14.28571429,   4.76190476],\n",
       "       [ 81.81818182,  13.63636364,   4.54545455],\n",
       "       [ 82.60869565,  13.04347826,   4.34782609],\n",
       "       [ 83.33333333,  12.5       ,   4.16666667]])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_scores = np.array(scores)\n",
    "np_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(np_scores)), np_scores.T[0], color='r', label='win rate')\n",
    "plt.plot(np.arange(len(np_scores)), np_scores.T[1], color='g', label='tie rate')\n",
    "plt.plot(np.arange(len(np_scores)), np_scores.T[2], color='b', label='lose rate')\n",
    "plt.ylabel('Score %')\n",
    "plt.xlabel('Episode #')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-b8811b42e2df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'file' is not defined"
     ]
    }
   ],
   "source": [
    "f = file.open()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "starcraft2",
   "language": "python",
   "name": "starcraft2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
